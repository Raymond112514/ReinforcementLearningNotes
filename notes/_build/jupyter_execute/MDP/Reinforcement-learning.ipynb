{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5e31c0",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edd805",
   "metadata": {},
   "source": [
    "Given an MDP instance, there are two algorithms for finding the optimal policy: the value iteration method and the policy iteration method. While both methods are guaranteed to converge, empirically, the policy iteration method typically converges faster and is therefore often preferred. However, both methods require knowledge of the transition dynamics \\( p(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, \\mathbf{a}_t) \\). Without the transition dynamics, performing the Bellman update becomes infeasible.\n",
    "\n",
    "In real-life scenarios, the transition dynamics are often unknown or too complex to compute. For example, in the game of blackjack, although we could theoretically compute the transition dynamics for all possible states, doing so would be extremely challenging. Reinforcement learning methods aim to learn the optimal policy without needing to know the transition dynamics. In the following section, we discuss two classes of methods\n",
    "\n",
    "1. **Value-based methods**: These methods attempt to learn the value or action-value functions and use them for control.\n",
    "2. **Policy-based methods**: These methods parameterize the policy directly to solve the reward maximization problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}