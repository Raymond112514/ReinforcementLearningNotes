Search.setIndex({"docnames": ["MDP/Markov-decision-process", "MDP/Policy-iteration", "MDP/Value-iteration", "MDP/Value-iteration-convergence", "Model based methods/Monte-Carlo-tree-search", "MonteCarlo/Monte-Carlo-Control", "MonteCarlo/Monte-Carlo-value-estimation", "PolicyGradient/Actor-critic-methods", "PolicyGradient/Deterministic-policy-gradient-method", "PolicyGradient/Generalized-advantage-estimation", "PolicyGradient/Policy-gradient-method", "PolicyGradient/Policy-gradient-theorem", "PolicyGradient/Proximal-policy-gradient", "PolicyGradient/Soft-actor-critic", "PolicyGradient/Trust-region-policy-optimization", "PolicyGradient/Variance-reduction", "TD/Temporal-difference", "TD/Temporal-difference-control", "TD/Untitled", "ValueBasedMethods/Deep-Q-Learning", "ValueBasedMethods/Dueling-Q-Learning", "intro", "markdown", "markdown-notebooks", "notebooks"], "filenames": ["MDP/Markov-decision-process.ipynb", "MDP/Policy-iteration.ipynb", "MDP/Value-iteration.ipynb", "MDP/Value-iteration-convergence.ipynb", "Model based methods/Monte-Carlo-tree-search.ipynb", "MonteCarlo/Monte-Carlo-Control.ipynb", "MonteCarlo/Monte-Carlo-value-estimation.ipynb", "PolicyGradient/Actor-critic-methods.ipynb", "PolicyGradient/Deterministic-policy-gradient-method.ipynb", "PolicyGradient/Generalized-advantage-estimation.ipynb", "PolicyGradient/Policy-gradient-method.ipynb", "PolicyGradient/Policy-gradient-theorem.ipynb", "PolicyGradient/Proximal-policy-gradient.ipynb", "PolicyGradient/Soft-actor-critic.ipynb", "PolicyGradient/Trust-region-policy-optimization.ipynb", "PolicyGradient/Variance-reduction.ipynb", "TD/Temporal-difference.ipynb", "TD/Temporal-difference-control.ipynb", "TD/Untitled.ipynb", "ValueBasedMethods/Deep-Q-Learning.ipynb", "ValueBasedMethods/Dueling-Q-Learning.ipynb", "intro.md", "markdown.md", "markdown-notebooks.md", "notebooks.ipynb"], "titles": ["<span class=\"section-number\">1. </span>Markov decision process", "<span class=\"section-number\">3. </span>Policy iteration", "<span class=\"section-number\">2. </span>Value iteration", "<span class=\"section-number\">2.4. </span>Theoretic analysis", "Monte Carlo tree search", "<span class=\"section-number\">4.5. </span>Monte Carlo control", "<span class=\"section-number\">4. </span>Value estimation", "<span class=\"section-number\">9. </span>Actor critic methods", "<span class=\"section-number\">12. </span>Deterministic policy gradient method", "<span class=\"section-number\">10. </span>Generalized advantage estimation", "<span class=\"section-number\">8. </span>Policy gradient method", "Policy gradient theorem", "<span class=\"section-number\">11. </span>Proximal policy gradient", "<span class=\"section-number\">13. </span>Soft actor critic", "Trust region policy optimization", "Variance reduction", "<span class=\"section-number\">5. </span>Temporal difference", "<span class=\"section-number\">5.3. </span>Temporal difference control", "&lt;no title&gt;", "<span class=\"section-number\">6. </span>Deep Q Learning", "<span class=\"section-number\">7. </span>Dueling Q Learning", "Welcome to your Jupyter Book", "Markdown Files", "Notebooks with MyST Markdown", "Content with notebooks"], "terms": {"some": [0, 9, 21, 22, 24], "text": [0, 1, 2, 7, 10, 12, 17, 22, 23], "about": [0, 10, 22, 23, 24], "mdp": [0, 1, 2, 11, 13], "anoth": [1, 7], "wai": [1, 6, 8, 10, 17], "solv": [1, 2, 3, 12, 19], "problem": [1, 6, 8, 10, 12, 15], "instead": [1, 9, 10, 12, 16, 17, 19], "learn": [1, 7, 8, 9, 10, 12, 13, 14, 21], "optim": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 17, 19, 20], "state": [1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 24], "valu": [1, 3, 5, 9, 10, 12, 13, 14, 17, 19, 20], "function": [1, 2, 3, 5, 9, 10, 13, 14, 17, 19, 22], "start": [1, 6, 9, 22, 23], "random": [1, 2, 5, 8, 10, 13, 14, 16, 17, 19, 20, 24], "gradual": 1, "imporv": 1, "two": [1, 3, 6, 7, 8, 9, 13, 22, 23], "step": [1, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "which": [1, 7, 8, 9, 10, 12, 16, 19, 23], "v_": [1, 2, 3, 6, 7, 16], "pi": [1, 2, 6, 11, 13, 16, 17], "": [1, 2, 3, 7, 8, 9, 10, 11, 12, 13, 16, 17, 19, 22, 23, 24], "updat": [1, 2, 3, 7, 10, 12, 13, 14, 16, 17, 19], "increas": [1, 8, 10], "In": [1, 3, 6, 7, 8, 9, 10, 11, 12, 13, 16, 17, 19, 22], "we": [1, 2, 3, 6, 7, 8, 9, 10, 11, 13, 16, 17, 19], "By": [1, 2, 8, 9], "bellman": [1, 2, 3], "equat": [1, 2, 3, 8, 9], "have": [1, 2, 3, 7, 8, 9, 16, 19, 23], "sum_": [1, 2, 3, 6, 7, 8, 9, 10, 11, 13], "p": [1, 2, 3, 6, 8, 10, 11], "r_0": [1, 2, 3, 10], "gamma": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "can": [1, 2, 3, 6, 7, 8, 9, 10, 15, 16, 17, 19, 22, 23, 24], "us": [1, 2, 3, 5, 6, 7, 8, 9, 10, 15, 16, 17, 19, 22, 23], "an": [1, 2, 7, 9, 10, 13, 16, 17, 19, 22], "fashion": [1, 10], "like": [1, 2, 6, 7, 8, 19, 22, 23], "The": [1, 2, 5, 6, 7, 8, 9, 10, 13, 16, 17, 19, 22, 23], "psuedocod": [1, 7, 8, 16, 17], "prsent": 1, "below": [1, 2, 7, 8, 9, 10, 16, 17, 19], "onli": [1, 8, 16, 19], "differ": [1, 6, 9, 21, 22], "rule": [1, 8, 10, 17, 19], "input": [1, 2, 7, 8, 10, 16, 17, 22], "given": [1, 2, 3, 7, 11, 13, 17], "instanc": [1, 2, 5, 6, 11], "output": [1, 2, 8, 16, 17, 23], "comput": [1, 2, 6, 7, 8, 9, 10, 16, 19, 20], "initi": [1, 2, 8, 10, 16, 17], "arrai": [1, 2, 8, 24], "v": [1, 2, 3, 7, 9, 10, 11, 13, 16], "0": [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 24], "all": [1, 2, 9, 16, 17, 22, 23], "mathcal": [1, 2, 3, 8, 11, 12, 13, 16, 17, 19], "while": [1, 2, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "converg": [1, 2, 3, 16, 19], "leftarrow": [1, 2, 3, 7, 8, 10, 16, 17], "A": [1, 2, 5, 6, 7, 8, 9, 11, 17, 19], "current": [1, 17], "consid": [1, 10, 12], "action": [1, 2, 5, 6, 7, 9, 10, 12, 13, 14, 16, 17, 19, 20], "fucntion": [1, 2, 5, 6], "q_": [1, 2, 6, 8, 11, 19], "intuit": [1, 8], "exist": [1, 3], "geq": [1, 7, 9, 10, 11, 13], "Then": 1, "should": [1, 9, 23], "get": [1, 2, 5, 7, 8, 9, 10, 12, 13, 17, 19, 20, 22, 23], "better": [1, 7], "payoff": 1, "choos": [1, 7, 9, 17], "thi": [1, 2, 3, 6, 7, 8, 9, 10, 11, 13, 16, 17, 19, 21, 22, 23, 24], "motiv": [1, 10], "u": [1, 3, 6, 7, 8, 10, 17], "our": [1, 2, 7, 8, 16, 17, 19], "greedi": [1, 17], "begin": [1, 2, 3, 7, 8, 9, 10, 11, 13, 24], "align": [1, 2, 3, 7, 8, 9, 10, 11, 13, 24], "hat": [1, 17], "underset": [1, 2, 10, 17], "argmax": [1, 2, 10, 17, 19, 20], "r": [1, 3, 6, 7, 8, 9, 10, 11, 13, 19], "end": [1, 2, 3, 7, 8, 9, 10, 11, 13, 16, 24], "now": [1, 2, 3, 7, 10, 11, 13, 16, 19], "prove": [1, 3], "new": [1, 13, 14], "good": [1, 7], "than": [1, 7], "let": [1, 3, 8, 9, 10, 11, 23], "ani": [1, 3, 5, 6, 11, 21, 23], "pair": 1, "determinist": [1, 21], "hspace": [1, 7, 12], "5mm": [1, 12], "proof": [1, 3, 11], "omit": 1, "therefor": [1, 3, 7], "sequenc": 1, "from": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 24], "pi_0": 1, "xrightarrow": 1, "pi_1": 1, "sinc": [1, 2, 3, 6, 9, 16, 17], "accord": [1, 3], "leq": [1, 3, 12], "pi_n": 1, "follow": [1, 2, 3, 7, 8, 10, 12, 17, 19, 22, 23], "algorithm": [1, 2, 3, 8, 9, 10, 12, 13, 16, 17, 19], "eventu": 1, "displai": [1, 23], "import": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 24], "numpi": [1, 2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 24], "np": [1, 2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 24], "matplotlib": [1, 2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 24], "pyplot": [1, 2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 24], "plt": [1, 2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 24], "gridworld": 1, "seed": [1, 2, 24], "def": [1, 2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "policy_evalu": 1, "grid_world": [1, 2], "max_it": [1, 2, 5, 6], "1": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 24], "height": [1, 2], "width": [1, 2], "zero": [1, 2], "rang": [1, 2, 5, 6, 7, 9, 10, 12, 14, 16, 17, 19, 20, 24], "get_stat": [1, 2], "_": [1, 2, 3, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20], "successor": [1, 2], "get_available_act": [1, 2], "prob_dist": [1, 2], "get_transition_prob": [1, 2], "prob": [1, 2, 7, 10, 12], "item": [1, 2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 19, 20], "reward": [1, 2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "get_reward": [1, 2], "return": [1, 2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "policy_improv": 1, "dict": [1, 2, 6], "available_act": [1, 2], "q_valu": [1, 2, 5, 6, 8, 17, 19, 20], "max": [1, 2, 5, 12, 17, 19, 20], "kei": [1, 2, 5, 6, 10, 17], "len": [1, 2, 5, 6, 8, 12, 13, 14, 19, 20], "els": [1, 2, 6, 7, 8, 9, 10, 12, 13, 14, 19, 20], "none": [1, 2, 7, 8, 14], "get_random_polici": 1, "random_idx": 1, "choic": [1, 5, 7, 8, 16, 17], "arang": [1, 19, 20], "policy_iter": 1, "histori": [1, 7, 8, 9, 10, 12, 13, 14, 19, 20], "append": [1, 12, 14, 19, 20], "5": [1, 2, 7, 8, 9, 12, 13, 14, 19, 20, 24], "exit": [1, 2], "good_exit": [1, 2], "3": [1, 2, 7, 8, 9, 10, 12, 13, 14, 19, 20], "bad_exit": [1, 2], "2": [1, 2, 6, 7, 8, 9, 10, 12, 13, 14, 16, 19, 20, 23], "wall": [1, 2], "living_reward": [1, 2], "win_reward": [1, 2], "lose_reward": [1, 2], "init_po": [1, 2], "10": [1, 5, 8, 10, 12, 14, 16, 17, 19, 20, 24], "display_polici": [1, 2, 17], "object": [2, 7, 10, 11], "i": [2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 23, 24], "find": [2, 6, 7, 9, 10, 13], "where": [2, 6, 7, 8, 9, 10, 16, 19], "defin": [2, 3, 7, 8, 23], "mathbb": [2, 3, 6, 7, 8, 10, 11, 12, 13], "e": [2, 6, 7, 8, 10, 11, 12, 13, 14, 17], "a_0": 2, "s_1": 2, "a_1": 2, "s_0": [2, 6], "infti": [2, 3, 9], "r_": [2, 9], "note": [2, 3, 7, 9, 10, 12, 16, 17, 20, 22], "simplifi": [2, 9, 10], "bigg": [2, 3, 7, 8, 10, 11, 12, 13], "s_2": 2, "a_2": 2, "third": 2, "line": [2, 22, 23, 24], "law": 2, "total": 2, "expect": [2, 6, 8, 9, 10, 13, 16, 17, 19], "final": [2, 7], "markov": [2, 13], "properti": [2, 19], "assum": [2, 10], "size": [2, 7, 8, 10, 15, 19, 20], "space": [2, 19], "finit": [2, 19], "tag": 2, "equal": 2, "known": [2, 8, 10, 13, 17, 19], "its": [2, 7, 8, 10], "correspond": [2, 7], "linear": [2, 12, 13, 14, 20], "system": 2, "deriv": [2, 8, 10, 11], "similar": [2, 17, 22], "result": [2, 7, 8, 10, 13, 19], "same": [2, 8, 17, 19, 22], "argument": 2, "s_": 2, "a_": [2, 9], "hold": [2, 19], "also": [2, 6, 9, 10, 19, 22, 23, 24], "recal": [2, 6, 7], "max_a": [2, 3], "q": [2, 5, 7, 8, 9, 10, 11, 12, 13, 14, 16, 21], "substitut": [2, 9, 10], "see": [2, 9, 10, 17, 21, 22, 23, 24], "must": [2, 7, 22], "satisfi": [2, 9], "condit": [2, 10], "case": [2, 19], "howev": [2, 7, 8, 9, 10, 19], "practic": [2, 7], "solut": [2, 17, 19], "max_": [2, 3, 12, 17, 19], "code": [2, 22, 23], "cell": 2, "value_iter": 2, "perform": [2, 7, 8, 9, 10, 13, 16], "param": [2, 5, 6, 7, 8, 9, 10, 13, 14, 19], "grid": 2, "world": [2, 16], "int": [2, 5, 6, 10, 19, 20], "maximum": 2, "number": [2, 5, 6, 7, 8, 9, 10, 19], "allow": [2, 7, 10, 22], "execut": [2, 23], "float": [2, 7, 8, 9, 10, 12, 13, 14, 19, 20], "option": 2, "default": [2, 5, 6, 7, 23], "discount": [2, 5, 6, 16, 17], "factor": [2, 5, 6, 9, 17], "futur": [2, 16, 17], "ndarrai": 2, "2d": 2, "x": [2, 6, 8], "y": [2, 7, 8, 9, 10, 12, 13, 19, 20], "entri": [2, 19], "repres": [2, 7, 16, 19], "max_valu": 2, "inf": 2, "instanti": [2, 8], "appli": [2, 6, 8, 9, 10], "100": [2, 9, 10, 12, 16, 17, 19, 24], "display_valu": [2, 16], "know": [2, 6], "how": [2, 7, 17, 21, 23], "goal": [2, 13, 19], "attain": 2, "To": [2, 3, 7, 8, 10, 17, 19], "do": [2, 8, 22, 24], "so": [2, 7, 23], "determin": [2, 6, 17], "onc": [2, 19], "first": [2, 3, 5, 7, 8, 9], "extract": 2, "extract_q_valu": 2, "dictionari": [2, 10], "ar": [2, 6, 7, 10, 13, 16, 17, 19, 22, 23], "tupl": 2, "coordin": 2, "each": [2, 10, 16, 17, 19], "inner": 2, "map": [2, 3, 8], "extract_polici": 2, "opt_act": 2, "display_qvalu": 2, "And": [2, 3, 13, 16], "previou": [3, 7, 9, 11, 17], "section": [3, 9, 10, 11, 16, 17], "k": [3, 7, 8, 9, 10], "v_k": 3, "It": [3, 10, 17, 19, 21, 22], "show": [3, 8, 12, 20, 21, 22, 23], "inde": 3, "introduc": [3, 8, 9, 19], "oper": [3, 10], "t": [3, 5, 6, 7, 8, 9, 10, 11, 13, 16, 17, 19, 22, 24], "rightarrow": [3, 8], "For": [3, 8, 10, 11, 16, 17, 19, 22, 24], "vector": 3, "notat": [3, 9], "compactli": 3, "rewrit": [3, 11], "iter": [3, 6, 13, 21], "contract": 3, "under": 3, "sup": 3, "norm": 3, "other": [3, 7, 8, 9, 16, 17, 23], "word": [3, 7, 16], "check": [3, 21, 24], "directli": [3, 6, 7, 8, 10, 19], "after": [3, 9, 16], "remain": 3, "standard": [3, 13], "fix": [3, 24], "point": [3, 19], "theorem": [3, 8], "lim_": 3, "cdot": [3, 12, 13, 16, 17], "v_0": 3, "impli": [3, 10], "pip": [5, 6, 7, 8, 9, 10, 12, 13, 19, 20], "instal": [5, 6, 7, 8, 9, 10, 12, 13, 19, 20], "gym": [5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "itertool": [5, 6], "montecarlo": [5, 6], "copi": 5, "seaborn": [5, 6], "sn": [5, 6], "env": [5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "make": [5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 24], "blackjack": 5, "v1": [5, 6, 7, 9, 10, 12, 19, 20], "natur": [5, 6], "fals": [5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "sab": [5, 6], "player_sum_rang": [5, 6], "32": [5, 6, 8, 10, 13, 14, 19, 20], "dealer_card_rang": [5, 6], "11": [5, 6], "usable_ace_rang": [5, 6], "true": [5, 6, 9, 12, 13, 14, 16, 17, 20], "list": [5, 6, 16, 17], "product": [5, 6], "state_action_dict": 5, "20": [5, 6, 12], "class": [5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "stick_at_20_or_21": [5, 6], "self": [5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "epsilon": [5, 17, 19, 20], "montecarlocontrol": 5, "estim": [5, 7, 8, 10, 13, 15, 17, 21], "__init__": [5, 6, 7, 8, 9, 10, 12, 13, 14, 19, 20], "avail": [5, 6, 16], "sampl": [5, 6, 7, 8, 10, 12, 13, 14, 16, 17, 19, 20, 21, 24], "implement": [5, 6, 16, 17, 19], "every_visit": [5, 6], "enviro": [5, 6, 8, 10, 16, 17], "agent": [5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 19], "interact": [5, 6, 16, 24], "n_episod": [5, 6, 10, 12, 14, 16, 17, 19, 20], "episod": [5, 6, 7, 8, 9, 10, 12, 14, 16, 17, 19, 20], "boolean": [5, 6], "indic": [5, 6], "whether": [5, 6, 7, 22], "everi": 5, "visit": 5, "form": [5, 6], "state1": [5, 6], "value1": [5, 6], "count": [5, 6], "cum_reward": [5, 6, 17], "state_act": [5, 6], "enumer": [5, 6, 12, 20], "mc": [5, 6, 7], "optimal_polici": 5, "5000000": 5, "plotblackjackoptimalstrategi": 5, "stick_at_20_or_21_stochast": 5, "rand": [5, 17, 19, 20], "plot_q_values_heatmap": [5, 6], "Not": 6, "being": [6, 19, 22], "abl": 6, "access": 6, "transit": [6, 8, 19], "probabl": [6, 7], "pose": 6, "challeng": [6, 7, 19], "polici": [6, 9, 13, 17, 19, 20], "longer": [6, 17], "dynam": 6, "program": 6, "method": [6, 9, 16, 17, 19], "both": [6, 7, 8, 9, 22], "requir": [6, 10, 17], "suffici": 6, "depend": [6, 22], "One": [6, 8, 10, 12, 15, 16, 17, 19], "come": 6, "around": [6, 8, 16, 17], "through": [6, 9, 10], "essenti": [6, 10], "approx": [6, 7, 9, 10], "frac": [6, 7, 8, 10, 12], "n": [6, 7, 8, 10, 16, 17, 19, 20, 22, 24], "x_i": 6, "distribut": [6, 7, 8, 10, 12, 13, 14, 19], "suggest": [6, 7, 9], "trajectori": [6, 7, 9, 10, 16, 19], "culmul": 6, "print": [6, 12, 13, 14, 19, 20, 23], "f": [6, 12, 13, 14, 19, 20], "640": 6, "montecarlovalu": 6, "simpl": [6, 19, 22], "stick": 6, "21": 6, "hit": 6, "500000": 6, "plot_valu": 6, "montecarloqvalu": 6, "action1": 6, "q11": 6, "action2": 6, "q12": 6, "state2": 6, "q21": 6, "q22": 6, "gradient": [7, 13, 14, 15, 21], "go": [7, 21], "baselin": 7, "subtract": 7, "j": [7, 8, 10, 11, 13], "theta": [7, 8, 10, 11, 12, 13], "tau": [7, 8, 10, 11, 13, 14], "sim": [7, 8, 10, 11, 13, 16], "p_": [7, 10, 11, 13], "sum_t": 7, "nabla_": [7, 8, 10, 11], "log": [7, 10, 13, 14], "pi_": [7, 8, 10, 11, 12, 13], "mathbf": [7, 8, 9, 10, 11, 12, 13, 16, 17, 19], "_t": [7, 8, 9, 10, 11, 12, 13, 16, 17, 19], "b": [7, 13, 14], "interpret": 7, "term": [7, 9], "singl": [], "If": [7, 23], "addit": [10, 13], "written": [7, 22, 23], "likelihood": 7, "weight": [7, 8, 9], "call": [7, 8, 9, 17, 22], "advantag": [7, 12, 20, 21], "rel": [], "take": [7, 8, 16, 17, 19], "when": [9, 10, 15, 16, 19, 22, 23], "posit": [], "neg": 7, "decreas": [], "question": 7, "parameter": [7, 10, 19], "neural": [7, 8, 10, 19], "network": [7, 8, 10, 13, 14, 19], "train": [8, 10, 12, 13, 14, 17, 19, 20], "turn": [], "out": [21, 24], "decompos": [], "need": [7, 8, 10, 17, 23], "fit": 7, "specifi": [7, 10], "target": [7, 8, 9, 12, 13, 14, 19, 20], "done": [7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20], "approach": [7, 8, 16, 19], "tempor": [9, 19, 21], "discuss": [9, 10, 16, 17], "earlier": 10, "collect": [8, 19], "abov": [9, 10, 11, 17], "particular": 21, "mean": [7, 8, 9, 10, 12, 13, 14, 16, 19, 20, 24], "squar": [7, 19], "loss": [7, 9, 10, 12, 14, 19, 20], "l": [7, 8, 12, 19, 20], "phi": [7, 8, 19], "pybullet": [7, 8, 9, 10, 12, 13, 19, 20], "stabl": [7, 8, 9, 10, 12, 13, 19, 20], "baselines3": [7, 8, 9, 10, 12, 13, 19, 20], "extra": [7, 8, 9, 10, 12, 13, 19, 20], "pyvirtualdisplai": [7, 8, 9, 10, 12, 13, 19, 20], "apt": [7, 8, 9, 10, 12, 13, 19, 20], "xvfb": [7, 8, 9, 10, 12, 13, 19, 20], "pybullet_env": [7, 8, 9, 10, 12, 13, 19, 20], "imageio": [7, 8, 9, 10, 12, 13, 16, 17, 19, 20], "torch": [7, 8, 9, 10, 12, 13, 14, 19, 20], "nn": [7, 8, 9, 10, 12, 13, 14, 19, 20], "util": [7, 8, 9, 10, 12, 13, 14, 19, 20], "data": [7, 8, 9, 10, 12, 13, 14, 19, 20, 24], "dataset": [7, 8, 9, 10, 12, 13, 14, 19, 20], "dataload": [7, 8, 9, 10, 12, 13, 14, 19, 20], "devic": [7, 8, 9, 10, 12, 13, 14, 19, 20], "cuda": [7, 8, 9, 10, 12, 13, 14, 19, 20], "is_avail": [7, 8, 9, 10, 12, 13, 14, 19, 20], "cpu": [7, 8, 9, 10, 12, 13, 14, 19, 20], "modul": [7, 8, 10, 12, 13, 14, 19, 20], "state_dim": [7, 8, 10, 12, 13, 14, 19, 20], "action_dim": [7, 8, 10, 12, 13, 14, 19, 20], "hidden_dim": [7, 8, 10, 12, 13, 14, 19, 20], "128": [7, 8, 9, 10, 12, 13, 14, 19, 20], "super": [7, 8, 10, 12, 13, 14, 19, 20], "net": [7, 8, 10, 12, 13, 14], "sequenti": [12, 13, 14, 20], "relu": [12, 13, 14, 20], "softmax": [7, 10, 12], "forward": [7, 8, 9, 10, 12, 13, 14, 19, 20], "ob": [10, 12, 13, 14, 19], "select_act": [7, 8, 10, 12, 13, 14, 19, 20], "tensor": [7, 8, 9, 10, 12, 13, 14, 19, 20], "action_dist": [7, 10, 12, 13, 14], "categor": [7, 8, 10, 12], "get_action_log_prob": [7, 12], "action_log_prob": [10, 12, 13, 14], "log_prob": [7, 9, 10, 12, 13, 14], "valuenetwork": [7, 9, 13, 14], "in_dim": [7, 8, 10, 19], "out_dim": [7, 8, 10, 13, 14, 19], "squeez": [7, 8, 13, 14], "actorcriticmcag": 7, "lr": [7, 8, 9, 10, 12, 13, 14, 19, 20], "1e": [7, 8, 9, 10, 12, 13, 14, 19, 20], "4": [7, 9, 10, 12, 13, 14, 16, 17, 20, 24], "policy_optim": [7, 8, 9, 12, 13, 14], "adam": [7, 8, 9, 10, 12, 13, 14, 19, 20], "paramet": [7, 8, 9, 10, 12, 13, 14, 16, 19, 20], "value_optim": [7, 9, 12, 13, 14], "policy_schedul": [7, 8, 9, 13, 14], "lr_schedul": [7, 8, 9, 10, 12, 13, 14, 19], "steplr": [7, 8, 9, 10, 12, 13, 14, 19], "step_siz": [7, 8, 9, 10, 12, 13, 14, 19], "1000": [7, 9, 16, 17, 19, 20], "value_schedul": [7, 9, 13, 14], "sample_trajectori": [7, 9, 12], "max_step": [7, 8, 9, 10, 12, 13, 14, 17, 19, 20], "10000": [8, 12, 13, 14, 17, 19, 20], "reset": [8, 10, 12, 13, 14, 16, 17, 19, 20], "no_grad": [7, 8, 9, 12, 13, 14, 19, 20], "next_stat": [8, 10, 12, 13, 14, 19, 20], "learn_episod": [7, 8, 9, 10, 12, 13, 14, 19], "batch_siz": [7, 8, 9, 10, 12, 13, 14, 19, 20], "value_loss": [7, 9, 12, 13, 14], "policy_loss": [7, 8, 9, 12, 13, 14], "rewards_cum": 7, "cumsum": 7, "flip": 7, "dim": [7, 8, 13, 14, 19, 20], "mseloss": [7, 8, 9, 12, 13, 14, 20], "detach": [7, 8, 9, 12, 13, 14], "sum": [7, 9, 12, 13, 14], "zero_grad": [7, 8, 9, 10, 12, 13, 14, 19, 20], "backward": [7, 8, 9, 10, 12, 13, 14, 19, 20], "print_everi": [7, 8, 9, 10, 12, 19, 20], "train_loss": [12, 19, 20], "averag": [8, 9, 12, 19, 20], "cartpolebulletenv": [7, 9, 10, 12, 19, 20], "3500": [7, 9], "50": [7, 8, 12, 13, 14, 20], "actorcritictdag": 7, "determinisitc": [], "propos": [8, 9, 10, 12, 17], "silver": 8, "et": [8, 9, 10, 12, 19], "al": [8, 9, 10, 12, 19], "2015": 8, "At": [], "time": [9, 16, 17, 19], "most": [8, 22], "popular": [], "reinforcemnet": [], "via": [], "improv": [8, 9, 10, 13], "select": 8, "what": 10, "continu": [8, 19], "cannot": 10, "easili": [], "global": [], "maxim": [10, 13], "more": [9, 10, 13, 17, 19, 21, 23, 24], "suitabl": [], "local": [13, 14], "move": [8, 17, 19], "proport": [], "name": [9, 12, 13], "theta_": 12, "theta_k": [], "alpha": [7, 8, 10, 13, 16, 17, 20], "rho_k": [], "mu_": 8, "clip_grad_norm_": [8, 12], "replaybuff": [8, 13, 14, 19, 20], "process": [8, 10, 13, 19], "maxlen": [8, 13, 14, 19, 20], "buffer": [8, 13, 14, 19, 20], "dequ": [8, 14, 19, 20], "reset_st": 8, "fill": [14, 19, 20], "__len__": [14, 19, 20], "clip": 8, "add": [8, 13, 14, 19, 20], "n_sampl": [10, 14, 19, 20], "zip": [8, 9, 12, 13, 14, 19, 20], "tanh": [13, 14], "shape": [8, 13, 14, 19, 20], "unsqueez": [8, 13, 14, 19, 20], "action_rang": [13, 14], "qvaluenetwork": [8, 13, 14], "cat": [8, 13, 14], "ornsteinuhlenbeckprocess": 8, "mu": 8, "sigma": 8, "15": [8, 12], "dt": 8, "x0": 8, "x_prev": 8, "sqrt": 8, "normal": [8, 9, 12, 13, 14], "zeros_lik": 8, "deterministicpolicygradientag": 8, "observation_spac": [8, 13, 14, 16, 17, 19, 20], "creat": [8, 9, 24], "replica": 8, "target_polici": 8, "target_q_valu": 8, "load_state_dict": [8, 12], "state_dict": [8, 12], "q_value_optim": 8, "9": [8, 10, 13, 14, 19], "q_value_schedul": 8, "ou_process": 8, "2000": [8, 12, 13, 14], "grad_clip_valu": 8, "ou": [], "episode_reward": [8, 10, 12, 13, 14, 19, 20], "train_q_value_loss": [], "train_policy_loss": [13, 14], "batch": [7, 8, 10, 13, 14, 15, 19], "q_value_loss": 8, "target_param": [8, 13, 14], "copy_": [8, 13, 14], "evalu": [8, 13, 14, 19], "eval": 8, "invertedpendulumbulletenv": [8, 13, 14], "v0": [8, 13, 14, 16, 17], "005": [8, 13, 14], "500": [8, 12, 17], "99": [8, 13, 14], "latter": 9, "usual": [], "give": [7, 8, 10, 17, 21], "lower": [7, 9], "varianc": [7, 9, 10, 19], "unbias": [7, 9], "wherea": [9, 22], "between": [17, 19], "want": [10, 13, 24], "one": [7, 8, 9, 10, 19, 22], "best": [], "bia": 9, "tradeoff": 9, "look": 9, "2r": 9, "clear": 9, "schulman": [9, 12], "exponenti": [8, 9], "gae": 9, "lambda": 9, "serv": [7, 9, 22], "express": [8, 9, 10], "delta_t": 9, "r_t": [8, 9, 16, 17], "denot": [9, 11], "residu": 9, "reexpress": 9, "2r_": 9, "delta_": 9, "moreov": 22, "recurs": 9, "a_t": [9, 12], "effici": [9, 19], "back": [], "recov": 9, "high": [9, 13], "low": [], "set": [8, 16, 17, 19], "control": [8, 9, 16], "tune": 9, "design": 9, "trade": 9, "expens": 9, "test": [7, 8, 9, 10, 17], "advntag": [], "cartpol": [7, 10, 12, 19, 20], "actorcriticgaeag": 9, "lambda_": [9, 12], "estimate_advantag": [9, 12], "next_valu": [9, 12], "revers": [9, 12], "delta": [9, 12], "insert": [9, 12, 22], "std": [9, 12, 13, 14], "8": [9, 10, 12, 19], "00": 9, "over": [13, 14, 19, 20], "cumul": [], "underbrac": 10, "simplic": [], "acsend": [], "attempt": [], "tau_i": 10, "mont": [8, 10, 16, 17], "carlo": [8, 10, 16, 17], "wouldn": [], "work": [9, 24], "noth": [], "backpropag": [], "issu": [8, 10], "slightli": [9, 10], "d": [10, 12, 19, 23], "next": [10, 16], "trick": 10, "further": [7, 10], "_0": [10, 11, 13, 16, 17], "prod_": 10, "approxim": [10, 17, 19], "present": [7, 8, 10, 11], "differenti": [7, 8, 10], "rate": [7, 8, 10, 17], "gener": [7, 10, 13, 16, 17, 20, 21], "reinforceag": 10, "schedul": [10, 12, 19], "total_log_prob": [], "total_reward": 19, "evaluate_n_episod": [19, 20], "vari": 10, "experi": [8, 10], "them": 10, "300": [10, 20], "decai": 10, "plot": [10, 12, 20, 24], "model": [8, 10], "partli": 10, "fact": 10, "larger": [10, 19], "smaller": 10, "nois": [8, 10], "explan": [10, 17], "address": [8, 10, 19], "drawback": [12, 16], "trust": 12, "region": 12, "difficult": 12, "2017": 12, "variant": [12, 19], "trpo": 12, "ppo": 12, "unlik": [8, 12], "old": 12, "subject": 12, "kl": 12, "diverg": 12, "hard": [7, 12], "constraint": [12, 13], "soft": [8, 12, 21], "beta": 12, "ppoagent": 12, "ep": 12, "policy_old": 12, "episode_loss": [8, 10, 12, 19], "critic": [9, 12, 21], "old_log_prob": [12, 13, 14], "new_log_prob": [12, 13, 14], "ratio": 12, "exp": [12, 13, 14], "surr1": 12, "surr2": 12, "clamp": 12, "min": [12, 13, 14], "load": [12, 20], "user": [12, 20], "raymondtsao": [12, 20], "desktop": [12, 20], "reinforcementlearningnot": [12, 20], "policygradi": 12, "e005_reward": 12, "npy": [12, 20], "e010_reward": 12, "e015_reward": 12, "e020_reward": 12, "e0": 12, "05": 12, "color": [12, 20, 24], "smooth": [12, 20], "smoothed_reward": [12, 20], "start_index": [12, 20], "c": [12, 20], "label": [11, 12], "titl": [12, 20], "xlabel": [12, 20], "ylabel": [12, 20], "legend": [12, 24], "haarnoja": [], "2018": [], "off": [21, 22, 23], "log_action_prob": [13, 14], "log_std": [13, 14], "chunk": [13, 14], "try": [13, 14], "randn_lik": [13, 14], "pow": [13, 14, 19], "6": [13, 14, 16], "keepdim": [13, 14, 20], "dimens": [10, 13, 14], "except": [13, 14], "occur": [13, 14], "rais": [7, 13, 14], "softactorcriticag": [13, 14], "target_valu": [13, 14], "q_value1": [13, 14], "q_value2": [13, 14], "q_value_optimizer1": [13, 14], "q_value_optimizer2": [13, 14], "150": [13, 14], "7": [13, 14], "q_value_scheduler1": [13, 14], "q_value_scheduler2": [13, 14], "train_value_loss": [13, 14], "train_q_value1_loss": [13, 14], "train_q_value2_loss": [13, 14], "prevent": [13, 14], "track": [13, 14], "old_stat": [13, 14], "old_act": [13, 14], "new_act": [13, 14, 17], "new_q_value1": [13, 14], "new_q_value2": [13, 14], "new_q_valu": [13, 14], "retain_graph": [13, 14], "old_q_value1": [13, 14], "old_q_value2": [13, 14], "q_value_loss1": [13, 14], "q_value_loss2": [13, 14], "q_value1_loss": 14, "q_value2_loss": 14, "reinforc": [8, 15, 19], "small": [15, 21, 22], "larg": [15, 19], "g_t": 16, "happen": 16, "could": 16, "less": [16, 17, 19], "desir": 16, "lot": [16, 22, 24], "dure": [8, 16, 19], "meaning": 16, "progress": 16, "mathemat": 16, "td": [16, 17], "ha": [7, 9, 16], "immedi": 16, "provid": [9, 11, 16, 17], "arbitrari": [16, 17], "termin": [16, 17], "observ": [16, 17], "extend": 16, "48": 16, "arriv": 16, "right": 16, "bottom": 16, "corner": 16, "reciev": 16, "penalti": 16, "live": 16, "katex": 16, "figur": [16, 17], "environ": [7, 10, 16, 17, 19], "estimate_value_funct": 16, "get_act": [16, 17], "new_stat": [16, 17], "info": [16, 17, 19], "47": [16, 17], "randompolici": 16, "cliffwalk": [16, 17], "action_spac": [16, 17, 19, 20], "run": [9, 16, 23], "seem": 16, "shown": [9, 16, 17, 19], "black": 16, "box": [16, 22], "much": [9, 16], "wors": [7, 16], "compar": [16, 19], "far": [16, 17], "awai": [16, 17], "sens": 16, "higher": 16, "chanc": 16, "fall": [16, 17], "cover": 17, "help": [8, 17, 19, 22], "idea": 17, "setup": 17, "adopt": 17, "strategi": 17, "cliff": 17, "walk": 17, "patch": 17, "polygon": 17, "step_count": 17, "learnedpolici": 17, "action_valu": 17, "3000": 17, "ran": 17, "epoch": [9, 17], "As": [17, 24], "close": [17, 19, 20], "prefer": 17, "path": [17, 23], "safer": 17, "modifi": 17, "place": 17, "offlin": 17, "version": 17, "1989": 17, "watkin": 17, "bootstrap": 17, "becaus": [9, 10, 17, 19], "q_learn": 17, "conserv": 17, "timestep": 17, "complet": 17, "hand": [9, 17], "riski": 17, "risk": 17, "thorough": 17, "210": [], "160": [], "randint": [19, 20], "qlearningagentwithoutbuff": 19, "evaluation_n_episod": 19, "agent_without_buff": 19, "qlearningagentwithbuff": 19, "buffer_max_len": 19, "element": [19, 20], "state_batch": [19, 20], "action_batch": [19, 20], "reward_batch": [19, 20], "next_state_batch": [19, 20], "done_batch": [19, 20], "up": [19, 20], "agent_with_buff": 19, "preprocess": 20, "qlearn": 20, "train_epoch": 20, "ql": 20, "5e": 20, "valuebasedmethod": 20, "duelq": 20, "you": [21, 22, 23, 24], "feel": 21, "content": [21, 22, 23], "structur": [21, 22], "few": 21, "major": 21, "file": [21, 23], "type": [7, 21], "well": [21, 24], "doe": [19, 21], "depth": 21, "topic": 21, "document": [21, 22, 23, 24], "inform": [11, 19, 21, 23, 24], "page": [21, 22, 23], "bundl": 21, "deep": 21, "duel": 21, "actor": [9, 21], "proxim": 21, "write": [22, 23], "your": [22, 23, 24], "book": [22, 23, 24], "jupyt": [22, 23, 24], "notebook": 22, "ipynb": 22, "regular": [10, 22], "md": [22, 23], "ll": [10, 22], "flavor": 22, "syntax": 22, "stand": 22, "markedli": 22, "slight": 22, "variat": [9, 22], "commonmark": 22, "extens": 22, "sphinx": 22, "ecosystem": 22, "overview": 22, "power": 22, "tool": 22, "thei": [8, 22], "markup": 22, "languag": 22, "purpos": 22, "span": 22, "mani": [22, 23], "accept": 22, "kind": 22, "those": 22, "specif": [10, 19, 22], "here": [22, 24], "render": 22, "special": 22, "build": 22, "inlin": 22, "refer": 22, "cite": 22, "store": [8, 19, 22], "bibtex": 22, "exampl": [19, 22, 24], "holdgraf_evidence_2014": 22, "hdhpk14": 22, "bibliographi": 22, "properli": 22, "bib": 22, "christoph": 22, "ramsai": 22, "holdgraf": 22, "wendi": 22, "de": 22, "heer": 22, "brian": 22, "paslei": 22, "robert": 22, "knight": 22, "evid": [19, 22], "predict": 22, "human": 22, "auditori": 22, "cortex": 22, "intern": 22, "confer": 22, "cognit": 22, "neurosci": 22, "brisban": 22, "australia": 22, "2014": 22, "frontier": 22, "just": 22, "starter": 22, "jupyterbook": 22, "org": 22, "base": [7, 10, 23], "detail": 23, "instruct": 23, "With": [11, 13, 23], "direct": 23, "built": 23, "block": 23, "kernel": 23, "rest": 23, "jupytext": 23, "convert": 23, "support": 23, "thing": 23, "understand": 23, "includ": [10, 23, 24], "top": 23, "presenc": 23, "That": 23, "treat": 23, "command": 23, "init": 23, "markdownfil": 23, "emb": 24, "imag": 24, "html": 24, "etc": 24, "post": 24, "add_": 24, "math": 24, "mbox": 24, "la_": 24, "tex": 24, "But": [19, 24], "sure": 24, "escap": 24, "dollar": 24, "sign": 24, "keep": 24, "guid": 24, "rcparam": 24, "cycler": 24, "ion": 24, "reproduc": 24, "19680801": 24, "logspac": 24, "randn": 24, "ii": 24, "cmap": 24, "cm": 24, "coolwarm": 24, "ax": 24, "prop_cycl": 24, "linspac": 24, "line2d": 24, "custom_lin": 24, "lw": 24, "fig": 24, "subplot": 24, "figsiz": 24, "cold": 24, "medium": 24, "hot": 24, "There": 24, "contrast": [10, 19], "without": [10, 19], "explicitli": 10, "formal": [10, 11], "_1": [10, 11], "induc": 10, "potenti": 10, "troubl": 10, "part": 10, "variabl": 10, "reformul": 10, "befor": [7, 10, 11, 19], "mention": [10, 11], "unrol": 10, "interchang": 10, "integr": 10, "statement": 10, "sutton": 10, "1999": 10, "remark": 10, "pseudocod": 10, "layer": [7, 8, 10], "hidden": [7, 8, 10], "simpleneuralnetwork": [7, 8, 10, 19], "final_lay": [7, 10], "inherit": 10, "pre": 10, "along": 10, "contain": [10, 19], "record": 10, "metric": 10, "rather": 11, "origin": 11, "paper": [8, 11], "worth": 11, "author": [8, 9, 11], "mind": 11, "cqlagent": 14, "my": [], "subspac": [], "subset": [], "uniqu": [], "minim": [8, 19], "argmin_": [], "z": [], "perp": [], "orthogon": [], "project": [], "onto": [], "reduc": [7, 9], "even": 7, "though": 7, "formula": [7, 9], "heurist": 7, "typic": [7, 9, 19], "natrual": 7, "postiv": 7, "would": 7, "convers": 7, "still": [7, 8, 9, 13], "achiev": [7, 8, 9], "primari": 7, "error": [7, 19], "wait": 7, "until": 7, "entir": 7, "been": 7, "calcul": 7, "altern": 7, "increment": 7, "full": 7, "either": 7, "4mm": 7, "unit": [7, 8], "3mm": 7, "often": [9, 19], "grow": 9, "linearli": 9, "second": 9, "bias": 9, "possibl": [9, 13, 19], "effect": [9, 19], "balanc": 9, "2016": 9, "combin": 9, "strength": 9, "offer": 9, "scalar": 9, "whera": 9, "wise": 9, "easi": 9, "recurr": 9, "lamb": 9, "graph": 9, "appear": 9, "fastest": 9, "exhibit": [9, 19], "signific": 9, "On": 9, "although": 9, "slower": 9, "demonstr": [9, 19], "stabil": [8, 9, 19], "long": 9, "dpg": 8, "wa": 8, "tradit": 8, "stochast": 8, "produc": 8, "gaussian": 8, "focus": 8, "analog": 8, "determinst": 8, "dircet": 8, "chain": 8, "subsequ": 8, "lillicrap": 8, "lear": [], "n_layer": [8, 19], "qvalu": 8, "replai": 8, "explor": 13, "minibatch": 8, "_i": [8, 19], "r_i": [8, 19], "y_i": 8, "nabla": 8, "nabla_a": 8, "style": 8, "enhanc": [8, 19], "avoid": 8, "unvisit": 8, "ad": 8, "chosen": 8, "suit": 8, "ornstein": 8, "uhlenbeck": 8, "dx_t": 8, "x_tdt": 8, "dw_t": 8, "invers": 8, "pendulum": 8, "task": [8, 19], "soft_upd": 8, "target_model": 8, "decis": 13, "formul": 13, "h": 13, "encourag": 13, "robust": 13, "framework": 13, "involv": 13, "thereaft": 19, "discret": 19, "straightforward": 19, "tabl": 19, "becom": 19, "infeas": 19, "deal": 19, "race": 19, "car": 19, "robot": 19, "arm": 19, "infinit": 19, "unbound": 19, "bin": 19, "complex": 19, "lead": 19, "explod": 19, "accuraci": 19, "overcom": 19, "techniqu": 19, "core": 19, "assumpt": 19, "supervis": 19, "independ": 19, "ident": 19, "correl": 19, "especi": 19, "within": 19, "ineffici": 19, "poor": 19, "rl": 19, "particularli": 19, "discard": 19, "reus": 19, "multipl": 19, "mai": 19, "fulli": 19, "exploit": 19, "mnih": 19, "concept": 19, "consecut": 19, "break": 19, "incorpor": 19, "architectur": 19, "hyperparamet": 19, "tri": 19, "ne": 19, "b8": 19, "b32": 19, "significantli": 19, "quickli": 19, "curv": 19, "acceler": 19}, "objects": {}, "objtypes": {}, "objnames": {}, "titleterms": {"markov": [0, 21], "decis": [0, 21], "process": [0, 21], "polici": [1, 2, 5, 7, 8, 10, 11, 12, 14, 16, 21], "iter": [1, 2], "evalu": [1, 16], "e": 1, "improv": 1, "i": [1, 22], "lemma": 1, "valu": [2, 6, 7, 8, 16, 21], "gridworld": 2, "environ": 2, "implement": [2, 7, 8, 9, 10, 12, 13, 14], "learn": [2, 17, 19, 20, 22], "optim": [2, 14], "theoret": 3, "analysi": 3, "mont": [4, 5, 6, 7], "carlo": [4, 5, 6, 7], "tree": 4, "search": 4, "control": [5, 17], "explor": [5, 8], "start": 5, "stochast": 5, "estim": [6, 9, 16], "first": 6, "visit": 6, "v": 6, "": 6, "everi": 6, "blackjack": 6, "function": [6, 7, 8, 16], "q": [6, 17, 19, 20], "actor": [7, 13], "critic": [7, 13], "method": [7, 8, 10, 21], "approxim": 7, "td": 7, "algorithm": 7, "mc": [], "determinist": 8, "gradient": [8, 10, 11, 12], "gener": 9, "advantag": 9, "reinforc": 10, "theorem": [10, 11], "proxim": 12, "soft": 13, "varianc": 15, "reduct": 15, "reward": 15, "go": 15, "baselin": 15, "subtract": 15, "tempor": [7, 16, 17], "differ": [7, 16, 17], "cliff": 16, "walk": 16, "sarsa": 17, "deep": [8, 19], "vanilla": 19, "experi": 19, "replai": 19, "duel": 20, "welcom": 21, "your": 21, "jupyt": 21, "book": 21, "A": 21, "b": 21, "base": 21, "c": 21, "markdown": [22, 23, 24], "file": 22, "what": 22, "myst": [22, 23, 24], "sampl": 22, "role": 22, "direct": 22, "citat": 22, "more": 22, "notebook": [23, 24], "an": 23, "exampl": 23, "cell": 23, "creat": 23, "quickli": 23, "add": 23, "yaml": 23, "metadata": 23, "content": 24, "code": 24, "block": 24, "output": 24, "inform": 10, "trust": 14, "region": 14, "ddpg": 8, "reep": [], "action": 8, "updat": 8, "maximum": 13, "entropi": 13, "object": 13}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinxcontrib.bibtex": 9, "sphinx": 60}, "alltitles": {"Markov decision process": [[0, "markov-decision-process"]], "Policy iteration": [[1, "policy-iteration"], [1, "id1"]], "Policy evaluation (E)": [[1, "policy-evaluation-e"]], " (Policy-evaluation)": [[1, "simple-algorithm"]], "Policy improvement (I)": [[1, "policy-improvement-i"]], " (Policy-improvement-lemma)": [[1, "my-theorem"]], " (Policy-iteration)": [[1, "simple-algorithm"]], "Value iteration": [[2, "value-iteration"]], " (Value iteration)": [[2, "simple-algorithm"]], "GridWorld environment": [[2, "gridworld-environment"]], "Value iteration implementation": [[2, "value-iteration-implementation"]], "Learning the optimal policy": [[2, "learning-the-optimal-policy"]], "Theoretic analysis": [[3, "theoretic-analysis"]], "": [[3, "my-theorem"], [3, "my-theorem"]], "Monte Carlo tree search": [[4, "monte-carlo-tree-search"]], "Monte Carlo control": [[5, "monte-carlo-control"]], "Monte Carlo with exploring starts": [[5, "monte-carlo-with-exploring-starts"]], "Monte Carlo with stochastic policies": [[5, "monte-carlo-with-stochastic-policies"]], "Value estimation": [[6, "value-estimation"]], "First-visit v.s. every visit Monte Carlo": [[6, "first-visit-v-s-every-visit-monte-carlo"]], "BlackJack": [[6, "blackjack"]], "Estimating of value functions": [[6, "estimating-of-value-functions"]], "Estimating of q-value function": [[6, "estimating-of-q-value-function"]], "Actor critic methods": [[7, "actor-critic-methods"]], "Monte Carlo approximation": [[7, "monte-carlo-approximation"]], "Temporal difference approximation": [[7, "temporal-difference-approximation"]], "Actor critic algorithm": [[7, "actor-critic-algorithm"]], " (Actor critic algorithm)": [[7, "my-algorithm"]], "Actor critic approximation": [[7, "actor-critic-approximation"]], "Policy and value function implementation": [[7, "policy-and-value-function-implementation"]], "Actor critic with Monte Carlo implementation": [[7, "actor-critic-with-monte-carlo-implementation"]], "Actor critic with TD implementation": [[7, "actor-critic-with-td-implementation"]], "Generalized advantage estimation": [[9, "generalized-advantage-estimation"]], "Generalized advantage estimator implementation": [[9, "generalized-advantage-estimator-implementation"]], " (REINFORCE)": [[10, "my-algorithm"]], "Policy gradient method": [[10, "policy-gradient-method"]], "Policy gradient theorem (Informal)": [[10, "policy-gradient-theorem-informal"]], "REINFORCE Implementation": [[10, "reinforce-implementation"]], "Policy gradient theorem": [[11, "policy-gradient-theorem"]], "Proximal policy gradient": [[12, "proximal-policy-gradient"]], "Implementation": [[12, "implementation"], [14, "implementation"], [13, "implementation"]], "Trust region policy optimization": [[14, "trust-region-policy-optimization"]], "Variance reduction": [[15, "variance-reduction"]], "Reward to go": [[15, "reward-to-go"]], "Baseline subtraction": [[15, "baseline-subtraction"]], "Temporal difference": [[16, "temporal-difference"]], " (Temporal difference policy evaluation)": [[16, "my-algorithm"]], "Cliff walking": [[16, "cliff-walking"]], "Estimating value function": [[16, "estimating-value-function"]], "Temporal difference control": [[17, "temporal-difference-control"]], "SARSA": [[17, "sarsa"]], " (SARSA)": [[17, "my-algorithm"]], "Q-Learning": [[17, "q-learning"]], " (Q-learning)": [[17, "my-algorithm"]], "Dueling Q Learning": [[20, "dueling-q-learning"]], "Welcome to your Jupyter Book": [[21, "welcome-to-your-jupyter-book"]], "A. Markov decision process": [[21, null]], "B. Value based methods": [[21, null]], "C. Policy based methods": [[21, null]], "Markdown Files": [[22, "markdown-files"]], "What is MyST?": [[22, "what-is-myst"]], "Sample Roles and Directives": [[22, "sample-roles-and-directives"]], "Citations": [[22, "citations"]], "Learn more": [[22, "learn-more"]], "Notebooks with MyST Markdown": [[23, "notebooks-with-myst-markdown"]], "An example cell": [[23, "an-example-cell"]], "Create a notebook with MyST Markdown": [[23, "create-a-notebook-with-myst-markdown"]], "Quickly add YAML metadata for MyST Notebooks": [[23, "quickly-add-yaml-metadata-for-myst-notebooks"]], "Content with notebooks": [[24, "content-with-notebooks"]], "Markdown + notebooks": [[24, "markdown-notebooks"]], "MyST markdown": [[24, "myst-markdown"]], "Code blocks and outputs": [[24, "code-blocks-and-outputs"]], "Deterministic policy gradient method": [[8, "deterministic-policy-gradient-method"]], "Deep deterministic policy gradient": [[8, "deep-deterministic-policy-gradient"]], " (Deep deterministic policy gradient)": [[8, "my-algorithm"]], "Action value function update": [[8, "action-value-function-update"]], "Exploration": [[8, "exploration"]], "DDPG Implementation": [[8, "ddpg-implementation"]], "Soft actor critic": [[13, "soft-actor-critic"]], "Maximum entropy objective": [[13, "maximum-entropy-objective"]], "Deep Q Learning": [[19, "deep-q-learning"]], "Vanilla deep Q Learning": [[19, "vanilla-deep-q-learning"]], "Deep Q Learning with experience replay": [[19, "deep-q-learning-with-experience-replay"]]}, "indexentries": {}})