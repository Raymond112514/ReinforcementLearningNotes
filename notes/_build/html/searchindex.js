Search.setIndex({"docnames": ["MDP/Markov-decision-process", "MDP/Policy-iteration", "MDP/Reinforcement-learning", "MDP/Value-iteration", "MDP/Value-iteration-convergence", "MonteCarlo/Monte-Carlo-Control", "MonteCarlo/Monte-Carlo-value-estimation", "PolicyGradient/Actor-critic-methods", "PolicyGradient/Deterministic-policy-gradient-method", "PolicyGradient/Generalized-advantage-estimation", "PolicyGradient/Policy-gradient-method", "PolicyGradient/Policy-gradient-theorem", "PolicyGradient/Proximal-policy-gradient", "PolicyGradient/Soft-actor-critic", "PolicyGradient/TRPO", "PolicyGradient/Variance-reduction", "TD/Temporal-difference", "TD/Temporal-difference-control", "TD/Untitled", "ValueBasedMethods/Deep-Q-Learning", "ValueBasedMethods/Double-Q-Learning", "ValueBasedMethods/Dueling-Q-Learning", "intro", "markdown", "markdown-notebooks", "notebooks"], "filenames": ["MDP/Markov-decision-process.ipynb", "MDP/Policy-iteration.ipynb", "MDP/Reinforcement-learning.ipynb", "MDP/Value-iteration.ipynb", "MDP/Value-iteration-convergence.ipynb", "MonteCarlo/Monte-Carlo-Control.ipynb", "MonteCarlo/Monte-Carlo-value-estimation.ipynb", "PolicyGradient/Actor-critic-methods.ipynb", "PolicyGradient/Deterministic-policy-gradient-method.ipynb", "PolicyGradient/Generalized-advantage-estimation.ipynb", "PolicyGradient/Policy-gradient-method.ipynb", "PolicyGradient/Policy-gradient-theorem.ipynb", "PolicyGradient/Proximal-policy-gradient.ipynb", "PolicyGradient/Soft-actor-critic.ipynb", "PolicyGradient/TRPO.ipynb", "PolicyGradient/Variance-reduction.ipynb", "TD/Temporal-difference.ipynb", "TD/Temporal-difference-control.ipynb", "TD/Untitled.ipynb", "ValueBasedMethods/Deep-Q-Learning.ipynb", "ValueBasedMethods/Double-Q-Learning.ipynb", "ValueBasedMethods/Dueling-Q-Learning.ipynb", "intro.md", "markdown.md", "markdown-notebooks.md", "notebooks.ipynb"], "titles": ["<span class=\"section-number\">1. </span>Markov decision process", "<span class=\"section-number\">3. </span>Policy iteration", "<span class=\"section-number\">4. </span>Reinforcement Learning", "<span class=\"section-number\">2. </span>Value iteration", "<span class=\"section-number\">2.3. </span>Convergence analysis", "<span class=\"section-number\">5.5. </span>Monte Carlo control", "<span class=\"section-number\">5. </span>Value estimation", "<span class=\"section-number\">11. </span>Actor critic methods", "<span class=\"section-number\">15. </span>Deterministic policy gradient method", "<span class=\"section-number\">12. </span>Generalized advantage estimation", "<span class=\"section-number\">10. </span>Policy gradient method", "Policy gradient theorem", "<span class=\"section-number\">14. </span>Proximal policy gradient", "<span class=\"section-number\">16. </span>Soft actor critic", "<span class=\"section-number\">13. </span>Trust region policy optimization", "Variance reduction", "<span class=\"section-number\">6. </span>Temporal difference", "<span class=\"section-number\">6.3. </span>Temporal difference control", "&lt;no title&gt;", "<span class=\"section-number\">7. </span>Deep Q Learning", "<span class=\"section-number\">8. </span>Double Q Learning", "<span class=\"section-number\">9. </span>Dueling Q Learning", "Welcome to your Jupyter Book", "Markdown Files", "Notebooks with MyST Markdown", "Content with notebooks"], "terms": {"A": [0, 1, 3, 4, 5, 6, 7, 8, 9, 11, 14, 17, 19, 20, 21], "mdp": [0, 1, 2, 3, 11, 13], "model": [0, 8, 10, 21], "contain": [0, 1, 10, 19], "four": 0, "element": [0, 19], "state": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21, 25], "space": [0, 3, 14, 19], "mathcal": [0, 1, 3, 4, 8, 11, 12, 13, 14, 16, 17, 19, 20, 21], "": [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21, 23, 24, 25], "transit": [0, 2, 3, 6, 8, 19], "dynam": [0, 2, 3, 6], "p": [0, 1, 2, 3, 4, 6, 8, 10, 11, 14], "mathbf": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "r": [0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 19, 20], "reward": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "an": [0, 1, 2, 3, 6, 7, 9, 10, 13, 14, 17, 19, 20, 23], "agent": [0, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 19, 20, 21], "start": [0, 1, 6, 9, 14, 16, 23, 24], "take": [0, 5, 7, 8, 16, 17, 19, 21], "reciev": [0, 3, 16], "arriv": [0, 16], "new": [0, 1, 13, 14], "accord": [0, 1, 4], "If": [0, 5, 7, 14, 20, 24], "we": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "continu": [0, 8, 19], "thi": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21, 22, 23, 24, 25], "get": [0, 1, 3, 5, 7, 8, 9, 10, 12, 13, 14, 17, 19, 20, 21, 23, 24], "trajectori": [0, 6, 7, 9, 10, 13, 14, 16, 19], "tau": [0, 3, 7, 8, 10, 11, 13, 14, 20], "_1": [0, 3, 10, 11], "r_1": 0, "_2": [0, 3], "r_2": 0, "which": [0, 3, 4, 5, 7, 8, 9, 10, 12, 13, 14, 16, 19, 20, 21, 24], "could": [0, 2], "potenti": [0, 6, 10, 14], "goe": [0, 20], "forev": 0, "given": [0, 1, 2, 3, 4, 5, 7, 11, 13, 14, 17, 21], "ani": [0, 1, 3, 4, 5, 6, 11, 22, 24], "defin": [0, 3, 4, 5, 7, 8, 20, 21, 24], "associ": [0, 3, 13], "sum_": [0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 21], "t": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, 19, 23, 25], "geq": [0, 1, 3, 7, 9, 10, 11, 13, 14], "0": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21, 25], "gamma": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "r_t": [0, 8, 9, 12, 14, 16, 17], "where": [0, 3, 6, 7, 8, 9, 10, 12, 14, 16, 19, 21], "1": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 16, 17, 19, 20, 21, 25], "denot": [0, 3, 9, 11, 14, 16], "discount": [0, 3, 5, 6, 14, 16, 17], "factor": [0, 3, 5, 6, 9, 17], "prevent": [0, 12, 13], "diverg": [0, 12], "natur": [0, 6], "our": [0, 1, 3, 5, 7, 8, 14, 16, 17, 19, 20], "goal": [0, 3, 6, 13, 19], "i": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25], "find": [0, 2, 3, 6, 7, 9, 10, 13, 14, 20], "polici": [0, 2, 6, 9, 17, 19, 21], "maxim": [0, 2, 3, 10, 13, 14, 20, 21], "pi": [0, 1, 3, 5, 6, 11, 13, 14, 16, 17], "map": [0, 3, 4, 8], "distribut": [0, 6, 7, 8, 10, 12, 13, 14, 19], "over": [0, 6, 13, 19, 21], "follow": [0, 1, 2, 3, 4, 5, 7, 8, 10, 12, 13, 14, 16, 17, 19, 20, 21, 23, 24], "along": [0, 10], "induc": [0, 10], "properti": [0, 3, 19], "can": [0, 1, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 19, 20, 21, 23, 24, 25], "express": [0, 8, 9, 10, 14], "p_": [0, 3, 7, 10, 11, 13, 14], "_0": [0, 3, 10, 11, 13, 16, 17], "prod_": [0, 10], "_t": [0, 2, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19], "_": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "also": [0, 3, 5, 6, 9, 10, 14, 19, 23, 24, 25], "expect": [0, 3, 6, 8, 9, 10, 13, 14, 16, 17, 19], "eta": [0, 14], "mathbb": [0, 3, 4, 6, 7, 8, 10, 11, 12, 13, 14], "e": [0, 3, 6, 7, 8, 10, 11, 12, 13, 14, 17], "sim": [0, 3, 7, 8, 10, 11, 12, 13, 14, 16], "The": [0, 1, 3, 5, 6, 7, 8, 9, 10, 13, 14, 16, 17, 19, 20, 21, 23, 24], "two": [0, 1, 2, 4, 5, 6, 7, 8, 9, 13, 14, 20, 23, 24], "quantiti": 0, "particular": [0, 20, 21, 22], "interest": 0, "ar": [0, 2, 3, 5, 6, 7, 10, 13, 14, 16, 17, 19, 20, 21, 23, 24], "v": [0, 1, 3, 4, 7, 9, 10, 11, 13, 16, 21], "q": [0, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 22], "from": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 25], "v_": [0, 1, 3, 4, 6, 7, 13, 16], "By": [0, 1, 3, 8, 9], "appli": [0, 1, 3, 6, 8, 9, 10], "law": [0, 3], "total": [0, 3, 6, 14], "see": [0, 3, 9, 10, 14, 17, 22, 23, 24, 25], "relat": 0, "begin": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14, 25], "align": [0, 1, 3, 4, 7, 8, 9, 10, 11, 13, 14, 25], "end": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 25], "sinc": [0, 1, 3, 4, 6, 9, 14, 16, 17, 21], "optim": [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 12, 13, 17, 19, 20, 21, 22], "prior": 0, "equival": 0, "all": [0, 1, 2, 3, 9, 12, 16, 17, 20, 23, 24], "other": [0, 3, 4, 7, 8, 9, 16, 17, 24], "hand": [0, 3, 5, 6, 9, 17], "q_": [0, 1, 3, 6, 8, 11, 13, 19], "probabl": [0, 5, 6, 7], "again": [0, 6], "cdot": [0, 4, 12, 13, 14, 16, 17], "In": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21, 23], "next": [0, 6, 10, 16], "section": [0, 2, 6, 9, 10, 11, 16, 17], "introduc": [0, 4, 8, 9, 19, 21], "simpl": [0, 19, 23], "algorithm": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 16, 17, 19, 20, 21], "estim": [0, 1, 3, 4, 5, 7, 8, 10, 13, 14, 15, 16, 17, 20, 22], "us": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17, 19, 20, 21, 23, 24], "anoth": [1, 5, 7, 12, 14, 20, 21], "wai": [1, 5, 6, 8, 10, 12, 17, 21], "solv": [1, 2, 3, 12, 14, 19, 21], "problem": [1, 2, 5, 6, 8, 10, 12, 14, 15, 21], "instead": [1, 9, 10, 12, 14, 16, 17, 19, 21], "learn": [1, 7, 8, 9, 10, 12, 13, 14, 22], "valu": [1, 2, 5, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "function": [1, 2, 3, 4, 5, 6, 9, 10, 13, 14, 16, 17, 19, 20, 21, 23], "random": [1, 3, 5, 8, 10, 13, 16, 17, 19, 25], "gradual": [1, 14], "imporv": 1, "step": [1, 5, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "current": [1, 6, 14, 17], "pi_k": 1, "updat": [1, 2, 3, 4, 7, 10, 12, 13, 14, 16, 17, 19, 20], "pi_": [1, 5, 7, 8, 10, 11, 12, 13, 14], "k": [1, 4, 7, 8, 9, 10, 14], "increas": [1, 5, 6, 8, 10], "bellman": [1, 2, 3, 4, 13], "equat": [1, 3, 4, 8, 9], "have": [1, 3, 4, 5, 6, 7, 8, 9, 14, 16, 19, 20, 24], "r_0": [1, 3, 4, 10], "approach": [1, 3, 5, 6, 7, 8, 12, 16, 19], "similar": [1, 3, 5, 16, 17, 23], "psuedocod": [1, 7, 8, 14, 16, 17, 20], "prsent": 1, "below": [1, 3, 5, 6, 7, 8, 9, 10, 14, 16, 17, 19, 20, 21], "note": [1, 4, 5, 7, 9, 10, 12, 14, 16, 17, 23], "onli": [1, 8, 16, 19], "differ": [1, 5, 6, 9, 14, 22, 23], "rule": [1, 8, 10, 14, 17, 19], "input": [1, 3, 7, 8, 10, 13, 14, 16, 17, 20, 23], "instanc": [1, 2, 3, 5, 6, 11], "output": [1, 3, 8, 16, 17, 24], "comput": [1, 2, 3, 6, 7, 8, 9, 10, 14, 16, 19], "initi": [1, 3, 5, 8, 10, 14, 16, 17], "arrai": [1, 3, 8, 25], "while": [1, 2, 3, 6, 7, 8, 9, 10, 12, 13, 16, 17, 19, 20, 21], "converg": [1, 2, 3, 14, 16, 19], "leftarrow": [1, 3, 4, 7, 8, 10, 13, 14, 16, 17, 20], "consid": [1, 10, 12, 14, 21], "action": [1, 2, 3, 5, 7, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "fucntion": [1, 3, 5, 6, 21], "intuit": [1, 8, 12, 20, 21], "exist": [1, 4], "Then": [1, 14], "better": [1, 5, 7, 14], "payoff": [1, 6], "choos": [1, 7, 9, 17, 20], "motiv": [1, 10, 14], "u": [1, 4, 6, 7, 8, 10, 14, 17], "greedili": [1, 3, 5], "hat": [1, 17], "underset": [1, 3, 10, 13, 14, 17, 20], "text": [1, 3, 5, 7, 10, 12, 13, 14, 17, 20, 23, 24], "argmax": [1, 3, 10, 14, 17, 19, 20, 21], "theorem": [1, 4, 8, 14], "prove": [1, 4, 14], "good": [1, 3, 7], "let": [1, 3, 4, 8, 9, 10, 11, 12, 14, 24], "pair": [1, 20], "determinist": [1, 5, 6, 22], "hspace": [1, 7, 12, 14], "5mm": [1, 12, 14], "proof": [1, 4, 11, 14], "omit": 1, "now": [1, 3, 4, 5, 7, 10, 11, 13, 14, 16, 19, 20], "olici": 1, "sequenc": [1, 14], "pi_0": [1, 14], "xrightarrow": 1, "pi_1": 1, "leq": [1, 4, 12, 14], "pi_n": 1, "eventu": 1, "displai": [1, 5, 14, 24], "same": [1, 3, 5, 6, 8, 17, 19, 23], "gridworld": [1, 3], "enviro": [1, 3, 5, 6, 8, 10, 14, 16, 17, 20, 21], "befor": [1, 4, 5, 6, 7, 10, 11, 19], "import": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 25], "numpi": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 25], "np": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 25], "matplotlib": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 25], "pyplot": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 25], "plt": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 25], "seed": [1, 3, 25], "def": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "policy_evalu": 1, "grid_world": [1, 3], "max_it": [1, 3, 5, 6], "height": [1, 3], "width": [1, 3], "zero": [1, 3, 14, 20], "rang": [1, 3, 5, 6, 7, 9, 10, 12, 14, 16, 17, 19, 20, 21, 25], "get_stat": [1, 3], "successor": [1, 3], "get_available_act": [1, 3], "prob_dist": [1, 3], "get_transition_prob": [1, 3], "prob": [1, 3, 7, 10, 12, 14], "item": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "get_reward": [1, 3], "return": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "policy_improv": 1, "dict": [1, 3, 6], "available_act": [1, 3], "q_valu": [1, 3, 5, 6, 8, 17, 19, 20, 21], "max": [1, 3, 5, 12, 14, 17, 19, 21], "kei": [1, 3, 5, 6, 10, 17], "len": [1, 3, 5, 6, 8, 12, 13, 19, 20, 21], "els": [1, 3, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "none": [1, 3, 7, 8], "policy_iter": 1, "get_random_polici": 1, "histori": [1, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "append": [1, 12, 14, 19, 20], "random_idx": 1, "choic": [1, 7, 8, 14, 17], "arang": [1, 19, 20, 21], "setup": [1, 17], "ran": [1, 5, 6, 17], "10": [1, 6, 8, 10, 12, 14, 16, 17, 19, 20, 25], "show": [1, 4, 8, 12, 22, 23, 24], "4": [1, 7, 9, 10, 12, 13, 14, 16, 17, 21, 25], "9": [1, 8, 10, 13, 14, 19], "5": [1, 3, 5, 7, 8, 9, 12, 13, 14, 19, 20, 25], "exit": [1, 3, 12], "good_exit": [1, 3], "3": [1, 3, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "bad_exit": [1, 3], "2": [1, 3, 6, 7, 8, 9, 10, 12, 13, 16, 19, 24], "wall": [1, 3], "living_reward": [1, 3], "win_reward": [1, 3], "lose_reward": [1, 3], "init_po": [1, 3], "iter": [2, 5, 6, 22], "method": [2, 5, 6, 9, 14, 16, 17, 19, 21], "both": [2, 3, 6, 7, 8, 9, 14, 21, 23], "guarante": 2, "empir": [2, 12, 21], "typic": [2, 3, 7, 9, 19], "faster": 2, "therefor": [2, 3, 7, 14, 20], "often": [2, 9, 14, 19, 20], "prefer": [2, 17], "howev": [2, 3, 5, 7, 8, 9, 10, 12, 14, 19, 20], "requir": [2, 6, 10, 17], "knowledg": 2, "mid": [2, 14], "without": [2, 3, 6, 10, 19, 21], "perform": [2, 3, 5, 7, 8, 9, 10, 13, 14, 16, 20, 21], "becom": [2, 6, 14, 19, 20], "infeas": [2, 19], "real": 2, "life": 2, "scenario": 2, "unknown": 2, "too": [2, 14], "complex": [2, 19], "For": [2, 4, 5, 8, 10, 11, 14, 16, 17, 19, 23, 25], "exampl": [2, 14, 19, 23, 25], "game": 2, "blackjack": [2, 5], "although": [2, 9], "theoret": [2, 14], "possibl": [2, 3, 6, 9, 13, 19, 20], "do": [2, 3, 8, 14, 21, 23, 25], "so": [2, 3, 7, 24], "would": [2, 7], "extrem": 2, "challeng": [2, 6, 7, 14, 19, 20], "aim": 2, "need": [2, 3, 7, 8, 10, 12, 14, 17, 20, 24], "know": [2, 3, 6, 21], "discuss": [2, 9, 10, 14, 16, 17], "class": [2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 17, 19, 20, 21], "base": [2, 5, 7, 10, 14, 20, 24], "These": 2, "attempt": 2, "them": [2, 10, 21], "control": [2, 6, 8, 9, 16], "parameter": [2, 7, 10, 13, 19], "directli": [2, 4, 6, 7, 8, 10, 14, 19], "recal": [3, 4, 6, 7, 21], "want": [3, 4, 10, 13, 25], "turn": [3, 6], "out": [3, 14, 22, 25], "each": [3, 5, 6, 10, 14, 16, 17, 19, 21], "r_": [3, 9], "determin": [3, 5, 6, 17], "sampl": [3, 5, 6, 7, 8, 10, 12, 13, 14, 16, 17, 19, 20, 21, 22, 25], "To": [3, 4, 5, 6, 7, 8, 10, 12, 14, 17, 19, 20], "evalu": [3, 5, 6, 8, 13, 19, 20], "first": [3, 4, 5, 7, 8, 9, 14], "simplifi": [3, 9, 10, 14], "bigg": [3, 4, 7, 8, 10, 11, 12, 13, 14], "s_1": 3, "a_0": 3, "s_0": [3, 6], "third": 3, "line": [3, 23, 24, 25], "final": [3, 5, 7, 14], "markov": [3, 13], "assum": [3, 10], "size": [3, 7, 8, 10, 14, 15, 19, 20, 21], "finit": [3, 19], "tag": 3, "abov": [3, 4, 9, 10, 11, 13, 14, 17, 20, 21], "known": [3, 5, 8, 10, 13, 17, 19], "its": [3, 7, 8, 10, 14, 20], "correspond": [3, 7, 14], "linear": [3, 13, 20, 21], "system": [3, 14], "deriv": [3, 6, 8, 10, 11], "result": [3, 4, 5, 7, 8, 10, 13, 14, 19], "argument": 3, "hold": [3, 19], "addit": [3, 6, 10, 13], "max_": [3, 4, 12, 14, 17, 19, 20, 21], "substitut": [3, 9, 10], "must": [3, 6, 7, 23], "satisfi": [3, 9], "condit": [3, 6, 10], "like": [3, 6, 7, 8, 14, 19, 20, 21, 23, 24], "case": [3, 5, 13, 19, 20, 21], "practic": [3, 7, 14], "test": [3, 5, 6, 7, 8, 9, 10, 14, 17, 20, 21], "environ": [3, 5, 6, 7, 10, 16, 17, 19, 21], "consist": [3, 5], "green": 3, "bad": 3, "red": 3, "black": [3, 16], "normal": [3, 8, 9, 12, 13], "block": [3, 24], "white": 3, "reach": 3, "when": [3, 6, 9, 10, 14, 15, 16, 19, 21, 23, 24], "win": 3, "On": [3, 9], "lose": 3, "value_iter": 3, "param": [3, 5, 6, 7, 8, 9, 10, 13, 14, 19, 20, 21], "grid": 3, "world": 3, "int": [3, 5, 6, 10, 19], "maximum": [3, 20], "number": [3, 5, 6, 7, 8, 9, 10, 13, 19], "allow": [3, 5, 7, 10, 16, 23], "execut": [3, 24], "float": [3, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "option": 3, "default": [3, 5, 6, 7, 24], "futur": [3, 16, 17], "ndarrai": 3, "2d": 3, "x": [3, 6, 8, 14], "y": [3, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "entri": [3, 19], "repres": [3, 4, 7, 14, 16, 19], "max_valu": 3, "inf": 3, "instanti": [3, 8], "time": [3, 9, 16, 17, 19, 20], "live": [3, 16], "encourag": [3, 13], "soon": 3, "init": [3, 24], "randomli": [3, 20], "100": [3, 9, 10, 12, 16, 17, 19, 25], "shown": [3, 5, 6, 9, 12, 14, 16, 17, 19, 21], "closer": 3, "gener": [3, 7, 10, 13, 14, 16, 17, 22], "higher": [3, 5, 6, 16], "than": [3, 7, 14], "display_valu": [3, 16], "how": [3, 7, 14, 17, 22, 24], "word": [3, 4, 7, 16], "select": [3, 8, 20], "best": 3, "onc": [3, 14, 19], "extract": [3, 5], "extract_q_valu": 3, "dictionari": [3, 10], "tupl": 3, "coordin": 3, "inner": 3, "extract_polici": 3, "opt_act": [3, 20], "display_qvalu": 3, "illustr": 3, "more": [3, 9, 10, 13, 14, 17, 19, 21, 22, 24, 25], "move": [3, 5, 8, 17, 19], "awai": [3, 16, 17], "attract": 3, "toward": 3, "display_polici": [3, 17], "v_k": 4, "inde": 4, "oper": [4, 10, 13, 20, 21], "rightarrow": [4, 8], "mean": [4, 6, 7, 8, 9, 10, 12, 13, 14, 19, 21, 25], "vector": [4, 14], "notat": [4, 9, 14], "compactli": 4, "infti": [4, 9], "contract": 4, "under": 4, "sup": 4, "norm": 4, "check": [4, 22, 25], "With": [4, 11, 13, 24], "remain": [4, 14], "standard": [4, 13], "fix": [4, 25], "point": [4, 19], "lim_": 4, "v_0": 4, "impli": [4, 10, 14, 21], "pip": [6, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "instal": [6, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "gym": [6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "itertool": 6, "montecarlo": [5, 6], "copi": [14, 20], "seaborn": 6, "sn": 6, "env": [5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "make": [6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 25], "v1": [6, 7, 9, 10, 12, 14, 19, 20, 21], "fals": [5, 6, 8, 9, 10, 12, 13, 16, 17, 19, 20, 21], "sab": 6, "player_sum_rang": 6, "32": [6, 8, 10, 13, 19, 20], "dealer_card_rang": 6, "11": 6, "usable_ace_rang": 6, "true": [5, 6, 9, 12, 13, 14, 16, 17, 20, 21], "list": [5, 6, 16, 17], "product": 6, "state_action_dict": 5, "20": [6, 12], "stick_at_20_or_21": [5, 6], "self": [5, 6, 7, 8, 9, 10, 12, 13, 14, 17, 19, 20, 21], "epsilon": [5, 12, 14, 17, 19, 20, 21], "montecarlocontrol": 5, "__init__": [5, 6, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "avail": [5, 6, 16], "implement": [5, 17, 19], "every_visit": [5, 6], "interact": [5, 6, 16, 25], "n_episod": [5, 6, 10, 12, 16, 17, 19, 20, 21], "episod": [5, 6, 7, 8, 9, 10, 12, 13, 16, 17, 19, 20], "boolean": [5, 6], "indic": [5, 6], "whether": [5, 6, 7, 14, 23], "everi": 5, "visit": [5, 14], "form": [5, 6], "state1": [5, 6], "value1": [5, 6], "count": [5, 6], "cum_reward": [5, 6, 17], "state_act": [5, 6], "enumer": [5, 6, 12], "mc": [5, 7], "optimal_polici": 5, "5000000": 5, "plotblackjackoptimalstrategi": 5, "stick_at_20_or_21_stochast": 5, "rand": [17, 19, 20, 21], "plot_q_values_heatmap": [5, 6], "Not": 6, "being": [6, 19, 23], "abl": 6, "access": 6, "pose": 6, "longer": [6, 17], "program": [6, 14], "suffici": 6, "depend": [6, 23], "One": [5, 6, 8, 10, 12, 14, 15, 16, 17, 19, 20, 21], "come": 6, "around": [6, 8, 16, 17], "through": [6, 9, 10], "essenti": 10, "approx": [6, 7, 9, 10, 14], "frac": [6, 7, 8, 10, 12, 13, 14, 21], "n": [6, 7, 8, 10, 13, 14, 16, 17, 19, 20, 21, 23, 25], "x_i": [6, 14], "suggest": [6, 7, 9, 14, 21], "culmul": 6, "print": [6, 12, 13, 19, 20, 24], "f": [6, 12, 13, 14, 19, 20], "640": 6, "montecarlovalu": 6, "stick": [5, 6], "21": 6, "hit": [5, 6], "500000": 6, "plot_valu": 6, "montecarloqvalu": 6, "action1": 6, "q11": 6, "action2": 6, "q12": 6, "state2": 6, "q21": 6, "q22": 6, "gradient": [7, 13, 15, 22], "go": [7, 22], "j": [7, 8, 10, 11, 13], "theta": [7, 8, 10, 11, 12, 13, 14], "sum_t": 7, "nabla_": [7, 8, 10, 11, 14], "log": [7, 10, 13, 14], "serv": [7, 9, 23], "unbias": [7, 9], "further": [7, 10], "reduc": [7, 9, 14], "varianc": [7, 9, 10, 19], "baselin": 7, "subtract": 7, "give": [7, 8, 10, 14, 17, 22], "b": [7, 13, 20], "even": [7, 14, 20], "though": [7, 14], "formula": [7, 9], "hard": [7, 12], "heurist": 7, "written": [7, 23, 24], "likelihood": 7, "weight": [7, 8, 9], "call": [7, 8, 9, 17, 23], "term": [7, 9, 14], "advantag": [7, 12, 14, 21, 22], "ha": [5, 6, 7, 9, 14, 16, 20], "natrual": 7, "interpret": 7, "wors": [7, 16], "postiv": 7, "rais": [7, 13], "convers": 7, "neg": 7, "lower": [7, 9, 14], "question": 7, "phi": [7, 8, 13, 19], "neural": [7, 8, 10, 13, 14, 19, 20], "network": [7, 8, 10, 13, 14, 19, 20, 21], "3mm": 7, "loss": [7, 9, 10, 12, 19, 20, 21], "object": [7, 10, 11, 12, 21], "still": [7, 8, 9, 13, 20], "achiev": [5, 6, 7, 8, 9], "specifi": [7, 10], "target": [7, 8, 9, 12, 13, 14, 19, 20, 21], "done": [7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "primari": 7, "fit": 7, "data": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21, 25], "squar": [7, 19], "error": [7, 14, 19, 20], "l": [7, 8, 12, 14, 19], "wait": 7, "until": [6, 7, 14], "entir": 7, "been": [7, 14, 20], "calcul": [7, 21], "altern": [5, 7], "increment": 7, "previou": [7, 9, 11, 17], "full": 7, "present": [7, 8, 10, 11, 14], "differenti": [7, 8, 10, 13], "rate": [7, 8, 10, 13, 17], "alpha": [7, 8, 10, 13, 14, 16, 17, 20], "batch": [7, 8, 10, 13, 15, 19], "either": [6, 7], "4mm": 7, "type": [7, 22], "cartpol": [7, 10, 12, 14, 19, 20, 21], "one": [5, 7, 8, 9, 10, 14, 19, 20, 23], "layer": [7, 8, 10, 14], "hidden": [7, 8, 10, 14], "unit": [7, 8, 14], "128": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "pybullet": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "stabl": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "baselines3": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "extra": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "pyvirtualdisplai": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "apt": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "xvfb": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "pybullet_env": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "imageio": [7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "torch": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "nn": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "util": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "dataset": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "dataload": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "devic": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "cuda": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "is_avail": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "cpu": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "modul": [7, 8, 10, 12, 13, 14, 19, 20, 21], "state_dim": [7, 8, 10, 12, 13, 14, 19, 20, 21], "action_dim": [7, 8, 10, 12, 13, 14, 19, 20, 21], "hidden_dim": [7, 8, 10, 12, 13, 14, 19, 20, 21], "super": [7, 8, 10, 12, 13, 14, 19, 20, 21], "net": [7, 8, 10, 12, 13, 14], "simpleneuralnetwork": [7, 8, 10, 12, 14, 19], "in_dim": [7, 8, 10, 12, 14, 19], "out_dim": [7, 8, 10, 12, 13, 14, 19], "final_lay": [7, 10, 12, 14], "softmax": [7, 10, 12, 14], "forward": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "select_act": [7, 8, 10, 12, 13, 14, 19, 20, 21], "tensor": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "action_dist": [7, 10, 12, 13, 14], "categor": [7, 8, 10, 12, 14], "log_prob": [7, 9, 10, 12, 13], "valuenetwork": [7, 9, 13], "squeez": [7, 8, 13, 20], "actorcriticmcag": 7, "lr": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "1e": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "policy_optim": [7, 8, 9, 12, 13], "adam": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "paramet": [7, 8, 9, 10, 12, 13, 14, 16, 19, 20, 21], "value_optim": [7, 9, 12, 13, 14], "policy_schedul": [7, 8, 9, 13], "lr_schedul": [7, 8, 9, 10, 12, 13, 19], "steplr": [7, 8, 9, 10, 12, 13, 19], "step_siz": [7, 8, 9, 10, 12, 13, 19], "1000": [7, 9, 16, 17, 19, 21], "value_schedul": [7, 9, 13], "learn_episod": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "batch_siz": [7, 8, 9, 10, 12, 13, 19, 20, 21], "max_step": [7, 8, 9, 10, 12, 13, 14, 17, 19, 20, 21], "value_loss": [7, 9, 12, 13, 14], "policy_loss": [7, 8, 9, 12, 13], "sample_trajectori": [7, 9, 12, 14], "rewards_cum": 7, "cumsum": 7, "flip": 7, "dim": [7, 8, 13, 19, 21], "mseloss": [7, 8, 9, 12, 13, 20, 21], "get_action_log_prob": [7, 12, 14], "detach": [7, 8, 9, 12, 13, 14, 20], "sum": [7, 9, 12, 13, 14], "zero_grad": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "backward": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "cartpolebulletenv": [7, 9, 10, 12, 14, 19, 20, 21], "3500": [7, 9], "print_everi": [7, 8, 9, 10, 12, 19, 20, 21], "50": [7, 8, 12, 13, 20, 21], "actorcritictdag": 7, "no_grad": [7, 8, 9, 12, 13, 14, 19, 20, 21], "dpg": 8, "wa": 8, "propos": [5, 8, 9, 10, 12, 14, 17, 20, 21], "silver": 8, "et": [8, 9, 10, 12, 14, 19, 21], "al": [8, 9, 10, 12, 14, 19, 21], "2015": [8, 21], "unlik": [8, 12], "tradit": [8, 21], "stochast": [8, 14], "produc": 8, "gaussian": 8, "focus": 8, "author": [8, 9, 11, 14, 21], "analog": 8, "determinst": 8, "mu": 8, "mu_": 8, "dircet": 8, "most": [8, 23], "chain": 8, "mont": [8, 10, 16, 17], "carlo": [8, 10, 16, 17], "subsequ": 8, "paper": [8, 11, 14, 20, 21], "reinforc": [8, 15, 19, 22], "lillicrap": 8, "replai": 8, "buffer": [8, 13, 19, 20, 21], "process": [8, 10, 13, 19], "experi": [8, 10], "store": [8, 19, 23], "minibatch": 8, "_i": [8, 14, 19], "r_i": [8, 19], "y_i": 8, "minim": [8, 19], "nabla": 8, "nabla_a": 8, "soft": [8, 12, 22], "style": [5, 8], "improv": [5, 8, 9, 10, 13], "thei": [8, 23], "train": [8, 10, 12, 13, 17, 19, 20, 21], "exponenti": [8, 9], "averag": [8, 9, 12, 19], "help": [5, 8, 17, 19, 21, 23], "enhanc": [8, 19], "stabil": [8, 9, 19, 21], "set": [5, 8, 14, 16, 17, 19], "005": [8, 13], "dure": [6, 8, 16, 19], "avoid": [8, 20], "unvisit": 8, "address": [5, 6, 8, 10, 12, 14, 19], "issu": [6, 8, 10, 14, 21], "ad": [8, 14], "nois": [8, 10], "chosen": 8, "suit": 8, "ornstein": 8, "uhlenbeck": 8, "dx_t": 8, "x_tdt": 8, "sigma": 8, "dw_t": 8, "invers": [8, 14], "pendulum": 8, "task": [8, 19], "collect": [8, 14, 19, 20], "dequ": [8, 19, 20], "clip_grad_norm_": [8, 12], "n_layer": [8, 12, 14, 19], "shape": [8, 13, 14, 19, 20, 21], "unsqueez": [8, 13, 19, 20, 21], "qvalu": 8, "qvaluenetwork": [8, 13], "cat": [8, 13, 14], "ornsteinuhlenbeckprocess": 8, "15": [8, 12], "dt": 8, "x0": 8, "reset_st": 8, "x_prev": 8, "sqrt": [8, 14], "zeros_lik": [8, 14], "deterministicpolicygradientag": 8, "observation_spac": [8, 13, 14, 16, 17, 19, 20, 21], "creat": [8, 9, 25], "replica": 8, "target_polici": 8, "target_q_valu": [8, 20], "load_state_dict": [8, 12], "state_dict": [8, 12], "q_value_optim": 8, "q_value_schedul": 8, "ou_process": 8, "replaybuff": [8, 13, 19, 20, 21], "maxlen": [8, 13, 19, 20, 21], "10000": [8, 12, 13, 14, 17, 19, 20, 21], "grad_clip_valu": 8, "reset": [8, 10, 12, 13, 16, 17, 19, 20, 21], "episode_loss": [8, 10, 12, 19, 21], "clip": [8, 12], "next_stat": [8, 10, 12, 13, 14, 19, 20, 21], "episode_reward": [8, 10, 12, 13, 19, 21], "add": [8, 13, 19, 20, 21], "q_value_loss": 8, "soft_upd": 8, "target_model": 8, "target_param": [8, 13, 20], "zip": [8, 9, 12, 13, 19, 20], "copy_": [8, 13, 20], "2000": [8, 12, 13], "eval": 8, "invertedpendulumbulletenv": [8, 13], "v0": [8, 13, 16, 17], "500": [8, 12, 17, 20], "99": [8, 13], "actor": [9, 22], "critic": [9, 12, 22], "latter": 9, "becaus": [9, 10, 17, 19], "provid": [5, 9, 11, 14, 16, 17, 20], "grow": 9, "linearli": 9, "wherea": [9, 23], "bia": [9, 20], "tradeoff": 9, "high": [9, 13], "second": [9, 14], "bias": [9, 20], "effect": [9, 19, 21], "balanc": 9, "2016": 9, "schulman": [9, 12, 14], "gae": 9, "design": 9, "combin": 9, "look": [9, 20], "2r": 9, "strength": 9, "offer": [5, 9], "clear": 9, "should": [9, 14, 20, 24], "name": [9, 12, 13], "lambda": 9, "scalar": 9, "delta_t": 9, "tempor": [9, 19, 22], "residu": 9, "reexpress": 9, "2r_": 9, "delta_": 9, "whera": 9, "recov": [9, 21], "tune": [9, 12], "trade": 9, "expens": [9, 16], "some": [5, 9, 12, 14, 20, 22, 23, 25], "wise": 9, "easi": 9, "recurr": 9, "a_t": [9, 12], "a_": [9, 14], "effici": [9, 19], "recurs": 9, "work": [9, 12, 21, 25], "actorcriticgaeag": 9, "lamb": 9, "estimate_advantag": [9, 12], "next_valu": [9, 12], "revers": [9, 12, 14], "delta": [9, 12, 14], "insert": [9, 12, 23], "std": [9, 12, 13], "8": [9, 10, 12, 14, 19, 20], "graph": 9, "appear": 9, "fastest": 9, "after": [9, 16, 20], "epoch": [9, 17], "exhibit": [9, 19], "signific": 9, "variat": [9, 14, 23], "slightli": [6, 9, 10], "slower": 9, "demonstr": [9, 19], "much": [5, 9, 14, 16], "long": [9, 14, 20], "run": [5, 9, 16, 24], "lambda_": [9, 12], "00": 9, "contrast": [10, 19], "explicitli": 10, "formal": [10, 11, 16], "underbrac": 10, "fashion": 10, "specif": [6, 10, 19, 23], "tau_i": [10, 13], "troubl": 10, "cannot": [10, 21], "part": 10, "variabl": 10, "reformul": 10, "mention": [10, 11], "earlier": 10, "unrol": 10, "regular": [10, 23], "interchang": 10, "integr": 10, "d": [10, 12, 14, 19, 24], "trick": 10, "statement": 10, "sutton": 10, "1999": 10, "what": 10, "remark": 10, "about": [10, 23, 24, 25], "approxim": [10, 13, 14, 17, 19], "pseudocod": [10, 14], "dimens": [10, 13], "ob": [10, 12, 13, 14, 19], "action_log_prob": [10, 12, 13], "ll": [10, 23], "reinforceag": 10, "inherit": 10, "includ": [10, 14, 24, 25], "pre": 10, "n_sampl": [10, 19], "It": [10, 14, 17, 19, 22, 23], "record": 10, "metric": 10, "schedul": [10, 12, 19], "vari": 10, "300": 10, "decai": 10, "plot": [10, 12, 25], "partli": 10, "fact": [10, 14], "larger": [10, 14, 19], "smaller": [10, 14], "explan": [10, 17], "rather": 11, "inform": [11, 19, 22, 24, 25], "origin": [11, 14, 20], "worth": [6, 11], "rewrit": 11, "mind": 11, "label": [11, 12], "drawback": [12, 16], "trust": [12, 22], "region": [12, 22], "difficult": [6, 12], "2017": [12, 14], "variant": [12, 19], "trpo": 12, "theta_": [12, 14], "old": [12, 13, 14], "subject": [12, 14], "rho": [12, 14], "kl": [12, 13], "constraint": [12, 13], "beta": 12, "doe": [12, 19, 22], "seem": [12, 16], "well": [12, 22, 25], "And": [6, 12, 13], "min": [12, 13], "hyperparamet": [12, 19], "far": [12, 16, 17], "ratio": [12, 14], "back": [12, 21], "ppoagent": 12, "ep": 12, "policy_old": 12, "old_log_prob": [12, 13, 14], "new_log_prob": [12, 13, 14], "exp": [12, 13, 14], "surr1": 12, "surr2": 12, "clamp": 12, "train_loss": [12, 19, 20], "load": 12, "user": 12, "raymondtsao": 12, "desktop": 12, "reinforcementlearningnot": 12, "policygradi": 12, "e005_reward": 12, "npy": 12, "e010_reward": 12, "e015_reward": 12, "e020_reward": 12, "e0": 12, "05": 12, "color": [12, 25], "smooth": 12, "smoothed_reward": 12, "start_index": 12, "c": [12, 14], "titl": 12, "xlabel": 12, "ylabel": 12, "legend": [12, 25], "decis": [5, 13], "formul": 13, "h": [13, 14], "explor": 13, "robust": [13, 14], "framework": 13, "tierat": 13, "involv": [5, 13], "argmin": 13, "d_": [13, 14], "z_": 13, "sequenti": [13, 20, 21], "relu": [13, 20, 21], "log_std": 13, "chunk": 13, "action_rang": 13, "try": [5, 13, 14], "tanh": 13, "randn_lik": 13, "pow": [13, 19], "6": [13, 16], "keepdim": [13, 21], "except": 13, "occur": [13, 16], "local": 13, "softactorcriticag": 13, "target_valu": 13, "q_value1": 13, "q_value2": 13, "q_value_optimizer1": 13, "q_value_optimizer2": 13, "150": 13, "7": 13, "q_value_scheduler1": 13, "q_value_scheduler2": 13, "train_value_loss": 13, "train_q_value1_loss": 13, "train_q_value2_loss": 13, "train_policy_loss": 13, "track": 13, "log_action_prob": 13, "old_stat": 13, "old_act": 13, "new_act": [13, 17], "new_q_value1": 13, "new_q_value2": 13, "new_q_valu": 13, "retain_graph": 13, "old_q_value1": 13, "old_q_value2": 13, "q_value_loss1": 13, "q_value_loss2": 13, "left": 14, "right": [14, 16], "suppos": [14, 20], "tild": 14, "rho_": 14, "frequenc": 14, "replac": 14, "accur": 14, "sens": [14, 16], "match": 14, "order": 14, "theta_0": 14, "isn": 14, "larg": [14, 15, 19], "unclear": 14, "small": [5, 14, 15, 22, 23], "insight": 14, "influenc": 14, "tv": 14, "measur": 14, "distanc": 14, "tight": 14, "cd_": 14, "monoton": 14, "pi_i": 14, "why": [14, 20], "m_i": 14, "definit": 14, "sound": 14, "necessari": 14, "tractabl": 14, "usal": 14, "simplif": 14, "ensur": 14, "intract": 14, "lot": [14, 23, 25], "TO": [14, 21], "overlin": 14, "theta_1": 14, "theta_2": 14, "manag": 14, "theta_k": 14, "g": 14, "th": 14, "hessian": 14, "matrix": 14, "quadrat": 14, "analyt": 14, "theta_i": 14, "g_i": 14, "h_i": 14, "numer": 14, "unstabl": 14, "thing": [14, 24], "hx": 14, "There": [14, 25], "popular": 14, "conjug": 14, "descent": 14, "solut": [5, 14, 17, 19], "found": 14, "adjust": 14, "chang": [5, 14, 21], "h_ix_i": 14, "touch": 14, "ve": 14, "made": [14, 21], "due": [14, 20], "veri": 14, "bring": 14, "maximium": 14, "delta_k": 14, "g_k": 14, "x_k": 14, "delta_i": 14, "accept": [14, 23], "break": [14, 19], "last": 14, "tqdm": 14, "o": 14, "base64": 14, "io": 14, "ipython": 14, "ipythondisplai": 14, "datetim": 14, "stable_baselines3": 14, "ppo": 14, "sy": 14, "warn": 14, "filterwarn": 14, "ignor": 14, "categori": 14, "userwarn": 14, "deprecationwarn": 14, "logger": 14, "set_level": 14, "action_prob": 14, "gather": [14, 20], "trpoagent": 14, "lmbda": 14, "action_spac": [14, 16, 17, 19, 20, 21], "compute_advantag": 14, "advantage_list": 14, "dtype": 14, "hessian_product": 14, "old_dist": 14, "new_dist": 14, "kl_diverg": 14, "kl_grad": 14, "autograd": 14, "grad": 14, "create_graph": 14, "view": 14, "kl_grad_product": 14, "dot": 14, "hessian_matrix_product": 14, "conjugate_gradi": 14, "tol": 14, "clone": 14, "r_r": 14, "hp": 14, "new_r_r": 14, "surrog": 14, "line_search": 14, "max_direct": 14, "old_param": 14, "convert_paramet": 14, "parameters_to_vector": 14, "old_surrog": 14, "new_param": 14, "new_polici": 14, "deepcopi": [14, 20], "vector_to_paramet": 14, "new_surrog": 14, "mse_loss": 14, "td_target": [14, 20], "surrogate_grad": 14, "direct": [14, 24], "env_nam": 14, "95": 14, "0005": [14, 20], "value_lr": 14, "num_episod": 14, "g_t": 16, "happen": [], "less": [16, 17, 19], "desir": 16, "meaning": 16, "progress": 16, "mathemat": [], "td": [16, 17], "immedi": 16, "arbitrari": [16, 17], "termin": [16, 17], "observ": [16, 17, 20], "extend": 16, "48": 16, "bottom": 16, "corner": 16, "penalti": 16, "katex": 16, "figur": [16, 17], "estimate_value_funct": 16, "get_act": [16, 17], "new_stat": [16, 17], "info": [16, 17, 19], "47": [16, 17], "randompolici": 16, "cliffwalk": [16, 17], "box": [16, 23], "compar": [16, 19, 20], "chanc": [5, 16, 20], "fall": [16, 17], "cover": 17, "idea": [5, 17], "adopt": 17, "strategi": [5, 6, 17], "greedi": [5, 17], "cliff": 17, "walk": 17, "patch": 17, "polygon": 17, "step_count": 17, "learnedpolici": 17, "action_valu": 17, "3000": 17, "As": [17, 20, 25], "close": [6, 17, 19], "path": [17, 24], "safer": 17, "modifi": [6, 17], "place": [5, 17], "offlin": 17, "version": 17, "1989": 17, "watkin": 17, "bootstrap": [16, 17], "q_learn": 17, "between": [5, 6, 17, 19], "conserv": 17, "timestep": [16, 17], "complet": 17, "riski": 17, "risk": [5, 17], "thorough": 17, "thereaft": 19, "discret": 19, "straightforward": [5, 19], "tabl": 19, "deal": 19, "race": 19, "car": 19, "robot": 19, "arm": 19, "infinit": 19, "unbound": 19, "bin": 19, "But": [19, 25], "lead": 19, "explod": 19, "accuraci": 19, "overcom": 19, "techniqu": 19, "randint": [19, 20, 21], "qlearningagentwithoutbuff": 19, "agent_without_buff": 19, "core": 19, "assumpt": [5, 19], "supervis": 19, "independ": 19, "ident": 19, "correl": 19, "especi": 19, "within": 19, "ineffici": [19, 20], "poor": [5, 19], "rl": 19, "particularli": [6, 19, 21], "discard": 19, "reus": 19, "multipl": 19, "mai": [5, 19], "fulli": 19, "exploit": 19, "mnih": 19, "concept": 19, "consecut": 19, "incorpor": 19, "architectur": [19, 21], "tri": 19, "fill": 19, "__len__": 19, "qlearningagentwithbuff": 19, "buffer_max_len": [19, 20], "state_batch": [19, 21], "action_batch": [19, 21], "reward_batch": [19, 21], "next_state_batch": [19, 21], "done_batch": [19, 21], "up": [19, 20], "evaluate_n_episod": [19, 21], "evaluation_n_episod": 19, "total_reward": 19, "agent_with_buff": 19, "ne": 19, "b8": 19, "b32": 19, "evid": [19, 23], "significantli": 19, "quickli": 19, "curv": 19, "acceler": 19, "overestim": 20, "highli": 20, "accumul": 20, "eventuuali": 20, "affect": [20, 21], "hassult": 20, "q_a": 20, "q_b": 20, "longrightarrow": 20, "counteract": 20, "thu": 20, "disentangl": 20, "separ": [5, 20, 21], "update_a": 20, "update_b": 20, "computation": [16, 20], "year": 20, "hassault": 20, "scheme": 20, "lunarland": 20, "m": 20, "doubleqlearningag": 20, "train_reward": 20, "v2": 20, "64": 20, "wang": 21, "slight": [21, 23], "deep": [21, 22], "great": 21, "success": 21, "reprsent": 21, "nueral": 21, "togeth": 21, "relev": 21, "unnecessari": 21, "architur": 21, "lack": 21, "modif": [5, 21], "plug": 21, "benefit": 21, "duelqlearningag": 21, "preprocess": 21, "ql": 21, "5e": 21, "you": [22, 23, 24, 25], "feel": 22, "content": [22, 23, 24], "structur": [22, 23], "off": [22, 23, 24], "few": 22, "major": 22, "file": [22, 24], "depth": 22, "topic": 22, "document": [22, 23, 24, 25], "page": [22, 23, 24], "bundl": 22, "doubl": 22, "duel": 22, "proxim": 22, "write": [23, 24], "your": [23, 24, 25], "book": [23, 24, 25], "jupyt": [23, 24, 25], "notebook": 23, "ipynb": 23, "md": [23, 24], "flavor": 23, "syntax": 23, "stand": 23, "markedli": 23, "commonmark": 23, "extens": 23, "sphinx": 23, "ecosystem": 23, "overview": 23, "power": 23, "tool": 23, "markup": 23, "languag": 23, "purpos": 23, "span": 23, "mani": [23, 24], "kind": 23, "those": 23, "here": [23, 25], "render": 23, "special": 23, "build": 23, "inlin": 23, "refer": 23, "cite": 23, "bibtex": 23, "holdgraf_evidence_2014": 23, "hdhpk14": 23, "moreov": 23, "bibliographi": 23, "properli": 23, "bib": 23, "christoph": 23, "ramsai": 23, "holdgraf": 23, "wendi": 23, "de": 23, "heer": 23, "brian": 23, "paslei": 23, "robert": 23, "knight": 23, "predict": 23, "code": [23, 24], "human": 23, "auditori": 23, "cortex": 23, "intern": 23, "confer": 23, "cognit": 23, "neurosci": 23, "brisban": 23, "australia": 23, "2014": 23, "frontier": 23, "just": 23, "starter": 23, "jupyterbook": 23, "org": 23, "detail": 24, "instruct": 24, "built": 24, "kernel": 24, "rest": 24, "jupytext": 24, "convert": 24, "support": 24, "understand": 24, "top": 24, "presenc": 24, "That": 24, "treat": 24, "command": 24, "markdownfil": 24, "emb": 25, "imag": 25, "html": 25, "etc": 25, "post": 25, "add_": 25, "math": 25, "mbox": 25, "la_": 25, "tex": 25, "sure": 25, "escap": 25, "dollar": 25, "sign": 25, "keep": [6, 25], "guid": 25, "rcparam": 25, "cycler": 25, "ion": 25, "reproduc": 25, "19680801": 25, "logspac": 25, "randn": 25, "ii": 25, "cmap": 25, "cm": 25, "coolwarm": 25, "ax": 25, "prop_cycl": 25, "linspac": 25, "line2d": 25, "custom_lin": 25, "lw": 25, "fig": 25, "subplot": 25, "figsiz": 25, "cold": 25, "medium": 25, "hot": 25, "openai": [6, 16], "player": [5, 6], "exceed": 6, "card": 6, "deck": 6, "face": 6, "ac": [5, 6], "gameplai": 6, "decid": 6, "draw": 6, "compet": 6, "against": 6, "dealer": [5, 6], "who": 6, "least": 6, "17": 6, "otherwis": 6, "unexplor": [5, 6], "problemat": 6, "might": [5, 6], "overlook": 5, "simpli": 5, "coverag": 5, "000": 5, "usabl": 5, "strong": 5, "realist": 5, "therebi": 5, "seemingli": 5, "suboptim": 5, "obtain": 5, "smoother": 5, "boundari": 5, "onlin": 5, "cumul": 16, "manner": 16}, "objects": {}, "objtypes": {}, "objnames": {}, "titleterms": {"markov": [0, 22], "decis": [0, 22], "process": [0, 22], "valu": [0, 3, 4, 6, 7, 8, 22], "function": [0, 7, 8], "action": [0, 6, 8], "polici": [1, 3, 5, 7, 8, 10, 11, 12, 13, 14, 16, 20, 22], "iter": [1, 3, 4, 13, 14], "evalu": [1, 16], "e": 1, "improv": [1, 14], "i": [1, 23], "lemma": 1, "implement": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 20, 21], "reinforc": [2, 10], "learn": [2, 3, 17, 19, 20, 21, 23], "optim": [3, 14], "converg": 4, "analysi": 4, "mont": [5, 6, 7], "carlo": [5, 6, 7], "control": [5, 17], "explor": [5, 8], "start": 5, "stochast": 5, "estim": [6, 9], "first": 6, "visit": 6, "v": 6, "": 6, "everi": 6, "blackjack": 6, "q": [17, 19, 20, 21], "actor": [7, 13], "critic": [7, 13], "method": [7, 8, 10, 22], "approxim": 7, "tempor": [7, 16, 17], "differ": [7, 16, 17], "algorithm": [7, 14], "td": 7, "determinist": 8, "gradient": [8, 10, 11, 12, 14], "deep": [8, 19, 20], "updat": 8, "ddpg": 8, "gener": 9, "advantag": 9, "theorem": [10, 11], "inform": 10, "proxim": 12, "ppo": 12, "soft": 13, "maximum": 13, "entropi": 13, "object": [13, 14], "trust": 14, "region": 14, "bound": 14, "natur": 14, "1": 14, "averag": 14, "kl": 14, "diverg": 14, "2": 14, "linear": 14, "constraint": 14, "truncat": 14, "trpo": 14, "backtrack": 14, "line": 14, "search": 14, "varianc": 15, "reduct": 15, "reward": 15, "go": 15, "baselin": 15, "subtract": 15, "cliff": 16, "walk": 16, "sarsa": 17, "vanilla": 19, "experi": 19, "replai": 19, "doubl": 20, "lunar": 20, "lander": 20, "duel": 21, "identifi": 21, "dqn": 21, "welcom": 22, "your": 22, "jupyt": 22, "book": 22, "A": 22, "b": 22, "base": 22, "c": 22, "markdown": [23, 24, 25], "file": 23, "what": 23, "myst": [23, 24, 25], "sampl": 23, "role": 23, "direct": 23, "citat": 23, "more": 23, "notebook": [24, 25], "an": 24, "exampl": 24, "cell": 24, "creat": 24, "quickli": 24, "add": 24, "yaml": 24, "metadata": 24, "content": 25, "code": 25, "block": 25, "output": 25, "mc": 6}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinxcontrib.bibtex": 9, "sphinx": 60}, "alltitles": {"Markov decision process": [[0, "markov-decision-process"]], "Value function and action value function": [[0, "value-function-and-action-value-function"]], "Policy iteration": [[1, "policy-iteration"], [1, "id1"]], "Policy evaluation (E)": [[1, "policy-evaluation-e"]], " (Policy-evaluation)": [[1, "simple-algorithm"]], "Policy improvement (I)": [[1, "policy-improvement-i"]], " (Policy improvement lemma)": [[1, "my-theorem"]], " (Policy iteration)": [[1, "simple-algorithm"]], "Policy iteration Implementation": [[1, "policy-iteration-implementation"]], "Reinforcement Learning": [[2, "reinforcement-learning"]], "Value iteration": [[3, "value-iteration"]], " (Value iteration)": [[3, "simple-algorithm"]], "Value Iteration Implementation": [[3, "value-iteration-implementation"]], "Learning the optimal policy": [[3, "learning-the-optimal-policy"]], "Convergence analysis": [[4, "convergence-analysis"]], "": [[4, "my-theorem"]], " (Value iteration convergence)": [[4, "my-theorem"]], "Actor critic methods": [[7, "actor-critic-methods"]], "Monte Carlo approximation": [[7, "monte-carlo-approximation"]], "Temporal difference approximation": [[7, "temporal-difference-approximation"]], "Actor critic algorithm": [[7, "actor-critic-algorithm"]], " (Actor critic algorithm)": [[7, "my-algorithm"]], "Actor critic approximation": [[7, "actor-critic-approximation"]], "Policy and value function implementation": [[7, "policy-and-value-function-implementation"]], "Actor critic with Monte Carlo implementation": [[7, "actor-critic-with-monte-carlo-implementation"]], "Actor critic with TD implementation": [[7, "actor-critic-with-td-implementation"]], "Deterministic policy gradient method": [[8, "deterministic-policy-gradient-method"]], "Deep deterministic policy gradient": [[8, "deep-deterministic-policy-gradient"]], " (Deep deterministic policy gradient)": [[8, "my-algorithm"]], "Action value function update": [[8, "action-value-function-update"]], "Exploration": [[8, "exploration"]], "DDPG Implementation": [[8, "ddpg-implementation"]], "Generalized advantage estimation": [[9, "generalized-advantage-estimation"]], "Generalized advantage estimator implementation": [[9, "generalized-advantage-estimator-implementation"]], "Policy gradient method": [[10, "policy-gradient-method"]], "Policy gradient theorem (Informal)": [[10, "policy-gradient-theorem-informal"]], " (REINFORCE)": [[10, "my-algorithm"]], "REINFORCE Implementation": [[10, "reinforce-implementation"]], "Policy gradient theorem": [[11, "policy-gradient-theorem"]], "Proximal policy gradient": [[12, "proximal-policy-gradient"]], "PPO Implementation": [[12, "ppo-implementation"]], "Soft actor critic": [[13, "soft-actor-critic"]], "Maximum entropy objective": [[13, "maximum-entropy-objective"]], " (Soft policy iteration)": [[13, "my-algorithm"]], "Soft actor critic implementation": [[13, "soft-actor-critic-implementation"]], "Trust region policy optimization": [[14, "trust-region-policy-optimization"]], " (Improvement bound)": [[14, "my-theorem"]], " (Policy iteration algorithm)": [[14, "my-theorem"]], "Natural policy gradient": [[14, "natural-policy-gradient"]], "1. Average KL divergence": [[14, "average-kl-divergence"]], "2. Linearize objective and constraint": [[14, "linearize-objective-and-constraint"]], " (Natural policy gradient)": [[14, "natural-policy-gradient"]], "Truncated natural policy gradient": [[14, "truncated-natural-policy-gradient"]], " (Truncated natural policy gradient)": [[14, "truncated-natural-policy-gradient"]], "Trust region policy optimization (TRPO)": [[14, "trust-region-policy-optimization-trpo"]], " (Backtracking line search)": [[14, "line-search"]], " (Trust region policy optimization)": [[14, "trpo"]], "TRPO Implementation": [[14, "trpo-implementation"]], "Variance reduction": [[15, "variance-reduction"]], "Reward to go": [[15, "reward-to-go"]], "Baseline subtraction": [[15, "baseline-subtraction"]], "Temporal difference control": [[17, "temporal-difference-control"]], "SARSA": [[17, "sarsa"]], " (SARSA)": [[17, "my-algorithm"]], "Q-Learning": [[17, "q-learning"]], " (Q-learning)": [[17, "my-algorithm"]], "Deep Q Learning": [[19, "deep-q-learning"]], "Vanilla deep Q Learning": [[19, "vanilla-deep-q-learning"]], "Deep Q Learning with experience replay": [[19, "deep-q-learning-with-experience-replay"]], "Double Q Learning": [[20, "double-q-learning"]], " (Double Q Learning)": [[20, "my-algorithm"]], " (Deep double Q Learning)": [[20, "my-algorithm"]], "Double Q Learning Implementation": [[20, "double-q-learning-implementation"]], "Policy": [[20, "policy"]], "Lunar Lander": [[20, "lunar-lander"]], "Dueling Q Learning": [[21, "dueling-q-learning"]], "Identifiability": [[21, "identifiability"]], "Duel DQN implementation": [[21, "duel-dqn-implementation"]], "Welcome to your Jupyter Book": [[22, "welcome-to-your-jupyter-book"]], "A. Markov decision process": [[22, null]], "B. Value based methods": [[22, null]], "C. Policy based methods": [[22, null]], "Markdown Files": [[23, "markdown-files"]], "What is MyST?": [[23, "what-is-myst"]], "Sample Roles and Directives": [[23, "sample-roles-and-directives"]], "Citations": [[23, "citations"]], "Learn more": [[23, "learn-more"]], "Notebooks with MyST Markdown": [[24, "notebooks-with-myst-markdown"]], "An example cell": [[24, "an-example-cell"]], "Create a notebook with MyST Markdown": [[24, "create-a-notebook-with-myst-markdown"]], "Quickly add YAML metadata for MyST Notebooks": [[24, "quickly-add-yaml-metadata-for-myst-notebooks"]], "Content with notebooks": [[25, "content-with-notebooks"]], "Markdown + notebooks": [[25, "markdown-notebooks"]], "MyST markdown": [[25, "myst-markdown"]], "Code blocks and outputs": [[25, "code-blocks-and-outputs"]], "Value estimation": [[6, "value-estimation"]], "First-visit v.s. every visit Monte Carlo": [[6, "first-visit-v-s-every-visit-monte-carlo"]], "BlackJack": [[6, "blackjack"]], "MC Value Estimation Implementation": [[6, "mc-value-estimation-implementation"]], "MC Action Value Estimation Implementation": [[6, "mc-action-value-estimation-implementation"]], "Monte Carlo control": [[5, "monte-carlo-control"]], "Monte Carlo with exploring starts": [[5, "monte-carlo-with-exploring-starts"]], "Monte Carlo with stochastic policies": [[5, "monte-carlo-with-stochastic-policies"]], "Temporal difference": [[16, "temporal-difference"]], " (Temporal difference policy evaluation)": [[16, "my-algorithm"]], "Cliff walking": [[16, "cliff-walking"]], "Temporal Difference Policy Evaluation Implementation": [[16, "temporal-difference-policy-evaluation-implementation"]]}, "indexentries": {}})