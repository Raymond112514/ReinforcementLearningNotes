{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf95167",
   "metadata": {},
   "source": [
    "# Markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d0f9e",
   "metadata": {},
   "source": [
    "A Markov decision process (MDP) model is composed of four elements\n",
    "\n",
    "1. State space $\\mathcal{S}$\n",
    "2. Action space $\\mathcal{A}$\n",
    "3. Transition dynamics $p(\\mathbf{s}', r|\\mathbf{s}, \\mathbf{a})$\n",
    "4. Reward dynamics $r(\\mathbf{s}, \\mathbf{a})$\n",
    "\n",
    "An agent interacts with the Markov Decision Process (MDP) by starting in state $\\mathbf{s} \\in \\mathcal{S}$, taking an action $\\mathbf{a} \\in \\mathcal{A}$, receiving a reward $r(\\mathbf{s}, \\mathbf{a})$, and transitioning to a new state $\\mathbf{s}'$ according to the transition dynamics $p(\\mathbf{s}', r \\mid \\mathbf{s}, \\mathbf{a})$. As this process continues, we obtain a trajectory $\\tau = (\\mathbf{s}_1, \\mathbf{a}_1, r_1, \\mathbf{s}_2, \\mathbf{a}_2, r_2, \\ldots)$, which may potentially go on forever. Given any trajectory $\\tau$, we define the reward associated with it as\n",
    "\n",
    "$$r(\\tau) = \\sum_{t\\geq 0} \\gamma^t r_t$$\n",
    "\n",
    "where $\\gamma \\in (0, 1)$ is the discount factor that ensures the reward $r(\\tau)$ remains finite. A trajectory is a random variable induced by a policy $\\pi(\\mathbf{a} \\mid \\mathbf{s})$, which maps a state to a distribution over actions. When following $\\pi$ in the MDP, the trajectory distribution is given by\n",
    "\n",
    "$$p_{\\pi}(\\tau) = p(\\mathbf{s}_0)\\prod_{t\\geq 0}\\pi(\\mathbf{a}_t|\\mathbf{s}_t) p(\\mathbf{s}_{t+1}|\\mathbf{s}_t, \\mathbf{a}_t)$$\n",
    "\n",
    "Under the trajectory distribution, the expected reward associated with the policy $\\pi$ is defined as\n",
    "\n",
    "$$\\eta(\\pi) = \\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)}[r(\\tau)]$$\n",
    "\n",
    "The goal in a MDP is to find a policy that maximizes the expected reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51997a",
   "metadata": {},
   "source": [
    "## Value function and action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33b1de",
   "metadata": {},
   "source": [
    "Two quantities of particular interest in a Markov Decision Process (MDP) are the value function $V(\\mathbf{s})$ and the state-action value function $Q(\\mathbf{s}, \\mathbf{a})$. The value function is defined as the expected reward starting from state $\\mathbf{s}$:\n",
    "\n",
    "$$V_{\\pi}(\\mathbf{s}) = \\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)|\\mathbf{s}_0=\\mathbf{s}}[r(\\tau)|\\mathbf{s}_0=\\mathbf{s}]$$\n",
    "\n",
    "Applying the law of total expectation, we can relate this to the expected reward:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\eta(\\pi) &= \\mathbb{E}_{\\mathbf{s}_0\\sim p(\\mathbf{s}_0)}[\\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)|\\mathbf{s}_0=\\mathbf{s}}[r(\\tau)|\\mathbf{s}_0=\\mathbf{s}]]\\\\\n",
    "&= \\mathbb{E}_{\\mathbf{s}_0\\sim p(\\mathbf{s}_0)}[V_{\\pi}(\\mathbf{s}_0)]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since we cannot optimize the prior distribution $p(\\mathbf{s}_0)$, maximizing the expected reward is equivalent to maximizing the value function for all states $\\mathbf{s} \\in \\mathcal{S}$. The state-action value function, on the other hand, is defined as the expected reward starting from state $\\mathbf{s}$ and taking action $\\mathbf{a}$:\n",
    "\n",
    "$$Q_{\\pi}(\\mathbf{s}, \\mathbf{a}) = \\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)|\\mathbf{s}_0=\\mathbf{s}, \\mathbf{a}_0=\\mathbf{a}}[r(\\tau)|\\mathbf{s}_0=\\mathbf{s}, \\mathbf{a}_0=\\mathbf{a}]$$\n",
    "\n",
    "Applying the law of total expectation again, we can relate the state-action value function to the value function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_{\\pi}(\\mathbf{s}) &= \\mathbb{E}_{\\mathbf{a}_0\\sim \\pi(\\cdot|\\mathbf{s}_0)}[ \\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)|\\mathbf{s}_0=\\mathbf{s}, \\mathbf{a}_0=\\mathbf{a}}[r(\\tau)|\\mathbf{s}_0=\\mathbf{s}, \\mathbf{a}_0=\\mathbf{a}]]\\\\\n",
    "&= \\mathbb{E}_{\\mathbf{a}_0\\sim \\pi(\\cdot|\\mathbf{s}_0)}[Q_{\\pi}(\\mathbf{s}, \\mathbf{a})]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In the next section, we will introduce a simple algorithm for estimating the optimal policy using these two functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}