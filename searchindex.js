Search.setIndex({"docnames": ["MDP/Markov-decision-process", "MDP/Policy-iteration", "MDP/Reinforcement-learning", "MDP/Value-iteration", "MDP/Value-iteration-convergence", "MonteCarlo/Monte-Carlo-Control", "MonteCarlo/Monte-Carlo-value-estimation", "PolicyGradient/Actor-critic-methods", "PolicyGradient/Deterministic-policy-gradient-method", "PolicyGradient/Generalized-advantage-estimation", "PolicyGradient/Policy-gradient-method", "PolicyGradient/Policy-gradient-theorem", "PolicyGradient/Proximal-policy-gradient", "PolicyGradient/Soft-actor-critic", "PolicyGradient/TRPO", "PolicyGradient/Variance-reduction", "TD/Temporal-difference", "TD/Temporal-difference-control", "TD/Untitled", "ValueBasedMethods/Deep-Q-Learning", "ValueBasedMethods/Double-Q-Learning", "ValueBasedMethods/Dueling-Q-Learning", "intro"], "filenames": ["MDP/Markov-decision-process.ipynb", "MDP/Policy-iteration.ipynb", "MDP/Reinforcement-learning.ipynb", "MDP/Value-iteration.ipynb", "MDP/Value-iteration-convergence.ipynb", "MonteCarlo/Monte-Carlo-Control.ipynb", "MonteCarlo/Monte-Carlo-value-estimation.ipynb", "PolicyGradient/Actor-critic-methods.ipynb", "PolicyGradient/Deterministic-policy-gradient-method.ipynb", "PolicyGradient/Generalized-advantage-estimation.ipynb", "PolicyGradient/Policy-gradient-method.ipynb", "PolicyGradient/Policy-gradient-theorem.ipynb", "PolicyGradient/Proximal-policy-gradient.ipynb", "PolicyGradient/Soft-actor-critic.ipynb", "PolicyGradient/TRPO.ipynb", "PolicyGradient/Variance-reduction.ipynb", "TD/Temporal-difference.ipynb", "TD/Temporal-difference-control.ipynb", "TD/Untitled.ipynb", "ValueBasedMethods/Deep-Q-Learning.ipynb", "ValueBasedMethods/Double-Q-Learning.ipynb", "ValueBasedMethods/Dueling-Q-Learning.ipynb", "intro.md"], "titles": ["<span class=\"section-number\">1. </span>Markov decision process", "<span class=\"section-number\">3. </span>Policy iteration", "<span class=\"section-number\">4. </span>Reinforcement Learning", "<span class=\"section-number\">2. </span>Value iteration", "<span class=\"section-number\">2.3. </span>Convergence analysis", "<span class=\"section-number\">5.5. </span>Monte Carlo control", "<span class=\"section-number\">5. </span>Monte Carlo methods", "<span class=\"section-number\">11. </span>Actor critic methods", "<span class=\"section-number\">15. </span>Deterministic policy gradient method", "<span class=\"section-number\">12. </span>Generalized advantage estimation", "<span class=\"section-number\">10. </span>Policy gradient method", "Policy gradient theorem", "<span class=\"section-number\">14. </span>Proximal policy optimization", "<span class=\"section-number\">16. </span>Soft actor critic", "<span class=\"section-number\">13. </span>Trust region policy optimization", "Variance reduction", "<span class=\"section-number\">6. </span>Temporal difference methods", "<span class=\"section-number\">6.3. </span>Temporal difference control", "&lt;no title&gt;", "<span class=\"section-number\">7. </span>Deep Q Learning", "<span class=\"section-number\">8. </span>Double Q Learning", "<span class=\"section-number\">9. </span>Duel Q Learning", "Notes on Reinforcement Learning"], "terms": {"A": [0, 1, 3, 4, 5, 6, 7, 8, 9, 11, 14, 17, 19, 20, 21], "mdp": [0, 1, 2, 3, 11, 13], "model": [0, 8, 10, 21, 22], "i": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22], "compos": 0, "four": 0, "element": 0, "state": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "space": [0, 3, 14, 19], "mathcal": [0, 1, 3, 4, 8, 11, 12, 13, 14, 16, 17, 19, 20, 21], "": [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21, 22], "transit": [0, 2, 3, 6, 8, 19, 20], "dynam": [0, 2, 3, 6], "p": [0, 1, 2, 3, 4, 6, 8, 10, 11, 14], "mathbf": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "r": [0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 19, 20], "reward": [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "an": [0, 1, 2, 3, 6, 7, 9, 10, 13, 14, 17, 19], "agent": [0, 3, 7, 8, 9, 10, 12, 13, 14, 16, 19, 20, 21], "interact": [0, 16], "start": [0, 1, 6, 9, 14, 16], "take": [0, 5, 7, 8, 16, 17, 19], "receiv": [0, 3], "new": [0, 1, 13, 14], "accord": [0, 1, 4], "mid": [0, 2, 14], "As": [0, 17, 20], "thi": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "continu": [0, 8, 19], "we": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "obtain": [0, 5, 7], "trajectori": [0, 6, 7, 9, 10, 13, 14, 16, 19], "tau": [0, 3, 7, 8, 10, 11, 13, 14, 20], "_1": [0, 3, 10, 11], "r_1": 0, "_2": [0, 3], "r_2": 0, "ldot": 0, "which": [0, 3, 4, 5, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "mai": [0, 5, 19], "potenti": [0, 6, 10, 14], "go": [0, 7], "forev": 0, "given": [0, 1, 2, 3, 4, 5, 7, 11, 13, 14, 17, 21], "ani": [0, 1, 3, 4, 5, 6, 10, 11, 16, 17, 19], "defin": [0, 3, 4, 5, 7, 8, 20, 21], "associ": [0, 3, 13], "sum_": [0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 13, 14, 21], "t": [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 17, 19], "geq": [0, 1, 3, 7, 9, 10, 11, 13, 14], "0": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "gamma": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "r_t": [0, 8, 9, 12, 14, 16, 17], "where": [0, 3, 6, 7, 8, 9, 10, 12, 14, 16, 19, 20, 21], "1": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 16, 17, 19, 20, 21], "discount": [0, 3, 14, 16, 17], "factor": [0, 3, 9, 17], "ensur": [0, 14], "remain": [0, 4, 14], "finit": [0, 3, 19], "random": [0, 1, 3, 5, 8, 10, 13, 16, 17, 19], "variabl": [0, 10], "induc": [0, 10], "polici": [0, 2, 6, 9, 17, 19, 21, 22], "pi": [0, 1, 3, 5, 6, 11, 13, 14, 16, 17], "map": [0, 3, 4, 8], "distribut": [0, 6, 7, 8, 10, 12, 13, 14, 19], "over": [0, 6, 13, 20, 21], "when": [0, 6, 9, 10, 14, 15, 16, 19, 21], "follow": [0, 1, 2, 3, 4, 5, 7, 8, 10, 12, 13, 14, 16, 17, 19, 20, 21], "p_": [0, 3, 7, 10, 11, 13, 14], "_0": [0, 3, 6, 10, 11, 13, 16, 17], "prod_": [0, 10], "_t": [0, 2, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19], "_": [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "under": [0, 4, 20], "expect": [0, 3, 6, 8, 9, 10, 13, 14, 16, 17, 19], "eta": [0, 14], "mathbb": [0, 3, 4, 6, 7, 8, 10, 11, 12, 13, 14], "e": [0, 3, 6, 7, 8, 10, 11, 12, 13, 14, 17, 20], "sim": [0, 3, 7, 8, 10, 11, 12, 13, 14, 16], "The": [0, 1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 22], "goal": [0, 3, 6, 13, 19], "find": [0, 2, 3, 6, 7, 9, 10, 12, 13, 14, 20], "maxim": [0, 2, 3, 10, 13, 14, 20, 21], "two": [0, 1, 2, 4, 5, 6, 7, 8, 9, 13, 14, 20], "quantiti": 0, "particular": [0, 20, 21], "interest": 0, "ar": [0, 2, 3, 5, 6, 7, 10, 12, 13, 14, 16, 17, 19, 20, 21, 22], "v": [0, 1, 3, 4, 7, 9, 10, 11, 13, 16, 21], "q": [0, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16], "from": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "v_": [0, 1, 3, 4, 6, 7, 13, 16], "appli": [0, 1, 3, 6, 8, 9, 10], "law": [0, 3], "total": [0, 3, 6, 14], "can": [0, 1, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 19, 20, 21], "relat": 0, "begin": [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 13, 14], "align": [0, 1, 3, 4, 7, 8, 9, 10, 11, 13, 14], "end": [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16], "sinc": [0, 1, 3, 4, 6, 7, 9, 14, 16, 17], "cannot": [0, 10, 21], "optim": [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 13, 17, 19, 20, 21], "prior": 0, "equival": 0, "all": [0, 1, 2, 3, 6, 9, 12, 16, 17, 20], "other": [0, 3, 4, 7, 8, 9, 16, 17], "hand": [0, 5, 6, 9, 17], "q_": [0, 1, 3, 6, 8, 11, 13, 19, 20], "again": [0, 6, 12], "cdot": [0, 4, 12, 13, 14, 16, 17, 20], "In": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 20, 21], "next": [0, 6, 10, 16], "section": [0, 2, 6, 9, 10, 11, 16, 17, 22], "introduc": [0, 4, 8, 9, 19, 21], "simpl": [0, 7, 19], "algorithm": [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 16, 17, 19, 20, 21], "estim": [0, 1, 3, 4, 5, 8, 10, 12, 13, 14, 15, 16, 17, 20], "us": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 19, 20, 21], "anoth": [1, 5, 7, 14, 20, 21], "wai": [1, 5, 6, 8, 10, 12, 17], "solv": [1, 2, 3, 12, 14, 19, 21], "problem": [1, 2, 5, 6, 8, 10, 12, 14, 15, 21], "instead": [1, 9, 10, 12, 14, 16, 17, 19, 21], "learn": [1, 7, 8, 9, 10, 12, 13, 14], "valu": [1, 2, 5, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21, 22], "function": [1, 2, 3, 4, 5, 6, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "gradual": [1, 14], "imporv": 1, "step": [1, 5, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "current": [1, 6, 14, 17, 22], "pi_k": 1, "updat": [1, 2, 3, 4, 6, 7, 10, 12, 13, 14, 16, 17, 19, 20], "pi_": [1, 5, 7, 8, 10, 11, 12, 13, 14], "k": [1, 4, 7, 8, 9, 10, 14], "increas": [1, 5, 6, 8, 10], "By": [1, 3, 8, 9, 20], "bellman": [1, 2, 3, 4, 13], "equat": [1, 3, 4, 8, 9], "have": [1, 3, 4, 5, 6, 7, 8, 9, 14, 16, 21], "r_0": [1, 3, 4, 10], "approach": [1, 3, 5, 6, 7, 8, 12, 16, 19], "similar": [1, 3, 5, 16, 17], "psuedocod": [1, 7, 8, 14, 16, 17, 20], "prsent": 1, "below": [1, 3, 5, 6, 7, 8, 9, 10, 12, 14, 16, 17, 19, 20, 21], "note": [1, 4, 5, 7, 9, 10, 12, 14, 16, 17, 20], "onli": [1, 6, 7, 8, 16, 19, 20], "differ": [1, 5, 6, 9, 12, 14, 20], "rule": [1, 8, 10, 14, 17, 19], "input": [1, 3, 7, 8, 10, 13, 14, 16, 17, 20], "instanc": [1, 2, 3, 5, 6, 11], "output": [1, 3, 8, 16, 17, 21], "comput": [1, 2, 3, 6, 7, 8, 9, 10, 14, 16, 20, 21], "initi": [1, 3, 5, 8, 10, 14, 16, 17, 19, 20], "arrai": [1, 3, 8], "while": [1, 2, 3, 6, 7, 8, 9, 10, 13, 16, 17, 19, 20, 21], "converg": [1, 2, 3, 14, 16, 19], "leftarrow": [1, 3, 4, 7, 8, 10, 13, 14, 16, 17, 20], "consid": [1, 10, 12, 14, 20, 21], "action": [1, 2, 3, 5, 7, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "fucntion": [1, 3, 5, 6], "intuit": [1, 8, 12, 20, 21], "exist": [1, 4], "Then": [1, 14], "get": [1, 3, 5, 7, 8, 9, 10, 12, 13, 14, 17, 19, 20, 21], "better": [1, 5, 7, 14], "payoff": [1, 6], "choos": [1, 7, 9, 17, 20], "motiv": [1, 10, 14], "u": [1, 4, 6, 7, 8, 10, 14, 17], "our": [1, 3, 5, 7, 8, 12, 14, 16, 17, 19], "greedili": [1, 3, 5], "hat": [1, 17], "underset": [1, 3, 10, 13, 14, 17, 20], "text": [1, 3, 5, 7, 10, 12, 13, 14, 17, 20], "argmax": [1, 3, 10, 14, 17, 19, 20, 21], "theorem": [1, 4, 8, 14], "prove": [1, 4, 14], "good": [1, 3, 7], "let": [1, 3, 4, 8, 9, 10, 11, 12, 14], "pair": [1, 20], "determinist": [1, 5, 6], "hspace": [1, 7, 12, 14], "5mm": [1, 12, 14], "proof": [1, 4, 11, 14], "omit": 1, "now": [1, 2, 3, 4, 5, 7, 10, 11, 13, 14, 16, 19, 20], "olici": 1, "contain": [1, 19, 20], "sequenc": [1, 14], "pi_0": [1, 14], "xrightarrow": 1, "pi_1": 1, "leq": [1, 4, 12, 14], "pi_n": 1, "eventu": 1, "displai": [1, 5, 14], "same": [1, 3, 5, 6, 7, 8, 17, 19, 20], "gridworld": [1, 3], "enviro": [1, 8, 10, 14, 16, 17, 21], "befor": [1, 4, 5, 6, 7, 10, 11, 19, 20], "import": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "numpi": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "np": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "matplotlib": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "pyplot": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "plt": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "seed": [1, 3], "def": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "policy_evalu": 1, "grid_world": [1, 3], "max_it": [1, 3, 5, 6], "int": [1, 3, 5, 6, 10, 16, 17, 19, 20], "float": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "perform": [1, 2, 3, 5, 7, 8, 9, 10, 12, 13, 14, 16, 20, 21], "height": [1, 3], "width": [1, 3], "zero": [1, 3, 14, 20], "rang": [1, 3, 5, 6, 7, 9, 10, 12, 14, 16, 17, 19, 21], "get_stat": [1, 3], "successor": [1, 3], "get_available_act": [1, 3], "prob_dist": [1, 3], "get_transition_prob": [1, 3], "prob": [1, 3, 7, 10, 12, 14], "item": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "get_reward": [1, 3], "return": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "policy_improv": 1, "dict": [1, 3], "available_act": [1, 3], "q_valu": [1, 3, 5, 6, 8, 17, 19, 20, 21], "max": [1, 3, 5, 12, 14, 17, 19, 21], "kei": [1, 3, 5, 6, 17], "len": [1, 3, 5, 6, 8, 12, 13, 19, 20, 21], "els": [1, 3, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "none": [1, 3, 7, 8], "policy_iter": 1, "get_random_polici": 1, "histori": [1, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "append": [1, 12, 14, 19], "setup": [1, 17], "ran": [1, 5, 6, 17], "10": [1, 6, 7, 8, 10, 12, 14, 16, 17, 20], "show": [1, 4, 8, 12], "4": [1, 7, 9, 10, 12, 13, 14, 16, 17, 21], "9": [1, 10, 13, 14, 19], "5": [1, 3, 5, 8, 9, 12, 13, 14, 19, 20], "exit": [1, 3], "good_exit": [1, 3], "3": [1, 3, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "bad_exit": [1, 3], "2": [1, 3, 6, 7, 8, 9, 10, 12, 13, 16, 19, 20, 21], "wall": [1, 3], "living_reward": [1, 3], "win_reward": [1, 3], "lose_reward": [1, 3], "init_po": [1, 3], "know": [2, 3, 6], "iter": [2, 5, 6, 17], "method": [2, 5, 9, 14, 17, 19, 22], "both": [2, 3, 6, 7, 8, 9, 14, 20, 21], "guarante": 2, "empir": [2, 21], "typic": [2, 3, 7, 9, 19], "faster": 2, "therefor": [2, 3, 7, 14], "often": [2, 9, 14, 19, 20], "prefer": [2, 17], "howev": [2, 3, 5, 7, 8, 9, 10, 12, 14, 19, 20], "requir": [2, 6, 7], "knowledg": 2, "without": [2, 6, 10, 19, 21], "becom": [2, 6, 14, 19, 20], "infeas": [2, 19], "real": 2, "life": 2, "scenario": [2, 20], "unknown": 2, "too": [2, 12, 14], "complex": [2, 19], "For": [2, 3, 4, 5, 7, 8, 10, 11, 12, 14, 16, 17, 19], "exampl": [2, 14, 19], "game": 2, "blackjack": [2, 5], "although": [2, 3, 9], "could": 2, "theoret": [2, 14], "possibl": [2, 3, 6, 9, 13, 19], "do": [2, 3, 8, 14], "so": [2, 3, 7], "would": [2, 7], "extrem": 2, "challeng": [2, 6, 7, 14, 19, 20], "aim": 2, "need": [2, 3, 7, 8, 10, 12, 14, 17, 20, 21], "discuss": [2, 9, 10, 14, 16, 17], "class": [2, 5, 6, 7, 8, 9, 10, 12, 13, 14, 17, 19, 20, 21], "base": [2, 5, 6, 7, 10, 14, 20, 22], "These": [2, 22], "attempt": 2, "them": [2, 10, 20], "control": [2, 6, 8, 9, 16], "parameter": [2, 7, 10, 13, 19], "directli": [2, 4, 6, 7, 8, 10, 12, 14, 19], "want": [3, 4, 10, 13], "turn": [3, 6], "out": [3, 14], "each": [3, 5, 6, 10, 14, 16, 17, 19, 21], "recal": [3, 4, 7, 21], "r_": [3, 9], "determin": [3, 5, 6, 17], "sampl": [3, 5, 6, 7, 8, 10, 12, 13, 14, 16, 17, 19, 20, 21], "To": [3, 4, 5, 6, 7, 8, 10, 12, 14, 17, 19, 20, 21], "evalu": [3, 5, 6, 8, 13, 20, 21], "first": [3, 4, 5, 7, 8, 9, 14], "simplifi": [3, 9, 10, 14], "bigg": [3, 4, 7, 8, 10, 11, 12, 13, 14], "third": 3, "line": 3, "final": [3, 5, 7, 14], "markov": [3, 13, 22], "properti": [3, 19], "assum": [3, 10], "size": [3, 7, 8, 10, 12, 14, 15, 19, 20, 21], "tag": 3, "abov": [3, 4, 9, 10, 11, 13, 14, 20, 21], "known": [3, 5, 8, 10, 13, 17, 19, 20], "its": [3, 6, 7, 8, 10, 14, 20], "correspond": [3, 7, 14], "linear": [3, 13], "system": [3, 14], "deriv": [3, 6, 8, 10, 11], "result": [3, 4, 5, 7, 8, 10, 12, 13, 14, 19, 20], "argument": 3, "hold": [3, 19], "also": [3, 5, 6, 9, 10, 14, 19], "denot": [3, 9, 11, 14, 16], "respect": 3, "max_": [3, 4, 12, 14, 17, 19, 20, 21], "substitut": [3, 9, 10], "see": [3, 7, 9, 10, 14, 17], "must": [3, 6, 7], "satisfi": [3, 9], "condit": [3, 6, 10], "practic": [3, 7, 14], "test": [3, 5, 6, 7, 8, 9, 10, 14, 17, 20, 21], "environ": [3, 5, 6, 7, 10, 16, 17, 19, 20, 21], "includ": [3, 14], "green": 3, "bad": 3, "red": 3, "black": [3, 16], "normal": [3, 8, 9, 12, 13], "block": 3, "white": 3, "reach": 3, "avoid": [3, 8, 20], "If": [3, 5, 7, 14, 20], "win": 3, "convers": [3, 7], "lose": 3, "value_iter": 3, "param": [3, 5, 6, 7, 8, 9, 10, 13, 14, 19, 20, 21], "grid": 3, "world": 3, "maximum": 3, "number": [3, 6, 7, 8, 9, 10, 13, 19], "allow": [3, 5, 7, 10, 16], "execut": 3, "futur": [3, 16, 17], "2d": 3, "x": [3, 6, 8, 14], "y": [3, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "entri": [3, 19], "repres": [3, 4, 7, 14, 16, 19], "max_valu": 3, "inf": 3, "instanti": 3, "time": [3, 6, 9, 16, 17, 19, 20], "live": [3, 16], "encourag": [3, 13], "soon": 3, "init": 3, "randomli": [3, 20], "100": [3, 9, 10, 12, 16, 17, 19], "shown": [3, 5, 6, 7, 8, 9, 12, 14, 16, 17, 19], "closer": 3, "gener": [3, 7, 10, 12, 13, 14, 16, 17], "higher": [3, 5, 6, 16], "than": [3, 7, 14], "display_valu": [3, 16], "how": [3, 7, 14, 17], "word": [3, 4, 7, 16], "select": [3, 8, 20], "best": [3, 8, 12], "onc": [3, 6, 14, 19], "extract": [3, 5], "extract_q_valu": 3, "ndarrai": 3, "dictionari": 3, "tupl": 3, "coordin": 3, "inner": 3, "extract_polici": 3, "opt_act": [3, 20], "display_qvalu": 3, "illustr": 3, "more": [3, 7, 9, 10, 13, 14, 17, 19, 21], "like": [3, 6, 7, 8, 14, 19, 20, 21], "move": [3, 5, 8, 17, 19, 20], "awai": [3, 16, 17], "attract": 3, "toward": 3, "display_polici": [3, 17], "v_k": 4, "inde": 4, "oper": [4, 10, 13, 21], "rightarrow": [4, 8], "mean": [4, 6, 7, 8, 9, 10, 12, 13, 14, 19, 21], "vector": [4, 14], "notat": [4, 9, 14], "compactli": 4, "infti": [4, 9], "contract": 4, "sup": 4, "norm": 4, "check": 4, "With": [4, 11, 13], "standard": [4, 13], "fix": 4, "point": [4, 19], "lim_": 4, "v_0": 4, "impli": [4, 10, 14, 21], "try": [5, 13, 14], "One": [5, 6, 8, 10, 12, 14, 15, 16, 17, 19, 20, 21], "straightforward": [5, 19], "style": [5, 8], "altern": [5, 7], "between": [5, 6, 9, 17, 19, 20], "provid": [5, 9, 11, 14, 16, 17, 20], "poor": [5, 19], "unexplor": [5, 6], "risk": [5, 17], "overlook": 5, "might": [5, 6], "offer": [5, 9], "address": [5, 6, 8, 10, 12, 14, 19, 20], "propos": [5, 8, 9, 10, 12, 14, 17, 20, 21], "modif": [5, 21], "simpli": 5, "involv": [5, 13], "episod": [5, 6, 7, 8, 9, 10, 12, 13, 16, 17, 19, 20], "idea": [5, 17], "achiev": [5, 6, 7, 8, 9], "coverag": 5, "implement": [5, 17, 19], "one": [5, 8, 9, 10, 14, 19, 20], "improv": [5, 8, 9, 10, 13, 20, 21], "montecarlocontrol": 5, "__init__": [5, 6, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "self": [5, 6, 7, 8, 9, 10, 12, 13, 14, 17, 19, 20, 21], "list": [5, 6, 16, 17], "avail": [5, 6, 16], "env": [5, 6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "every_visit": [5, 6], "bool": [5, 6], "fals": [5, 6, 8, 9, 10, 13, 16, 17, 19, 20, 21], "form": [5, 6, 17], "state1": [5, 6, 17], "value1": [5, 6], "count": [5, 6], "cum_reward": [5, 6, 17], "state_act": [5, 6], "enumer": [5, 6, 12], "state_action_dict": 5, "strategi": [5, 6, 17], "run": [5, 9, 16], "000": 5, "separ": [5, 20, 21], "case": [5, 13, 19, 21], "whether": [5, 7, 14], "player": [5, 6], "ha": [5, 6, 7, 9, 14, 16, 20], "usabl": 5, "ac": [5, 6], "dealer": [5, 6], "hit": [5, 6], "stick": [5, 6], "stick_at_20_or_21": [5, 6], "mc": [5, 7], "optimal_polici": 5, "5000000": 5, "plotblackjackoptimalstrategi": 5, "place": [5, 17], "strong": 5, "assumpt": [5, 19], "chang": [5, 14], "realist": 5, "some": [5, 9, 12, 14, 20], "set": [5, 8, 12, 14, 16, 17, 19, 20], "epsilon": [5, 12, 14, 17, 19, 20, 21], "greedi": [5, 17], "probabl": [5, 6, 7], "small": [5, 12, 14, 15], "therebi": 5, "chanc": [5, 16, 20], "seemingli": 5, "suboptim": 5, "help": [5, 8, 17, 19, 20, 21], "much": [5, 7, 9, 14, 16], "smoother": 5, "decis": [5, 13, 22], "boundari": 5, "consist": 5, "solut": [5, 14, 17, 19], "onlin": 5, "stick_at_20_or_21_stochast": 5, "montecarlo": [5, 6], "plot_q_values_heatmap": [5, 6], "Not": 6, "access": 6, "present": [6, 7, 8, 10, 11, 14], "longer": [6, 17], "program": [6, 14], "suffici": [6, 7], "depend": 6, "come": 6, "around": [6, 8, 16, 17], "through": [6, 9, 10], "approx": [6, 7, 9, 10, 14], "frac": [6, 7, 8, 10, 12, 13, 14, 21], "n": [6, 7, 8, 10, 13, 14, 16, 17, 19, 20, 21], "x_i": [6, 14], "suggest": [6, 7, 9, 14, 21], "culmul": 6, "There": [6, 14], "variant": [6, 12, 19], "specif": [6, 10, 19], "encount": 6, "until": [6, 7, 14], "here": 6, "within": [6, 19], "averag": [6, 8, 9, 20], "throughout": 6, "openai": [6, 16], "gym": [6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "close": [6, 17], "21": 6, "exceed": 6, "card": 6, "deck": 6, "worth": [6, 11], "face": 6, "either": [6, 7, 12], "11": 6, "dure": [6, 8, 16, 19, 20], "gameplai": 6, "decid": 6, "draw": 6, "addit": [6, 13], "keep": 6, "compet": 6, "against": 6, "who": 6, "least": 6, "17": 6, "pip": [6, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "instal": [6, 7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "itertool": 6, "seaborn": 6, "sn": 6, "make": [6, 7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "v1": [6, 7, 9, 10, 12, 14, 19, 20, 21], "natur": 6, "sab": 6, "player_sum_rang": 6, "32": [6, 8, 10, 13, 19, 20], "dealer_card_rang": 6, "usable_ace_rang": 6, "true": [6, 9, 12, 13, 14, 16, 17, 20, 21], "product": 6, "print": [6, 13], "f": [6, 13, 14], "640": 6, "20": [6, 12], "otherwis": 6, "500000": 6, "And": [6, 12, 13, 22], "montecarlovalu": 6, "n_episod": [6, 10, 16, 17, 19, 20, 21], "plot_valu": 6, "modifi": [6, 17], "slightli": [6, 7, 9, 10, 20], "montecarloqvalu": 6, "action1": [6, 17], "q11": [6, 17], "action2": [6, 17], "q12": [6, 17], "state2": [6, 17], "q21": [6, 17], "q22": [6, 17], "difficult": [6, 12], "issu": [6, 8, 10, 14, 20, 21], "particularli": [6, 19, 21], "problemat": 6, "gradient": [7, 13, 15, 20], "j": [7, 8, 10, 11, 13], "theta": [7, 8, 10, 11, 12, 13, 14, 20], "sum_t": 7, "nabla_": [7, 8, 10, 11, 14], "log": [7, 10, 13, 14], "serv": [7, 9], "unbias": [7, 9], "further": [7, 10], "reduc": [7, 9, 14], "varianc": [7, 9, 10, 19], "baselin": 7, "subtract": 7, "give": [7, 8, 10, 14], "b": [7, 13, 20], "even": [7, 14, 20], "though": [7, 14], "formula": [7, 9], "hard": [7, 12], "heurist": 7, "written": 7, "likelihood": 7, "weight": [7, 8, 9, 20], "call": [7, 8, 9], "term": [7, 9, 14], "advantag": [7, 12, 14, 21], "natrual": 7, "interpret": 7, "wors": [7, 16], "default": 7, "postiv": 7, "rais": [7, 13], "neg": 7, "lower": [7, 9, 14], "question": 7, "phi": [7, 8, 13, 19], "neural": [7, 8, 10, 13, 14, 19, 20, 21], "network": [7, 8, 10, 13, 14, 19, 20, 21], "choic": [7, 8, 14, 17], "3mm": 7, "valid": 7, "former": 7, "loss": [7, 9, 10, 19, 20, 21], "object": [7, 10, 11, 12, 21], "still": [7, 8, 9, 13], "specifi": 7, "target": [7, 8, 9, 12, 13, 14, 19, 20, 21], "done": [7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "primari": [7, 20], "fit": 7, "data": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "squar": [7, 19], "error": [7, 14, 19, 20], "l": [7, 8, 12, 14, 19], "wait": 7, "entir": 7, "been": [7, 14], "calcul": [7, 21], "increment": 7, "previou": [7, 9, 11, 17], "full": 7, "differenti": [7, 8, 10, 13], "rate": [7, 8, 10, 13, 17], "alpha": [7, 8, 10, 13, 14, 16, 17, 20], "batch": [7, 8, 10, 12, 13, 15, 19, 20], "4mm": 7, "type": 7, "cartpol": [7, 10, 12, 14, 19, 20, 21], "pybullet": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "stabl": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "baselines3": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "extra": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "pyvirtualdisplai": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "apt": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "xvfb": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "pybullet_env": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "imageio": [7, 8, 9, 10, 12, 13, 14, 16, 17, 19, 20, 21], "torch": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "nn": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "util": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "dataset": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "dataload": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "devic": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "cuda": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "is_avail": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "cpu": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "modul": [7, 8, 10, 12, 13, 14, 19, 20, 21], "state_dim": [7, 8, 10, 12, 13, 14, 19, 20, 21], "action_dim": [7, 8, 10, 12, 13, 14, 19, 20, 21], "hidden_dim": [7, 8, 10, 12, 13, 14, 19, 20, 21], "128": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "super": [7, 8, 10, 12, 13, 14, 19, 20, 21], "net": [7, 8, 10, 12, 13, 14], "simpleneuralnetwork": [7, 8, 10, 12, 14, 19, 20, 21], "in_dim": [7, 8, 10, 12, 14, 19, 20, 21], "out_dim": [7, 8, 10, 12, 13, 14, 19, 20, 21], "final_lay": [7, 10, 12, 14], "softmax": [7, 10, 12, 14], "forward": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "select_act": [7, 8, 10, 12, 13, 14, 19, 20, 21], "tensor": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "action_dist": [7, 10, 12, 13, 14], "categor": [7, 8, 10, 12, 14], "log_prob": [7, 9, 10, 12, 13], "valuenetwork": [7, 9, 13], "squeez": [7, 8, 13, 20], "actorcriticmcag": 7, "lr": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "1e": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "policy_optim": [7, 8, 9, 12, 13], "adam": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "paramet": [7, 8, 9, 10, 12, 13, 14, 16, 19, 20, 21], "value_optim": [7, 9, 12, 13, 14], "learn_episod": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "batch_siz": [7, 8, 9, 10, 12, 13, 19, 20, 21], "max_step": [7, 8, 9, 10, 12, 13, 14, 17, 19, 20, 21], "value_loss": [7, 9, 12, 13, 14], "policy_loss": [7, 8, 9, 12, 13], "sample_trajectori": [7, 9, 12, 14], "rewards_cum": 7, "cumsum": 7, "flip": 7, "dim": [7, 8, 13, 19, 21], "mseloss": [7, 8, 9, 12, 13, 20, 21], "get_action_log_prob": [7, 12, 14], "detach": [7, 8, 9, 12, 13, 14, 19, 20], "sum": [7, 9, 12, 13, 14], "zero_grad": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "backward": [7, 8, 9, 10, 12, 13, 14, 19, 20, 21], "hidden": [7, 8, 10, 14], "unit": [7, 8, 14], "train": [7, 8, 10, 12, 13, 17, 19, 20, 21], "3500": [7, 9], "figur": [7, 16, 17], "cartpolebulletenv": [7, 9, 10, 12, 14, 19, 20, 21], "print_everi": [7, 8, 9, 10, 12, 19, 20, 21], "50": [7, 12, 13, 20, 21], "actorcritictdag": 7, "no_grad": [7, 8, 9, 12, 13, 14, 19, 20, 21], "architectur": [7, 19, 21], "notabl": [7, 12], "wa": [7, 8], "abl": 7, "effect": [7, 9, 19, 21], "becaus": [7, 9, 10, 17, 19, 21], "being": [7, 19, 20], "bias": [7, 9, 20], "accur": [7, 14], "seen": 7, "plot": [7, 10, 12], "exhibit": [7, 9, 19], "larger": [7, 10, 14, 19], "variat": [7, 9, 14], "citat": [7, 8, 9, 10, 12, 14, 19, 20, 21], "http": [7, 8, 9, 10, 12, 14, 19, 20, 21], "lilianweng": [7, 10], "github": [7, 10], "io": [7, 10, 14], "post": [7, 10], "2018": [7, 10], "04": [7, 10], "08": [7, 10], "rail": [7, 10], "eec": [7, 10], "berkelei": [7, 10, 22], "edu": [7, 10], "deeprlcours": [7, 10], "dpg": 8, "silver": 8, "et": [8, 9, 10, 12, 14, 19, 21], "al": [8, 9, 10, 12, 14, 19, 21], "2015": [8, 21], "unlik": [8, 12], "tradit": [8, 21], "stochast": [8, 14], "produc": 8, "gaussian": 8, "focus": 8, "author": [8, 9, 11, 14, 21], "analog": 8, "determinst": 8, "mu": 8, "express": [8, 9, 10, 14], "mu_": 8, "dircet": 8, "most": 8, "chain": 8, "mont": [8, 10, 16, 17], "carlo": [8, 10, 16, 17], "subsequ": 8, "paper": [8, 10, 11, 14, 20, 21], "reinforc": [8, 15, 19], "lillicrap": 8, "replai": [8, 20], "buffer": [8, 13, 19, 20, 21], "process": [8, 13, 19, 22], "experi": [8, 10, 12], "store": [8, 19, 20], "minibatch": 8, "_i": [8, 14, 19], "r_i": [8, 19], "y_i": 8, "minim": [8, 19], "nabla": 8, "nabla_a": 8, "soft": [8, 12], "thei": 8, "exponenti": [8, 9], "enhanc": [8, 19], "stabil": [8, 9, 19, 20, 21], "005": [8, 13], "unvisit": 8, "ad": [8, 14, 22], "nois": [8, 10], "chosen": 8, "suit": 8, "ornstein": 8, "uhlenbeck": 8, "dx_t": 8, "x_tdt": 8, "sigma": 8, "dw_t": 8, "invers": [8, 14], "pendulum": 8, "task": [8, 12, 19, 20], "layer": [8, 10, 14], "collect": [8, 14, 19, 20], "dequ": [8, 19, 20], "clip_grad_norm_": [8, 12], "n_layer": [8, 12, 14, 19, 20, 21], "shape": [8, 13, 14, 19, 20, 21], "unsqueez": [8, 13, 19, 20, 21], "qvalu": 8, "qvaluenetwork": [8, 13], "cat": [8, 13, 14], "ornsteinuhlenbeckprocess": 8, "reset": [8, 10, 13, 16, 17, 19, 20, 21], "whole": 8, "15": [8, 12], "dt": 8, "x0": 8, "reset_st": 8, "x_prev": 8, "sqrt": [8, 14], "zeros_lik": [8, 14], "deterministicpolicygradientag": 8, "observation_spac": [8, 13, 14, 16, 17, 19, 20, 21], "target_polici": 8, "target_q_valu": [8, 20], "load_state_dict": [8, 12], "state_dict": [8, 12], "q_value_optim": 8, "ou_process": 8, "replaybuff": [8, 13, 19, 20, 21], "maxlen": [8, 13, 19, 20, 21], "10000": [8, 12, 13, 14, 17, 19, 20, 21], "grad_clip_valu": 8, "episode_loss": [8, 10, 12, 19, 21], "clip": [8, 12], "next_stat": [8, 10, 13, 14, 19, 20, 21], "episode_reward": [8, 10, 12, 13, 19, 21], "add": [8, 13, 19, 20, 21], "q_value_loss": 8, "soft_upd": 8, "target_model": 8, "target_param": [8, 13, 20], "zip": [8, 9, 12, 13, 19, 20], "copy_": [8, 13, 20], "500": [8, 12, 17, 20], "epoch": [8, 9, 12, 17, 20], "indic": 8, "unstabl": [8, 14], "sensit": 8, "hyperparamet": [8, 12, 19, 20], "despit": 8, "save": 8, "capabl": 8, "deliv": 8, "decent": 8, "invertedpendulumbulletenv": [8, 13], "v0": [8, 13, 16, 17], "99": [8, 13], "proceed": [8, 10, 20], "mlr": 8, "press": 8, "v32": 8, "silver14": 8, "pdf": [8, 9, 10, 12, 19, 20, 21], "arxiv": [8, 9, 12, 14, 19, 20, 21], "org": [8, 9, 12, 14, 19, 20, 21], "1509": [8, 20], "02971": 8, "actor": 9, "critic": [9, 12], "latter": 9, "grow": 9, "linearli": 9, "bia": [9, 20], "tradeoff": 9, "high": [9, 13], "second": [9, 14], "balanc": 9, "trade": 9, "off": 9, "2016": 9, "schulman": [9, 12, 14], "gae": 9, "design": 9, "combin": [9, 21], "look": 9, "2r": 9, "strength": 9, "clear": 9, "should": [9, 14, 20], "name": [9, 12, 13], "lambda": 9, "scalar": 9, "delta_t": 9, "tempor": [9, 19], "residu": 9, "reexpress": 9, "2r_": 9, "delta_": 9, "whera": 9, "recov": [9, 21], "tune": [9, 12], "expens": [9, 16], "wise": 9, "easi": 9, "recurr": 9, "a_t": [9, 12], "a_": [9, 14], "effici": [9, 19], "recurs": 9, "work": [9, 21], "actorcriticgaeag": 9, "policy_schedul": [9, 13], "lr_schedul": [9, 10, 12, 13, 19], "steplr": [9, 10, 12, 13, 19], "step_siz": [9, 10, 12, 13, 19], "1000": [9, 16, 17, 19, 21], "value_schedul": [9, 13], "lamb": 9, "estimate_advantag": [9, 12], "next_valu": [9, 12], "revers": [9, 12, 14], "delta": [9, 12, 14], "insert": [9, 12], "std": [9, 12, 13], "8": [9, 10, 12, 14, 19, 20], "graph": 9, "appear": [9, 20], "fastest": 9, "after": [9, 16, 20], "signific": [9, 20, 21], "On": 9, "slower": 9, "demonstr": [9, 19, 21], "long": [9, 14, 20], "lambda_": [9, 12], "00": 9, "1506": 9, "02438": 9, "contrast": [10, 19], "explicitli": 10, "formal": [10, 11, 16], "underbrac": 10, "fashion": 10, "tau_i": [10, 13], "troubl": 10, "part": 10, "reformul": 10, "mention": [10, 11], "earlier": 10, "unrol": 10, "regular": 10, "interchang": 10, "integr": 10, "d": [10, 12, 14, 19, 20], "trick": 10, "statement": 10, "sutton": [10, 22], "1999": 10, "what": 10, "remark": 10, "about": 10, "approxim": [10, 13, 14, 17, 19], "essenti": 10, "pseudocod": [10, 14], "dimens": [10, 13], "ob": [10, 12, 13, 14], "action_log_prob": [10, 12, 13], "reinforceag": 10, "str": [10, 19, 20], "schedul": [10, 12, 19], "vari": 10, "300": 10, "decai": 10, "partli": 10, "fact": [10, 14], "smaller": [10, 14], "explan": [10, 17], "neurip": [10, 20], "cc": [10, 20], "paper_fil": [10, 20], "file": [10, 20], "464d828b85b0bed98e80ade0a5c43b0f": 10, "rather": 11, "inform": [11, 19], "origin": [11, 14, 20], "rewrit": 11, "mind": 11, "label": [11, 12], "drawback": [12, 16], "trust": 12, "region": 12, "2017": [12, 14], "trpo": 12, "theta_": [12, 14], "old": [12, 13, 14], "subject": [12, 14], "rho": [12, 14], "kl": [12, 13], "diverg": 12, "constraint": [12, 13], "beta": 12, "singl": 12, "well": 12, "across": 12, "surrog": [12, 14], "min": [12, 13], "prevent": [12, 13], "far": [12, 16, 17], "ratio": [12, 14], "back": 12, "copi": [12, 14, 20], "ppoagent": 12, "ep": 12, "policy_old": 12, "old_log_prob": [12, 13, 14], "new_log_prob": [12, 13, 14], "exp": [12, 13, 14], "surr1": 12, "surr2": 12, "clamp": 12, "five": 12, "were": 12, "2000": [12, 13], "larg": [12, 14, 15, 19], "did": 12, "yield": 12, "load": 12, "user": 12, "raymondtsao": 12, "desktop": 12, "reinforcementlearningnot": 12, "policygradi": 12, "e005_reward": 12, "npy": 12, "e010_reward": 12, "e015_reward": 12, "e020_reward": 12, "e0": 12, "05": 12, "color": 12, "smooth": 12, "smoothed_reward": 12, "start_index": 12, "c": [12, 14], "titl": 12, "xlabel": 12, "ylabel": 12, "legend": 12, "1707": 12, "06347": 12, "formul": 13, "h": [13, 14], "explor": 13, "robust": [13, 14], "framework": 13, "tierat": 13, "argmin": 13, "d_": [13, 14], "z_": 13, "sequenti": 13, "relu": 13, "log_std": 13, "chunk": 13, "action_rang": 13, "tanh": 13, "randn_lik": 13, "pow": [13, 19], "6": [13, 16], "keepdim": [13, 21], "except": 13, "occur": [13, 16], "local": 13, "softactorcriticag": 13, "target_valu": 13, "q_value1": 13, "q_value2": 13, "q_value_optimizer1": 13, "q_value_optimizer2": 13, "150": 13, "7": 13, "q_value_scheduler1": 13, "q_value_scheduler2": 13, "train_value_loss": 13, "train_q_value1_loss": 13, "train_q_value2_loss": 13, "train_policy_loss": 13, "track": 13, "log_action_prob": 13, "old_stat": 13, "old_act": 13, "new_act": [13, 17], "new_q_value1": 13, "new_q_value2": 13, "new_q_valu": 13, "retain_graph": 13, "old_q_value1": 13, "old_q_value2": 13, "q_value_loss1": 13, "q_value_loss2": 13, "left": 14, "right": [14, 16], "suppos": 14, "tild": 14, "rho_": 14, "visit": 14, "frequenc": 14, "replac": 14, "sens": [14, 16], "match": 14, "order": 14, "theta_0": 14, "isn": 14, "unclear": 14, "insight": 14, "influenc": 14, "tv": 14, "measur": 14, "distanc": 14, "tight": 14, "cd_": 14, "monoton": 14, "pi_i": 14, "why": [14, 20], "m_i": 14, "definit": 14, "sound": 14, "necessari": 14, "tractabl": 14, "usal": 14, "simplif": 14, "intract": 14, "lot": 14, "TO": 14, "overlin": 14, "theta_1": 14, "theta_2": 14, "manag": 14, "theta_k": 14, "g": 14, "th": 14, "hessian": 14, "matrix": 14, "quadrat": 14, "It": [14, 17, 19, 20], "analyt": 14, "theta_i": 14, "g_i": 14, "h_i": 14, "numer": 14, "thing": 14, "hx": 14, "popular": 14, "conjug": 14, "descent": [14, 20], "found": 14, "adjust": 14, "h_ix_i": 14, "touch": 14, "ve": 14, "made": 14, "due": [14, 20], "veri": 14, "bring": 14, "maximium": 14, "delta_k": 14, "g_k": 14, "x_k": 14, "delta_i": 14, "accept": 14, "break": [14, 19], "last": 14, "tqdm": 14, "o": 14, "base64": 14, "ipython": 14, "ipythondisplai": 14, "datetim": 14, "stable_baselines3": 14, "ppo": 14, "sy": 14, "warn": 14, "filterwarn": 14, "ignor": 14, "categori": 14, "userwarn": 14, "deprecationwarn": 14, "logger": 14, "set_level": 14, "action_prob": 14, "gather": [14, 20], "trpoagent": 14, "lmbda": 14, "action_spac": [14, 16, 17, 19, 20, 21], "compute_advantag": 14, "advantage_list": 14, "dtype": 14, "hessian_product": 14, "old_dist": 14, "new_dist": 14, "kl_diverg": 14, "kl_grad": 14, "autograd": 14, "grad": 14, "create_graph": 14, "view": 14, "kl_grad_product": 14, "dot": 14, "hessian_matrix_product": 14, "conjugate_gradi": 14, "tol": 14, "clone": 14, "r_r": 14, "hp": 14, "new_r_r": 14, "line_search": 14, "max_direct": 14, "old_param": 14, "convert_paramet": 14, "parameters_to_vector": 14, "old_surrog": 14, "new_param": 14, "new_polici": 14, "deepcopi": [14, 20], "vector_to_paramet": 14, "new_surrog": 14, "mse_loss": 14, "td_target": [14, 20], "surrogate_grad": 14, "direct": 14, "env_nam": 14, "95": 14, "0005": [14, 20], "value_lr": 14, "num_episod": 14, "ab": 14, "1502": 14, "05477": 14, "g_t": 16, "cumul": 16, "manner": 16, "less": [16, 17, 19], "desir": 16, "computation": [16, 20], "td": [16, 17], "meaning": 16, "progress": 16, "bootstrap": [16, 17], "immedi": 16, "timestep": [16, 17], "arbitrari": [16, 17], "termin": [16, 17], "observ": [16, 17], "extend": 16, "48": 16, "arriv": 16, "bottom": 16, "corner": 16, "reciev": 16, "penalti": 16, "katex": 16, "estimate_value_funct": 16, "get_act": [16, 17], "new_stat": [16, 17], "info": [16, 17], "47": [16, 17], "cliffwalk": [16, 17], "randompolici": 16, "seem": [16, 20], "box": 16, "compar": [16, 19, 20], "fall": [16, 17], "cover": 17, "lead": [17, 19], "cliff": 17, "walk": 17, "patch": 17, "polygon": 17, "step_count": 17, "learnedpolici": 17, "action_valu": 17, "rand": [17, 19, 20, 21], "3000": 17, "path": 17, "safer": 17, "offlin": 17, "version": 17, "1989": 17, "watkin": 17, "q_learn": 17, "conserv": 17, "complet": 17, "riski": 17, "thorough": 17, "thereaft": 19, "discret": 19, "tabl": 19, "deal": 19, "race": 19, "car": 19, "robot": 19, "arm": 19, "infinit": 19, "unbound": 19, "bin": 19, "But": 19, "explod": 19, "accuraci": 19, "overcom": 19, "techniqu": [19, 20], "arang": [19, 20, 21], "randint": [19, 20, 21], "qlearningagentwithoutbuff": 19, "agent_without_buff": 19, "supervis": 19, "independ": 19, "ident": 19, "doe": [19, 20], "correl": 19, "especi": 19, "ineffici": [19, 20], "rl": 19, "discard": 19, "reus": 19, "multipl": 19, "fulli": 19, "exploit": 19, "mnih": 19, "concept": 19, "consecut": 19, "incorpor": 19, "tri": 19, "fill": 19, "__len__": 19, "n_sampl": 19, "qlearningagentwithbuff": 19, "buffer_max_len": [19, 20], "train_loss": [19, 20], "state_batch": [19, 21], "action_batch": [19, 21], "reward_batch": [19, 21], "next_state_batch": [19, 21], "done_batch": [19, 21], "agent_with_buff": 19, "ne": 19, "b8": 19, "b32": 19, "evid": 19, "significantli": 19, "quickli": 19, "curv": 19, "acceler": 19, "1312": 19, "5602": 19, "overestim": 20, "understand": 20, "consequ": 20, "accumul": 20, "advers": 20, "affect": 20, "hasselt": 20, "q_a": 20, "q_b": 20, "longrightarrow": 20, "counteract": 20, "disentangl": 20, "proce": 20, "update_a": 20, "update_b": 20, "hassault": 20, "scheme": 20, "polyak": 20, "lunarland": 20, "doubleqlearningag": 20, "train_reward": 20, "via": 20, "successfulli": 20, "outperform": 20, "200": 20, "v2": 20, "64": 20, "2010": 20, "091d584fced301b442654dd8c23b3fc9": 20, "www": 20, "06461": 20, "wang": 21, "slight": 21, "deep": [21, 22], "represent": 21, "enabl": 21, "valuabl": 21, "littl": 21, "impact": 21, "unnecessari": 21, "architur": 21, "lack": 21, "plug": 21, "benefit": 21, "duelqlearningag": 21, "ql": 21, "5e": 21, "evaluate_n_episod": 21, "1511": 21, "06581": 21, "my": 22, "primarili": 22, "barto": 22, "cs285": 22, "cours": 22, "organ": 22, "three": 22, "main": 22, "m": 22}, "objects": {}, "objtypes": {}, "objnames": {}, "titleterms": {"markov": 0, "decis": 0, "process": 0, "valu": [0, 3, 4, 6, 7, 8], "function": [0, 7, 8], "action": [0, 6, 8], "polici": [1, 3, 5, 7, 8, 10, 11, 12, 13, 14, 16, 20], "iter": [1, 3, 4, 13, 14], "evalu": [1, 16], "e": 1, "improv": [1, 14], "i": 1, "lemma": 1, "implement": [1, 3, 6, 7, 8, 9, 10, 12, 13, 14, 16, 20, 21], "reinforc": [2, 10, 22], "learn": [2, 3, 17, 19, 20, 21, 22], "optim": [3, 12, 14], "converg": 4, "analysi": 4, "mont": [5, 6, 7], "carlo": [5, 6, 7], "control": [5, 17], "explor": [5, 8], "start": 5, "stochast": 5, "method": [6, 7, 8, 10, 16], "first": 6, "visit": 6, "v": 6, "": 6, "everi": 6, "blackjack": 6, "mc": 6, "estim": [6, 7, 9], "actor": [7, 13], "critic": [7, 13], "tempor": [7, 16, 17], "differ": [7, 16, 17], "algorithm": [7, 14], "approxim": 7, "td": 7, "determinist": 8, "gradient": [8, 10, 11, 14], "deep": [8, 19, 20], "updat": 8, "ddpg": 8, "gener": 9, "advantag": 9, "theorem": [10, 11], "proxim": 12, "ppo": 12, "soft": 13, "maximum": 13, "entropi": 13, "object": [13, 14], "trust": 14, "region": 14, "bound": 14, "natur": 14, "1": 14, "averag": 14, "kl": 14, "diverg": 14, "2": 14, "linear": 14, "constraint": 14, "truncat": 14, "trpo": 14, "backtrack": 14, "line": 14, "search": 14, "varianc": 15, "reduct": 15, "reward": 15, "go": 15, "baselin": 15, "subtract": 15, "cliff": 16, "walk": 16, "sarsa": 17, "q": [17, 19, 20, 21], "vanilla": 19, "experi": 19, "replai": 19, "doubl": 20, "lunar": 20, "lander": 20, "duel": 21, "identifi": 21, "dqn": 21, "note": 22}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinxcontrib.bibtex": 9, "sphinx": 60}, "alltitles": {"Markov decision process": [[0, "markov-decision-process"]], "Value function and action value function": [[0, "value-function-and-action-value-function"]], "Policy iteration": [[1, "policy-iteration"], [1, "id1"]], "Policy evaluation (E)": [[1, "policy-evaluation-e"]], " (Policy-evaluation)": [[1, "simple-algorithm"]], "Policy improvement (I)": [[1, "policy-improvement-i"]], " (Policy improvement lemma)": [[1, "my-theorem"]], " (Policy iteration)": [[1, "simple-algorithm"]], "Policy iteration Implementation": [[1, "policy-iteration-implementation"]], "Reinforcement Learning": [[2, "reinforcement-learning"]], "Value iteration": [[3, "value-iteration"]], " (Value iteration)": [[3, "simple-algorithm"]], "Value Iteration Implementation": [[3, "value-iteration-implementation"]], "Learning the optimal policy": [[3, "learning-the-optimal-policy"]], "Convergence analysis": [[4, "convergence-analysis"]], "": [[4, "my-theorem"]], " (Value iteration convergence)": [[4, "my-theorem"]], "Monte Carlo control": [[5, "monte-carlo-control"]], "Monte Carlo with exploring starts": [[5, "monte-carlo-with-exploring-starts"]], "Monte Carlo with stochastic policies": [[5, "monte-carlo-with-stochastic-policies"]], "Monte Carlo methods": [[6, "monte-carlo-methods"]], "First-visit v.s. every visit Monte Carlo": [[6, "first-visit-v-s-every-visit-monte-carlo"]], "BlackJack": [[6, "blackjack"]], "MC Value Estimation Implementation": [[6, "mc-value-estimation-implementation"]], "MC Action Value Estimation Implementation": [[6, "mc-action-value-estimation-implementation"]], "Actor critic methods": [[7, "actor-critic-methods"]], "Monte Carlo value estimation": [[7, "monte-carlo-value-estimation"]], "Temporal difference value estimation": [[7, "temporal-difference-value-estimation"]], "Actor critic algorithm": [[7, "actor-critic-algorithm"]], " (Actor critic algorithm)": [[7, "my-algorithm"]], "Actor critic approximation": [[7, "actor-critic-approximation"]], "Policy and value function implementation": [[7, "policy-and-value-function-implementation"]], "Actor critic with Monte Carlo implementation": [[7, "actor-critic-with-monte-carlo-implementation"]], "Actor critic with TD implementation": [[7, "actor-critic-with-td-implementation"]], "Deterministic policy gradient method": [[8, "deterministic-policy-gradient-method"]], "Deep deterministic policy gradient": [[8, "deep-deterministic-policy-gradient"]], " (Deep deterministic policy gradient)": [[8, "my-algorithm"]], "Action value function update": [[8, "action-value-function-update"]], "Exploration": [[8, "exploration"]], "DDPG Implementation": [[8, "ddpg-implementation"]], "Generalized advantage estimation": [[9, "generalized-advantage-estimation"]], "Generalized advantage estimator implementation": [[9, "generalized-advantage-estimator-implementation"]], "Policy gradient method": [[10, "policy-gradient-method"]], "Policy gradient theorem": [[10, "policy-gradient-theorem"], [11, "policy-gradient-theorem"]], " (REINFORCE)": [[10, "my-algorithm"]], "REINFORCE Implementation": [[10, "reinforce-implementation"]], "Proximal policy optimization": [[12, "proximal-policy-optimization"]], "PPO Implementation": [[12, "ppo-implementation"]], "Soft actor critic": [[13, "soft-actor-critic"]], "Maximum entropy objective": [[13, "maximum-entropy-objective"]], " (Soft policy iteration)": [[13, "my-algorithm"]], "Soft actor critic implementation": [[13, "soft-actor-critic-implementation"]], "Trust region policy optimization": [[14, "trust-region-policy-optimization"]], " (Improvement bound)": [[14, "my-theorem"]], " (Policy iteration algorithm)": [[14, "my-theorem"]], "Natural policy gradient": [[14, "natural-policy-gradient"]], "1. Average KL divergence": [[14, "average-kl-divergence"]], "2. Linearize objective and constraint": [[14, "linearize-objective-and-constraint"]], " (Natural policy gradient)": [[14, "natural-policy-gradient"]], "Truncated natural policy gradient": [[14, "truncated-natural-policy-gradient"]], " (Truncated natural policy gradient)": [[14, "truncated-natural-policy-gradient"]], "Trust region policy optimization (TRPO)": [[14, "trust-region-policy-optimization-trpo"]], " (Backtracking line search)": [[14, "line-search"]], " (Trust region policy optimization)": [[14, "trpo"]], "TRPO Implementation": [[14, "trpo-implementation"]], "Variance reduction": [[15, "variance-reduction"]], "Reward to go": [[15, "reward-to-go"]], "Baseline subtraction": [[15, "baseline-subtraction"]], "Temporal difference methods": [[16, "temporal-difference-methods"]], " (Temporal difference policy evaluation)": [[16, "my-algorithm"]], "Cliff walking": [[16, "cliff-walking"]], "Temporal Difference Policy Evaluation Implementation": [[16, "temporal-difference-policy-evaluation-implementation"]], "Temporal difference control": [[17, "temporal-difference-control"]], "SARSA": [[17, "sarsa"]], " (SARSA)": [[17, "my-algorithm"]], "Q-Learning": [[17, "q-learning"]], " (Q-learning)": [[17, "my-algorithm"]], "Deep Q Learning": [[19, "deep-q-learning"]], "Vanilla deep Q Learning": [[19, "vanilla-deep-q-learning"]], "Deep Q Learning with experience replay": [[19, "deep-q-learning-with-experience-replay"]], "Double Q Learning": [[20, "double-q-learning"]], " (Double Q Learning)": [[20, "my-algorithm"]], " (Deep double Q Learning)": [[20, "my-algorithm"]], "Double Q Learning Implementation": [[20, "double-q-learning-implementation"]], "Policy": [[20, "policy"]], "Lunar Lander": [[20, "lunar-lander"]], "Duel Q Learning": [[21, "duel-q-learning"]], "Identifiability": [[21, "identifiability"]], "Duel DQN implementation": [[21, "duel-dqn-implementation"]], "Notes on Reinforcement Learning": [[22, "notes-on-reinforcement-learning"]]}, "indexentries": {}})