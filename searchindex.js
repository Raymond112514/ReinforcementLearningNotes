Search.setIndex({"docnames": ["MDP/Markov-decision-process", "MDP/Policy-iteration", "MDP/Value-iteration", "MDP/Value-iteration-convergence", "MonteCarlo/Monte-Carlo-Control", "MonteCarlo/Monte-Carlo-value-estimation", "PolicyGradient/Actor-critic-methods", "PolicyGradient/Deterministic-policy-gradient-method", "PolicyGradient/Generalized-advantage-estimation", "PolicyGradient/Policy-gradient-method", "PolicyGradient/Policy-gradient-theorem", "PolicyGradient/Proximal-policy-gradient", "PolicyGradient/Soft-actor-critic", "PolicyGradient/TRPO", "PolicyGradient/Variance-reduction", "TD/Temporal-difference", "TD/Temporal-difference-control", "TD/Untitled", "ValueBasedMethods/Deep-Q-Learning", "ValueBasedMethods/Double-Q-Learning", "ValueBasedMethods/Dueling-Q-Learning", "intro", "markdown", "markdown-notebooks", "notebooks"], "filenames": ["MDP/Markov-decision-process.ipynb", "MDP/Policy-iteration.ipynb", "MDP/Value-iteration.ipynb", "MDP/Value-iteration-convergence.ipynb", "MonteCarlo/Monte-Carlo-Control.ipynb", "MonteCarlo/Monte-Carlo-value-estimation.ipynb", "PolicyGradient/Actor-critic-methods.ipynb", "PolicyGradient/Deterministic-policy-gradient-method.ipynb", "PolicyGradient/Generalized-advantage-estimation.ipynb", "PolicyGradient/Policy-gradient-method.ipynb", "PolicyGradient/Policy-gradient-theorem.ipynb", "PolicyGradient/Proximal-policy-gradient.ipynb", "PolicyGradient/Soft-actor-critic.ipynb", "PolicyGradient/TRPO.ipynb", "PolicyGradient/Variance-reduction.ipynb", "TD/Temporal-difference.ipynb", "TD/Temporal-difference-control.ipynb", "TD/Untitled.ipynb", "ValueBasedMethods/Deep-Q-Learning.ipynb", "ValueBasedMethods/Double-Q-Learning.ipynb", "ValueBasedMethods/Dueling-Q-Learning.ipynb", "intro.md", "markdown.md", "markdown-notebooks.md", "notebooks.ipynb"], "titles": ["<span class=\"section-number\">1. </span>Markov decision process", "<span class=\"section-number\">3. </span>Policy iteration", "<span class=\"section-number\">2. </span>Value iteration", "<span class=\"section-number\">2.4. </span>Theoretic analysis", "<span class=\"section-number\">4.5. </span>Monte Carlo control", "<span class=\"section-number\">4. </span>Value estimation", "<span class=\"section-number\">10. </span>Actor critic methods", "<span class=\"section-number\">14. </span>Deterministic policy gradient method", "<span class=\"section-number\">11. </span>Generalized advantage estimation", "<span class=\"section-number\">9. </span>Policy gradient method", "Policy gradient theorem", "<span class=\"section-number\">13. </span>Proximal policy gradient", "<span class=\"section-number\">15. </span>Soft actor critic", "<span class=\"section-number\">12. </span>Trust region policy optimization", "Variance reduction", "<span class=\"section-number\">5. </span>Temporal difference", "<span class=\"section-number\">5.3. </span>Temporal difference control", "&lt;no title&gt;", "<span class=\"section-number\">6. </span>Deep Q Learning", "<span class=\"section-number\">7. </span>Double Q Learning", "<span class=\"section-number\">8. </span>Dueling Q Learning", "Welcome to your Jupyter Book", "Markdown Files", "Notebooks with MyST Markdown", "Content with notebooks"], "terms": {"some": [0, 8, 11, 13, 19, 21, 22, 24], "text": [0, 1, 2, 6, 9, 11, 12, 13, 16, 19, 22, 23], "about": [0, 9, 22, 23, 24], "mdp": [0, 1, 2, 10, 12], "anoth": [1, 6, 11, 13, 19, 20], "wai": [1, 5, 7, 9, 11, 16, 20], "solv": [1, 2, 3, 11, 13, 18, 20], "problem": [1, 5, 7, 9, 11, 13, 14, 20], "instead": [1, 8, 9, 11, 13, 15, 16, 18, 20], "learn": [1, 6, 7, 8, 9, 11, 12, 13, 21], "optim": [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 16, 18, 19, 20, 21], "state": [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 24], "valu": [1, 3, 4, 8, 9, 11, 12, 13, 16, 18, 19, 20], "function": [1, 2, 3, 4, 8, 9, 12, 13, 16, 18, 19, 20, 22], "start": [1, 5, 8, 13, 22, 23], "random": [1, 2, 4, 7, 9, 12, 15, 16, 18, 24], "gradual": [1, 13], "imporv": 1, "two": [1, 3, 5, 6, 7, 8, 12, 13, 19, 22, 23], "step": [1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "which": [1, 6, 7, 8, 9, 11, 12, 13, 15, 18, 19, 20, 23], "v_": [1, 2, 3, 5, 6, 12, 15], "pi": [1, 2, 5, 10, 12, 13, 15, 16], "": [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 24], "updat": [1, 2, 3, 6, 9, 11, 12, 13, 15, 16, 18, 19], "increas": [1, 7, 9], "In": [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22], "we": [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20], "By": [1, 2, 7, 8], "bellman": [1, 2, 3, 12], "equat": [1, 2, 3, 7, 8], "have": [1, 2, 3, 6, 7, 8, 13, 15, 18, 19, 23], "sum_": [1, 2, 3, 5, 6, 7, 8, 9, 10, 12, 13, 20], "p": [1, 2, 3, 5, 7, 9, 10, 13], "r_0": [1, 2, 3, 9], "gamma": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "can": [1, 2, 3, 5, 6, 7, 8, 9, 13, 14, 15, 16, 18, 19, 20, 22, 23, 24], "us": [1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 14, 15, 16, 18, 19, 20, 22, 23], "an": [1, 2, 6, 8, 9, 12, 13, 15, 16, 18, 19, 22], "fashion": [1, 9], "like": [1, 2, 5, 6, 7, 13, 18, 19, 20, 22, 23], "The": [1, 2, 4, 5, 6, 7, 8, 9, 12, 13, 15, 16, 18, 19, 20, 22, 23], "psuedocod": [1, 6, 7, 13, 15, 16, 19], "prsent": 1, "below": [1, 2, 6, 7, 8, 9, 13, 15, 16, 18, 19, 20], "onli": [1, 7, 15, 18], "differ": [1, 5, 8, 13, 21, 22], "rule": [1, 7, 9, 13, 16, 18], "input": [1, 2, 6, 7, 9, 12, 13, 15, 16, 19, 22], "given": [1, 2, 3, 6, 10, 12, 13, 16, 20], "instanc": [1, 2, 4, 5, 10], "output": [1, 2, 7, 15, 16, 23], "comput": [1, 2, 5, 6, 7, 8, 9, 13, 15, 18], "initi": [1, 2, 7, 9, 13, 15, 16], "arrai": [1, 2, 7, 24], "v": [1, 2, 3, 6, 8, 9, 10, 12, 15, 20], "0": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 24], "all": [1, 2, 8, 11, 15, 16, 19, 22, 23], "mathcal": [1, 2, 3, 7, 10, 11, 12, 13, 15, 16, 18, 19, 20], "while": [1, 2, 6, 7, 8, 9, 11, 12, 15, 16, 18, 19, 20], "converg": [1, 2, 3, 13, 15, 18], "leftarrow": [1, 2, 3, 6, 7, 9, 12, 13, 15, 16, 19], "A": [1, 2, 4, 5, 6, 7, 8, 10, 13, 16, 18, 19, 20], "current": [1, 13, 16], "consid": [1, 9, 11, 13, 20], "action": [1, 2, 4, 5, 6, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "fucntion": [1, 2, 4, 5, 20], "q_": [1, 2, 5, 7, 10, 12, 18], "intuit": [1, 7, 11, 19, 20], "exist": [1, 3], "geq": [1, 6, 8, 9, 10, 12, 13], "Then": [1, 13], "should": [1, 8, 13, 19, 23], "get": [1, 2, 4, 6, 7, 8, 9, 11, 12, 13, 16, 18, 19, 20, 22, 23], "better": [1, 6, 13], "payoff": 1, "choos": [1, 6, 8, 16, 19], "thi": [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 21, 22, 23, 24], "motiv": [1, 9, 13], "u": [1, 3, 5, 6, 7, 9, 13, 16], "our": [1, 2, 6, 7, 13, 15, 16, 18, 19], "greedi": [1, 16], "begin": [1, 2, 3, 6, 7, 8, 9, 10, 12, 13, 24], "align": [1, 2, 3, 6, 7, 8, 9, 10, 12, 13, 24], "hat": [1, 16], "underset": [1, 2, 9, 12, 13, 16, 19], "argmax": [1, 2, 9, 13, 16, 18, 19, 20], "r": [1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 18, 19], "end": [1, 2, 3, 6, 7, 8, 9, 10, 12, 13, 15, 24], "now": [1, 2, 3, 6, 9, 10, 12, 13, 15, 18, 19], "prove": [1, 3, 13], "new": [1, 12, 13], "good": [1, 6], "than": [1, 6, 13], "let": [1, 3, 7, 8, 9, 10, 11, 13, 23], "ani": [1, 3, 4, 5, 10, 21, 23], "pair": [1, 19], "determinist": [1, 21], "hspace": [1, 6, 11, 13], "5mm": [1, 11, 13], "proof": [1, 3, 10, 13], "omit": 1, "therefor": [1, 3, 6, 13, 19], "sequenc": [1, 13], "from": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 24], "pi_0": [1, 13], "xrightarrow": 1, "pi_1": 1, "sinc": [1, 2, 3, 5, 8, 13, 15, 16, 20], "accord": [1, 3], "leq": [1, 3, 11, 13], "pi_n": 1, "follow": [1, 2, 3, 6, 7, 9, 11, 12, 13, 16, 18, 19, 20, 22, 23], "algorithm": [1, 2, 3, 7, 8, 9, 11, 12, 15, 16, 18, 19, 20], "eventu": 1, "displai": [1, 13, 23], "import": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 24], "numpi": [1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 24], "np": [1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 24], "matplotlib": [1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 24], "pyplot": [1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 24], "plt": [1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 24], "gridworld": 1, "seed": [1, 2, 24], "def": [1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "policy_evalu": 1, "grid_world": [1, 2], "max_it": [1, 2, 4, 5], "1": [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 15, 16, 18, 19, 20, 24], "height": [1, 2], "width": [1, 2], "zero": [1, 2, 13, 19], "rang": [1, 2, 4, 5, 6, 8, 9, 11, 13, 15, 16, 18, 19, 20, 24], "get_stat": [1, 2], "_": [1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20], "successor": [1, 2], "get_available_act": [1, 2], "prob_dist": [1, 2], "get_transition_prob": [1, 2], "prob": [1, 2, 6, 9, 11, 13], "item": [1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "reward": [1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "get_reward": [1, 2], "return": [1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "policy_improv": 1, "dict": [1, 2, 5], "available_act": [1, 2], "q_valu": [1, 2, 4, 5, 7, 16, 18, 19, 20], "max": [1, 2, 4, 11, 13, 16, 18, 20], "kei": [1, 2, 4, 5, 9, 16], "len": [1, 2, 4, 5, 7, 11, 12, 18, 19, 20], "els": [1, 2, 5, 6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "none": [1, 2, 6, 7], "get_random_polici": 1, "random_idx": 1, "choic": [1, 4, 6, 7, 13, 15, 16], "arang": [1, 18, 19, 20], "policy_iter": 1, "histori": [1, 6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "append": [1, 11, 13, 18, 19], "5": [1, 2, 6, 7, 8, 11, 12, 13, 18, 19, 24], "exit": [1, 2, 11], "good_exit": [1, 2], "3": [1, 2, 6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "bad_exit": [1, 2], "2": [1, 2, 5, 6, 7, 8, 9, 11, 12, 15, 18, 23], "wall": [1, 2], "living_reward": [1, 2], "win_reward": [1, 2], "lose_reward": [1, 2], "init_po": [1, 2], "10": [1, 4, 7, 9, 11, 13, 15, 16, 18, 19, 24], "display_polici": [1, 2, 16], "object": [2, 6, 9, 10, 11, 20], "i": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 23, 24], "find": [2, 5, 6, 8, 9, 12, 13, 19], "where": [2, 5, 6, 7, 8, 9, 11, 13, 15, 18, 20], "defin": [2, 3, 6, 7, 19, 20, 23], "mathbb": [2, 3, 5, 6, 7, 9, 10, 11, 12, 13], "e": [2, 5, 6, 7, 9, 10, 11, 12, 13, 16], "a_0": 2, "s_1": 2, "a_1": 2, "s_0": [2, 5], "infti": [2, 3, 8], "r_": [2, 8], "note": [2, 3, 6, 8, 9, 11, 13, 15, 16, 22], "simplifi": [2, 8, 9, 13], "bigg": [2, 3, 6, 7, 9, 10, 11, 12, 13], "s_2": 2, "a_2": 2, "third": 2, "line": [2, 22, 23, 24], "law": 2, "total": [2, 13], "expect": [2, 5, 7, 8, 9, 12, 13, 15, 16, 18], "final": [2, 6, 13], "markov": [2, 12], "properti": [2, 18], "assum": [2, 9], "size": [2, 6, 7, 9, 13, 14, 18, 19, 20], "space": [2, 13, 18], "finit": [2, 18], "tag": 2, "equal": 2, "known": [2, 7, 9, 12, 16, 18], "its": [2, 6, 7, 9, 13, 19], "correspond": [2, 6, 13], "linear": [2, 12, 19, 20], "system": [2, 13], "deriv": [2, 7, 9, 10], "similar": [2, 16, 22], "result": [2, 6, 7, 9, 12, 13, 18], "same": [2, 7, 16, 18, 22], "argument": 2, "s_": 2, "a_": [2, 8, 13], "hold": [2, 18], "also": [2, 5, 8, 9, 13, 18, 22, 23, 24], "recal": [2, 5, 6, 20], "max_a": [2, 3], "q": [2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 15, 21], "substitut": [2, 8, 9], "see": [2, 8, 9, 13, 16, 21, 22, 23, 24], "must": [2, 6, 22], "satisfi": [2, 8], "condit": [2, 9], "case": [2, 18, 19, 20], "howev": [2, 6, 7, 8, 9, 11, 13, 18, 19], "practic": [2, 6, 13], "solut": [2, 13, 16, 18], "max_": [2, 3, 11, 13, 16, 18, 19, 20], "code": [2, 22, 23], "cell": 2, "value_iter": 2, "perform": [2, 6, 7, 8, 9, 12, 13, 15, 19, 20], "param": [2, 4, 5, 6, 7, 8, 9, 12, 13, 18, 19, 20], "grid": 2, "world": [2, 15], "int": [2, 4, 5, 9, 18], "maximum": [2, 19], "number": [2, 4, 5, 6, 7, 8, 9, 12, 18], "allow": [2, 6, 9, 22], "execut": [2, 23], "float": [2, 6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "option": 2, "default": [2, 4, 5, 6, 23], "discount": [2, 4, 5, 13, 15, 16], "factor": [2, 4, 5, 8, 16], "futur": [2, 15, 16], "ndarrai": 2, "2d": 2, "x": [2, 5, 7, 13], "y": [2, 6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "entri": [2, 18], "repres": [2, 6, 13, 15, 18], "max_valu": 2, "inf": 2, "instanti": [2, 7], "appli": [2, 5, 7, 8, 9], "100": [2, 8, 9, 11, 15, 16, 18, 24], "display_valu": [2, 15], "know": [2, 5, 20], "how": [2, 6, 13, 16, 21, 23], "goal": [2, 12, 18], "attain": 2, "To": [2, 3, 6, 7, 9, 11, 13, 16, 18, 19], "do": [2, 7, 13, 20, 22, 24], "so": [2, 6, 23], "determin": [2, 5, 16], "onc": [2, 13, 18], "first": [2, 3, 4, 6, 7, 8, 13], "extract": 2, "extract_q_valu": 2, "dictionari": [2, 9], "ar": [2, 5, 6, 9, 12, 13, 15, 16, 18, 19, 20, 22, 23], "tupl": 2, "coordin": 2, "each": [2, 9, 13, 15, 16, 18, 20], "inner": 2, "map": [2, 3, 7], "extract_polici": 2, "opt_act": [2, 19], "display_qvalu": 2, "And": [2, 3, 11, 12, 15], "previou": [3, 6, 8, 10, 16], "section": [3, 8, 9, 10, 15, 16], "k": [3, 6, 7, 8, 9, 13], "v_k": 3, "It": [3, 9, 13, 16, 18, 21, 22], "show": [3, 7, 11, 21, 22, 23], "inde": 3, "introduc": [3, 7, 8, 18, 20], "oper": [3, 9, 12, 19, 20], "t": [3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 16, 18, 22, 24], "rightarrow": [3, 7], "For": [3, 7, 9, 10, 13, 15, 16, 18, 22, 24], "vector": [3, 13], "notat": [3, 8, 13], "compactli": 3, "rewrit": [3, 10], "iter": [3, 5, 21], "contract": 3, "under": 3, "sup": 3, "norm": 3, "other": [3, 6, 7, 8, 15, 16, 23], "word": [3, 6, 15], "check": [3, 21, 24], "directli": [3, 5, 6, 7, 9, 13, 18], "after": [3, 8, 15, 19], "remain": [3, 13], "standard": [3, 12], "fix": [3, 24], "point": [3, 18], "theorem": [3, 7, 13], "lim_": 3, "cdot": [3, 11, 12, 13, 15, 16], "v_0": 3, "impli": [3, 9, 13, 20], "pip": [4, 5, 6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "instal": [4, 5, 6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "gym": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "itertool": [4, 5], "montecarlo": [4, 5], "copi": [4, 13, 19], "seaborn": [4, 5], "sn": [4, 5], "env": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "make": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20, 24], "blackjack": 4, "v1": [4, 5, 6, 8, 9, 11, 13, 18, 19, 20], "natur": [4, 5], "fals": [4, 5, 7, 8, 9, 11, 12, 15, 16, 18, 19, 20], "sab": [4, 5], "player_sum_rang": [4, 5], "32": [4, 5, 7, 9, 12, 18, 19], "dealer_card_rang": [4, 5], "11": [4, 5], "usable_ace_rang": [4, 5], "true": [4, 5, 8, 11, 12, 13, 15, 16, 19, 20], "list": [4, 5, 15, 16], "product": [4, 5], "state_action_dict": 4, "20": [4, 5, 11], "class": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "stick_at_20_or_21": [4, 5], "self": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "epsilon": [4, 11, 13, 16, 18, 19, 20], "montecarlocontrol": 4, "estim": [4, 6, 7, 9, 12, 13, 14, 16, 19, 21], "__init__": [4, 5, 6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "avail": [4, 5, 15], "sampl": [4, 5, 6, 7, 9, 11, 12, 13, 15, 16, 18, 19, 20, 21, 24], "implement": [4, 5, 15, 16, 18], "every_visit": [4, 5], "enviro": [4, 5, 7, 9, 13, 15, 16, 19, 20], "agent": [4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 18, 19, 20], "interact": [4, 5, 15, 24], "n_episod": [4, 5, 9, 11, 15, 16, 18, 19, 20], "episod": [4, 5, 6, 7, 8, 9, 11, 12, 15, 16, 18, 19], "boolean": [4, 5], "indic": [4, 5], "whether": [4, 5, 6, 13, 22], "everi": 4, "visit": [4, 13], "form": [4, 5], "state1": [4, 5], "value1": [4, 5], "count": [4, 5], "cum_reward": [4, 5, 16], "state_act": [4, 5], "enumer": [4, 5, 11], "mc": [4, 5, 6], "optimal_polici": 4, "5000000": 4, "plotblackjackoptimalstrategi": 4, "stick_at_20_or_21_stochast": 4, "rand": [4, 16, 18, 19, 20], "plot_q_values_heatmap": [4, 5], "Not": 5, "being": [5, 18, 22], "abl": 5, "access": 5, "transit": [5, 7, 18], "probabl": [5, 6], "pose": 5, "challeng": [5, 6, 13, 18, 19], "polici": [5, 8, 16, 18, 20], "longer": [5, 16], "dynam": 5, "program": [5, 13], "method": [5, 8, 13, 15, 16, 18, 20], "both": [5, 6, 7, 8, 13, 20, 22], "requir": [5, 9, 16], "suffici": 5, "depend": [5, 22], "One": [5, 7, 9, 11, 13, 14, 15, 16, 18, 19, 20], "come": 5, "around": [5, 7, 15, 16], "through": [5, 8, 9], "essenti": [5, 9], "approx": [5, 6, 8, 9, 13], "frac": [5, 6, 7, 9, 11, 12, 13, 20], "n": [5, 6, 7, 9, 12, 13, 15, 16, 18, 19, 20, 22, 24], "x_i": [5, 13], "distribut": [5, 6, 7, 9, 11, 12, 13, 18], "suggest": [5, 6, 8, 13, 20], "trajectori": [5, 6, 8, 9, 12, 13, 15, 18], "culmul": 5, "print": [5, 11, 12, 18, 19, 23], "f": [5, 11, 12, 13, 18, 19], "640": 5, "montecarlovalu": 5, "simpl": [5, 18, 22], "stick": 5, "21": 5, "hit": 5, "500000": 5, "plot_valu": 5, "montecarloqvalu": 5, "action1": 5, "q11": 5, "action2": 5, "q12": 5, "state2": 5, "q21": 5, "q22": 5, "gradient": [6, 12, 14, 21], "go": [6, 21], "j": [6, 7, 9, 10, 12], "theta": [6, 7, 9, 10, 11, 12, 13], "tau": [6, 7, 9, 10, 12, 13, 19], "sim": [6, 7, 9, 10, 11, 12, 13, 15], "p_": [6, 9, 10, 12, 13], "sum_t": 6, "nabla_": [6, 7, 9, 10, 13], "log": [6, 9, 12, 13], "pi_": [6, 7, 9, 10, 11, 12, 13], "mathbf": [6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20], "_t": [6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18], "serv": [6, 8, 22], "unbias": [6, 8], "further": [6, 9], "reduc": [6, 8, 13], "varianc": [6, 8, 9, 18], "baselin": 6, "subtract": 6, "give": [6, 7, 9, 13, 16, 21], "b": [6, 12, 19], "even": [6, 13, 19], "though": [6, 13], "formula": [6, 8], "hard": [6, 11], "heurist": 6, "typic": [6, 8, 18], "written": [6, 22, 23], "likelihood": 6, "weight": [6, 7, 8], "call": [6, 7, 8, 16, 22], "term": [6, 8, 13], "advantag": [6, 11, 13, 20, 21], "ha": [6, 8, 13, 15, 19], "natrual": 6, "interpret": 6, "wors": [6, 15], "If": [6, 13, 19, 23], "take": [6, 7, 15, 16, 18, 20], "postiv": 6, "would": 6, "rais": [6, 12], "convers": 6, "neg": 6, "lower": [6, 8, 13], "question": 6, "parameter": [6, 9, 18], "phi": [6, 7, 12, 18], "neural": [6, 7, 9, 13, 18, 19], "network": [6, 7, 9, 12, 13, 18, 19, 20], "3mm": 6, "loss": [6, 8, 9, 11, 18, 19, 20], "still": [6, 7, 8, 12, 19], "need": [6, 7, 9, 11, 13, 16, 19, 23], "achiev": [6, 7, 8], "specifi": [6, 9], "target": [6, 7, 8, 11, 12, 13, 18, 19, 20], "done": [6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "primari": 6, "approach": [6, 7, 11, 15, 18], "fit": 6, "data": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20, 24], "mean": [6, 7, 8, 9, 11, 12, 13, 15, 18, 20, 24], "squar": [6, 18], "error": [6, 13, 18, 19], "l": [6, 7, 11, 13, 18], "wait": 6, "until": [6, 13], "entir": 6, "been": [6, 13, 19], "befor": [6, 9, 10, 18], "calcul": [6, 20], "altern": 6, "increment": 6, "base": [6, 9, 13, 19, 23], "full": 6, "present": [6, 7, 9, 10, 13], "differenti": [6, 7, 9, 12], "rate": [6, 7, 9, 12, 16], "alpha": [6, 7, 9, 12, 13, 15, 16, 19], "batch": [6, 7, 9, 12, 14, 18], "gener": [6, 9, 12, 13, 15, 16, 21], "either": 6, "4mm": 6, "type": [6, 21], "test": [6, 7, 8, 9, 13, 16, 19, 20], "cartpol": [6, 9, 11, 13, 18, 19, 20], "environ": [6, 9, 15, 16, 18, 20], "one": [6, 7, 8, 9, 13, 18, 19, 22], "layer": [6, 7, 9, 13], "hidden": [6, 7, 9, 13], "unit": [6, 7, 13], "128": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "pybullet": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "stabl": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "baselines3": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "extra": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "pyvirtualdisplai": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "apt": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "xvfb": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "pybullet_env": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "imageio": [6, 7, 8, 9, 11, 12, 13, 15, 16, 18, 19, 20], "torch": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "nn": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "util": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "dataset": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "dataload": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "devic": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "cuda": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "is_avail": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "cpu": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "modul": [6, 7, 9, 11, 12, 13, 18, 19, 20], "state_dim": [6, 7, 9, 11, 12, 13, 18, 19, 20], "action_dim": [6, 7, 9, 11, 12, 13, 18, 19, 20], "hidden_dim": [6, 7, 9, 11, 12, 13, 18, 19, 20], "super": [6, 7, 9, 11, 12, 13, 18, 19, 20], "net": [6, 7, 9, 11, 12, 13], "simpleneuralnetwork": [6, 7, 9, 11, 13, 18], "in_dim": [6, 7, 9, 11, 13, 18], "out_dim": [6, 7, 9, 11, 12, 13, 18], "final_lay": [6, 9, 11, 13], "softmax": [6, 9, 11, 13], "forward": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "select_act": [6, 7, 9, 11, 12, 13, 18, 19, 20], "tensor": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "action_dist": [6, 9, 11, 12, 13], "categor": [6, 7, 9, 11, 13], "log_prob": [6, 8, 9, 11, 12], "valuenetwork": [6, 8, 12], "squeez": [6, 7, 12, 19], "actorcriticmcag": 6, "lr": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "1e": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "4": [6, 8, 9, 11, 12, 13, 15, 16, 20, 24], "policy_optim": [6, 7, 8, 11, 12], "adam": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "paramet": [6, 7, 8, 9, 11, 12, 13, 15, 18, 19, 20], "value_optim": [6, 8, 11, 12, 13], "policy_schedul": [6, 7, 8, 12], "lr_schedul": [6, 7, 8, 9, 11, 12, 18], "steplr": [6, 7, 8, 9, 11, 12, 18], "step_siz": [6, 7, 8, 9, 11, 12, 18], "1000": [6, 8, 15, 16, 18, 20], "value_schedul": [6, 8, 12], "learn_episod": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "batch_siz": [6, 7, 8, 9, 11, 12, 18, 19, 20], "max_step": [6, 7, 8, 9, 11, 12, 13, 16, 18, 19, 20], "value_loss": [6, 8, 11, 12, 13], "policy_loss": [6, 7, 8, 11, 12], "sample_trajectori": [6, 8, 11, 13], "rewards_cum": 6, "cumsum": 6, "flip": 6, "dim": [6, 7, 12, 18, 20], "mseloss": [6, 7, 8, 11, 12, 19, 20], "get_action_log_prob": [6, 11, 13], "detach": [6, 7, 8, 11, 12, 13, 19], "sum": [6, 8, 11, 12, 13], "zero_grad": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "backward": [6, 7, 8, 9, 11, 12, 13, 18, 19, 20], "cartpolebulletenv": [6, 8, 9, 11, 13, 18, 19, 20], "3500": [6, 8], "print_everi": [6, 7, 8, 9, 11, 18, 19, 20], "50": [6, 7, 11, 12, 19, 20], "actorcritictdag": 6, "no_grad": [6, 7, 8, 11, 12, 13, 18, 19, 20], "dpg": 7, "wa": 7, "propos": [7, 8, 9, 11, 13, 16, 19, 20], "silver": 7, "et": [7, 8, 9, 11, 13, 18, 20], "al": [7, 8, 9, 11, 13, 18, 20], "2015": [7, 20], "unlik": [7, 11], "tradit": [7, 20], "stochast": [7, 13], "produc": 7, "gaussian": 7, "focus": 7, "author": [7, 8, 10, 13, 20], "analog": 7, "determinst": 7, "mu": 7, "express": [7, 8, 9, 13], "mu_": 7, "move": [7, 16, 18], "dircet": 7, "most": [7, 22], "chain": 7, "evalu": [7, 12, 18, 19], "mont": [7, 9, 15, 16], "carlo": [7, 9, 15, 16], "subsequ": 7, "paper": [7, 10, 13, 19, 20], "continu": [7, 18], "control": [7, 8, 15], "reinforc": [7, 14, 18], "lillicrap": 7, "replai": 7, "buffer": [7, 12, 18, 19, 20], "process": [7, 9, 12, 18], "select": [7, 19], "experi": [7, 9], "r_t": [7, 8, 11, 13, 15, 16], "store": [7, 18, 22], "minibatch": 7, "_i": [7, 13, 18], "r_i": [7, 18], "y_i": 7, "minim": [7, 18], "nabla": 7, "nabla_a": 7, "soft": [7, 11, 21], "style": 7, "improv": [7, 8, 9, 12], "thei": [7, 22], "train": [7, 9, 11, 12, 16, 18, 19, 20], "exponenti": [7, 8], "averag": [7, 8, 11, 18], "help": [7, 16, 18, 20, 22], "enhanc": [7, 18], "stabil": [7, 8, 18, 20], "set": [7, 13, 15, 16, 18], "005": [7, 12], "dure": [7, 15, 18], "avoid": [7, 19], "unvisit": 7, "address": [7, 9, 11, 13, 18], "issu": [7, 9, 13, 20], "ad": [7, 13], "nois": [7, 9], "chosen": 7, "suit": 7, "ornstein": 7, "uhlenbeck": 7, "dx_t": 7, "x_tdt": 7, "sigma": 7, "dw_t": 7, "invers": [7, 13], "pendulum": 7, "task": [7, 18], "collect": [7, 13, 18, 19], "dequ": [7, 18, 19], "clip_grad_norm_": [7, 11], "n_layer": [7, 11, 13, 18], "shape": [7, 12, 13, 18, 19, 20], "unsqueez": [7, 12, 18, 19, 20], "qvalu": 7, "qvaluenetwork": [7, 12], "cat": [7, 12, 13], "ornsteinuhlenbeckprocess": 7, "15": [7, 11], "dt": 7, "x0": 7, "reset_st": 7, "x_prev": 7, "sqrt": [7, 13], "normal": [7, 8, 11, 12], "zeros_lik": [7, 13], "deterministicpolicygradientag": 7, "observation_spac": [7, 12, 13, 15, 16, 18, 19, 20], "creat": [7, 8, 24], "replica": 7, "target_polici": 7, "target_q_valu": [7, 19], "load_state_dict": [7, 11], "state_dict": [7, 11], "q_value_optim": 7, "9": [7, 9, 12, 13, 18], "q_value_schedul": 7, "ou_process": 7, "replaybuff": [7, 12, 18, 19, 20], "maxlen": [7, 12, 18, 19, 20], "10000": [7, 11, 12, 13, 16, 18, 19, 20], "grad_clip_valu": 7, "reset": [7, 9, 11, 12, 15, 16, 18, 19, 20], "episode_loss": [7, 9, 11, 18, 20], "clip": [7, 11], "next_stat": [7, 9, 11, 12, 13, 18, 19, 20], "episode_reward": [7, 9, 11, 12, 18, 20], "add": [7, 12, 18, 19, 20], "q_value_loss": 7, "soft_upd": 7, "target_model": 7, "model": [7, 9, 20], "target_param": [7, 12, 19], "zip": [7, 8, 11, 12, 18, 19], "copy_": [7, 12, 19], "2000": [7, 11, 12], "eval": 7, "invertedpendulumbulletenv": [7, 12], "v0": [7, 12, 15, 16], "500": [7, 11, 16, 19], "99": [7, 12], "discuss": [8, 9, 13, 15, 16], "actor": [8, 21], "critic": [8, 11, 21], "often": [8, 13, 18, 19], "latter": 8, "becaus": [8, 9, 16, 18], "provid": [8, 10, 13, 15, 16, 19], "grow": 8, "linearli": 8, "wherea": [8, 22], "bia": [8, 19], "tradeoff": 8, "high": [8, 12], "second": [8, 13], "bias": [8, 19], "possibl": [8, 12, 18, 19], "effect": [8, 18, 20], "balanc": 8, "2016": 8, "schulman": [8, 11, 13], "gae": 8, "design": 8, "combin": 8, "abov": [8, 9, 10, 13, 16, 19, 20], "look": [8, 19], "2r": 8, "strength": 8, "offer": 8, "clear": 8, "name": [8, 11, 12], "lambda": 8, "scalar": 8, "delta_t": 8, "denot": [8, 10, 13], "tempor": [8, 18, 21], "residu": 8, "time": [8, 15, 16, 18, 19], "reexpress": 8, "2r_": 8, "delta_": 8, "when": [8, 9, 13, 14, 15, 18, 20, 22, 23], "whera": 8, "recov": [8, 20], "tune": [8, 11], "trade": 8, "expens": 8, "wise": 8, "easi": 8, "recurr": 8, "a_t": [8, 11], "effici": [8, 18], "recurs": 8, "work": [8, 11, 20, 24], "actorcriticgaeag": 8, "lamb": 8, "estimate_advantag": [8, 11], "next_valu": [8, 11], "revers": [8, 11, 13], "delta": [8, 11, 13], "insert": [8, 11, 22], "std": [8, 11, 12], "8": [8, 9, 11, 13, 18, 19], "shown": [8, 11, 13, 15, 16, 18, 20], "graph": 8, "appear": 8, "fastest": 8, "epoch": [8, 16], "exhibit": [8, 18], "signific": 8, "variat": [8, 13, 22], "On": 8, "hand": [8, 16], "although": 8, "slightli": [8, 9], "slower": 8, "demonstr": [8, 18], "much": [8, 13, 15], "more": [8, 9, 12, 13, 16, 18, 20, 21, 23, 24], "long": [8, 13, 19], "run": [8, 15, 23], "lambda_": [8, 11], "00": 8, "contrast": [9, 18], "without": [9, 18, 20], "explicitli": 9, "formal": [9, 10], "want": [9, 12, 24], "maxim": [9, 12, 13, 19, 20], "underbrac": 9, "_0": [9, 10, 12, 15, 16], "_1": [9, 10], "induc": 9, "prod_": 9, "potenti": [9, 13], "specif": [9, 18, 22], "tau_i": [9, 12], "troubl": 9, "cannot": [9, 20], "part": 9, "variabl": 9, "reformul": 9, "mention": [9, 10], "earlier": 9, "unrol": 9, "regular": [9, 22], "interchang": 9, "integr": 9, "d": [9, 11, 13, 18, 23], "next": [9, 15], "trick": 9, "statement": 9, "sutton": 9, "1999": 9, "what": 9, "remark": 9, "approxim": [9, 13, 16, 18], "pseudocod": [9, 13], "dimens": [9, 12], "ob": [9, 11, 12, 13, 18], "action_log_prob": [9, 11, 12], "ll": [9, 22], "reinforceag": 9, "inherit": 9, "includ": [9, 13, 23, 24], "pre": 9, "n_sampl": [9, 18], "along": 9, "addit": [9, 12], "contain": [9, 18], "record": 9, "metric": 9, "schedul": [9, 11, 18], "vari": 9, "them": [9, 20], "300": 9, "decai": 9, "plot": [9, 11, 24], "partli": 9, "fact": [9, 13], "larger": [9, 13, 18], "smaller": [9, 13], "explan": [9, 16], "rather": 10, "inform": [10, 18, 21, 23, 24], "origin": [10, 13, 19], "worth": 10, "With": [10, 12, 23], "mind": 10, "label": [10, 11], "drawback": [11, 15], "trust": [11, 21], "region": [11, 21], "difficult": 11, "2017": [11, 13], "variant": [11, 18], "trpo": 11, "theta_": [11, 13], "old": [11, 12, 13], "subject": [11, 13], "rho": [11, 13], "kl": [11, 12], "diverg": 11, "constraint": [11, 12], "beta": 11, "empir": [11, 20], "doe": [11, 18, 21], "seem": [11, 15], "well": [11, 21, 24], "min": [11, 12], "hyperparamet": [11, 18], "prevent": [11, 12], "far": [11, 15, 16], "ratio": [11, 13], "back": [11, 20], "ppoagent": 11, "ep": 11, "policy_old": 11, "old_log_prob": [11, 12, 13], "new_log_prob": [11, 12, 13], "exp": [11, 12, 13], "surr1": 11, "surr2": 11, "clamp": 11, "train_loss": [11, 18, 19], "load": 11, "user": 11, "raymondtsao": 11, "desktop": 11, "reinforcementlearningnot": 11, "policygradi": 11, "e005_reward": 11, "npy": 11, "e010_reward": 11, "e015_reward": 11, "e020_reward": 11, "e0": 11, "05": 11, "color": [11, 24], "smooth": 11, "smoothed_reward": 11, "start_index": 11, "c": [11, 13], "titl": 11, "xlabel": 11, "ylabel": 11, "legend": [11, 24], "decis": 12, "formul": 12, "h": [12, 13], "encourag": 12, "explor": 12, "robust": [12, 13], "framework": 12, "involv": 12, "sequenti": [12, 19, 20], "relu": [12, 19, 20], "log_std": 12, "chunk": 12, "action_rang": 12, "try": [12, 13], "tanh": 12, "randn_lik": 12, "pow": [12, 18], "6": [12, 15], "keepdim": [12, 20], "over": [12, 18, 20], "except": 12, "occur": 12, "local": 12, "softactorcriticag": 12, "target_valu": 12, "q_value1": 12, "q_value2": 12, "q_value_optimizer1": 12, "q_value_optimizer2": 12, "150": 12, "7": 12, "q_value_scheduler1": 12, "q_value_scheduler2": 12, "train_value_loss": 12, "train_q_value1_loss": 12, "train_q_value2_loss": 12, "train_policy_loss": 12, "track": 12, "log_action_prob": 12, "old_stat": 12, "old_act": 12, "new_act": [12, 16], "new_q_value1": 12, "new_q_value2": 12, "new_q_valu": 12, "retain_graph": 12, "old_q_value1": 12, "old_q_value2": 12, "q_value_loss1": 12, "q_value_loss2": 12, "eta": 13, "left": 13, "right": [13, 15], "suppos": [13, 19], "tild": 13, "mid": 13, "rho_": 13, "frequenc": 13, "replac": 13, "accur": 13, "sens": [13, 15], "match": 13, "order": 13, "theta_0": 13, "isn": 13, "too": 13, "larg": [13, 14, 18], "unclear": 13, "small": [13, 14, 21, 22], "insight": 13, "influenc": 13, "d_": [12, 13], "tv": 13, "measur": 13, "distanc": 13, "tight": 13, "exampl": [13, 18, 22, 24], "cd_": 13, "monoton": 13, "pi_i": 13, "why": [13, 19], "m_i": 13, "definit": 13, "theoret": 13, "sound": 13, "necessari": 13, "tractabl": 13, "usal": 13, "simplif": 13, "ensur": 13, "intract": 13, "lot": [13, 15, 22, 24], "TO": [13, 20], "overlin": 13, "theta_1": 13, "theta_2": 13, "manag": 13, "theta_k": 13, "g": 13, "th": 13, "hessian": 13, "matrix": 13, "becom": [13, 18, 19], "quadrat": 13, "analyt": 13, "theta_i": 13, "g_i": 13, "h_i": 13, "numer": 13, "unstabl": 13, "thing": [13, 23], "hx": 13, "There": [13, 24], "popular": 13, "conjug": 13, "descent": 13, "found": 13, "adjust": 13, "chang": [13, 20], "h_ix_i": 13, "touch": 13, "ve": 13, "made": [13, 20], "due": [13, 19], "veri": 13, "bring": 13, "out": [13, 21, 24], "maximium": 13, "delta_k": 13, "g_k": 13, "x_k": 13, "delta_i": 13, "accept": [13, 22], "break": [13, 18], "last": 13, "tqdm": 13, "o": 13, "base64": 13, "io": 13, "ipython": 13, "ipythondisplai": 13, "datetim": 13, "stable_baselines3": 13, "ppo": 13, "sy": 13, "warn": 13, "filterwarn": 13, "ignor": 13, "categori": 13, "userwarn": 13, "deprecationwarn": 13, "logger": 13, "set_level": 13, "action_prob": 13, "gather": [13, 19], "trpoagent": 13, "lmbda": 13, "action_spac": [13, 15, 16, 18, 19, 20], "compute_advantag": 13, "advantage_list": 13, "dtype": 13, "hessian_product": 13, "old_dist": 13, "new_dist": 13, "kl_diverg": 13, "kl_grad": 13, "autograd": 13, "grad": 13, "create_graph": 13, "view": 13, "kl_grad_product": 13, "dot": 13, "hessian_matrix_product": 13, "conjugate_gradi": 13, "tol": 13, "clone": 13, "r_r": 13, "hp": 13, "new_r_r": 13, "surrog": 13, "line_search": 13, "max_direct": 13, "old_param": 13, "convert_paramet": 13, "parameters_to_vector": 13, "old_surrog": 13, "new_param": 13, "new_polici": 13, "deepcopi": [13, 19], "vector_to_paramet": 13, "new_surrog": 13, "mse_loss": 13, "td_target": [13, 19], "surrogate_grad": 13, "direct": [13, 23], "env_nam": 13, "95": 13, "0005": [13, 19], "value_lr": 13, "num_episod": 13, "g_t": 15, "happen": 15, "could": 15, "less": [15, 16, 18], "desir": 15, "meaning": 15, "progress": 15, "mathemat": 15, "td": [15, 16], "immedi": 15, "arbitrari": [15, 16], "termin": [15, 16], "observ": [15, 16, 19], "extend": 15, "48": 15, "arriv": 15, "bottom": 15, "corner": 15, "reciev": 15, "penalti": 15, "live": 15, "katex": 15, "figur": [15, 16], "estimate_value_funct": 15, "get_act": [15, 16], "new_stat": [15, 16], "info": [15, 16, 18], "47": [15, 16], "randompolici": 15, "cliffwalk": [15, 16], "black": 15, "box": [15, 22], "compar": [15, 18, 19], "awai": [15, 16], "higher": 15, "chanc": [15, 19], "fall": [15, 16], "cover": 16, "idea": 16, "setup": 16, "adopt": 16, "strategi": 16, "cliff": 16, "walk": 16, "patch": 16, "polygon": 16, "step_count": 16, "learnedpolici": 16, "action_valu": 16, "3000": 16, "ran": 16, "As": [16, 19, 24], "close": [16, 18], "prefer": 16, "path": [16, 23], "safer": 16, "modifi": 16, "place": 16, "offlin": 16, "version": 16, "1989": 16, "watkin": 16, "bootstrap": 16, "q_learn": 16, "between": [16, 18], "conserv": 16, "timestep": 16, "complet": 16, "riski": 16, "risk": 16, "thorough": 16, "thereaft": 18, "discret": 18, "straightforward": 18, "tabl": 18, "infeas": 18, "deal": 18, "race": 18, "car": 18, "robot": 18, "arm": 18, "infinit": 18, "unbound": 18, "bin": 18, "But": [18, 24], "complex": 18, "lead": 18, "explod": 18, "accuraci": 18, "overcom": 18, "techniqu": 18, "randint": [18, 19, 20], "qlearningagentwithoutbuff": 18, "agent_without_buff": 18, "core": 18, "assumpt": 18, "supervis": 18, "independ": 18, "ident": 18, "correl": 18, "especi": 18, "within": 18, "ineffici": [18, 19], "poor": 18, "rl": 18, "particularli": [18, 20], "discard": 18, "reus": 18, "multipl": 18, "mai": 18, "fulli": 18, "exploit": 18, "mnih": 18, "concept": 18, "consecut": 18, "incorpor": 18, "architectur": [18, 20], "tri": 18, "fill": 18, "__len__": 18, "qlearningagentwithbuff": 18, "buffer_max_len": [18, 19], "element": 18, "state_batch": [18, 20], "action_batch": [18, 20], "reward_batch": [18, 20], "next_state_batch": [18, 20], "done_batch": [18, 20], "up": [18, 19], "evaluate_n_episod": [18, 20], "evaluation_n_episod": 18, "total_reward": 18, "agent_with_buff": 18, "ne": 18, "b8": 18, "b32": 18, "evid": [18, 22], "significantli": 18, "quickli": 18, "curv": 18, "acceler": 18, "doubleqlearningag": 19, "train_reward": 19, "lunarland": 19, "v2": 19, "64": 19, "qlearn": [], "preprocess": 20, "ql": 20, "5e": 20, "valuebasedmethod": [], "duelq": [], "you": [21, 22, 23, 24], "feel": 21, "content": [21, 22, 23], "structur": [21, 22], "off": [21, 22, 23], "few": 21, "major": 21, "file": [21, 23], "depth": 21, "particular": [19, 20, 21], "topic": 21, "document": [21, 22, 23, 24], "page": [21, 22, 23], "bundl": 21, "deep": [20, 21], "doubl": 21, "duel": 21, "proxim": 21, "write": [22, 23], "your": [22, 23, 24], "book": [22, 23, 24], "jupyt": [22, 23, 24], "notebook": 22, "ipynb": 22, "md": [22, 23], "flavor": 22, "syntax": 22, "stand": 22, "markedli": 22, "slight": [20, 22], "commonmark": 22, "extens": 22, "sphinx": 22, "ecosystem": 22, "overview": 22, "power": 22, "tool": 22, "markup": 22, "languag": 22, "purpos": 22, "span": 22, "mani": [22, 23], "kind": 22, "those": 22, "here": [22, 24], "render": 22, "special": 22, "build": 22, "inlin": 22, "refer": 22, "cite": 22, "bibtex": 22, "holdgraf_evidence_2014": 22, "hdhpk14": 22, "moreov": 22, "bibliographi": 22, "properli": 22, "bib": 22, "christoph": 22, "ramsai": 22, "holdgraf": 22, "wendi": 22, "de": 22, "heer": 22, "brian": 22, "paslei": 22, "robert": 22, "knight": 22, "predict": 22, "human": 22, "auditori": 22, "cortex": 22, "intern": 22, "confer": 22, "cognit": 22, "neurosci": 22, "brisban": 22, "australia": 22, "2014": 22, "frontier": 22, "just": 22, "starter": 22, "jupyterbook": 22, "org": 22, "detail": 23, "instruct": 23, "built": 23, "block": 23, "kernel": 23, "rest": 23, "jupytext": 23, "convert": 23, "support": 23, "understand": 23, "top": 23, "presenc": 23, "That": 23, "treat": 23, "command": 23, "init": 23, "markdownfil": 23, "emb": 24, "imag": 24, "html": 24, "etc": 24, "post": 24, "add_": 24, "math": 24, "mbox": 24, "la_": 24, "tex": 24, "sure": 24, "escap": 24, "dollar": 24, "sign": 24, "keep": 24, "guid": 24, "rcparam": 24, "cycler": 24, "ion": 24, "reproduc": 24, "19680801": 24, "logspac": 24, "randn": 24, "ii": 24, "cmap": 24, "cm": 24, "coolwarm": 24, "ax": 24, "prop_cycl": 24, "linspac": 24, "line2d": 24, "custom_lin": 24, "lw": 24, "fig": 24, "subplot": 24, "figsiz": 24, "cold": 24, "medium": 24, "hot": 24, "overestim": 19, "highli": 19, "goe": 19, "accumul": 19, "eventuuali": 19, "affect": [19, 20], "hassult": 19, "q_a": 19, "q_b": 19, "longrightarrow": 19, "counteract": 19, "thu": 19, "disentangl": 19, "separ": [19, 20], "randomli": 19, "update_a": 19, "update_b": 19, "computation": 19, "year": 19, "hassault": 19, "scheme": 19, "m": 19, "associ": 12, "tierat": 12, "wang": 20, "great": 20, "success": 20, "reprsent": 20, "nueral": 20, "togeth": 20, "relev": 20, "unnecessari": 20, "architur": 20, "lack": 20, "modif": 20, "plug": 20, "benefit": 20, "duelqlearningag": 20, "argmin": 12, "z_": 12}, "objects": {}, "objtypes": {}, "objnames": {}, "titleterms": {"markov": [0, 21], "decis": [0, 21], "process": [0, 21], "polici": [1, 2, 4, 6, 7, 9, 10, 11, 12, 13, 15, 19, 21], "iter": [1, 2, 12, 13], "evalu": [1, 15], "e": 1, "improv": [1, 13], "i": [1, 22], "lemma": 1, "valu": [2, 5, 6, 7, 15, 21], "gridworld": 2, "environ": 2, "implement": [2, 6, 7, 8, 9, 11, 12, 13, 19, 20], "learn": [2, 16, 18, 19, 20, 22], "optim": [2, 13], "theoret": 3, "analysi": 3, "mont": [4, 5, 6], "carlo": [4, 5, 6], "tree": [], "search": 13, "control": [4, 16], "explor": [4, 7], "start": 4, "stochast": 4, "estim": [5, 8, 15], "first": 5, "visit": 5, "v": 5, "": 5, "everi": 5, "blackjack": 5, "function": [5, 6, 7, 15], "q": [5, 16, 18, 19, 20], "actor": [6, 12], "critic": [6, 12], "method": [6, 7, 9, 21], "approxim": 6, "tempor": [6, 15, 16], "differ": [6, 15, 16], "algorithm": [6, 13], "td": 6, "determinist": 7, "gradient": [7, 9, 10, 11, 13], "deep": [7, 18, 19], "action": 7, "updat": 7, "ddpg": 7, "gener": 8, "advantag": 8, "theorem": [9, 10], "inform": 9, "reinforc": 9, "proxim": 11, "ppo": 11, "soft": 12, "maximum": 12, "entropi": 12, "object": [12, 13], "trust": 13, "region": 13, "bound": 13, "natur": 13, "1": 13, "averag": 13, "kl": 13, "diverg": 13, "2": 13, "linear": 13, "constraint": 13, "truncat": 13, "trpo": 13, "backtrack": 13, "line": 13, "varianc": 14, "reduct": 14, "reward": 14, "go": 14, "baselin": 14, "subtract": 14, "cliff": 15, "walk": 15, "sarsa": 16, "vanilla": 18, "experi": 18, "replai": 18, "doubl": 19, "lunar": 19, "lander": 19, "duel": 20, "welcom": 21, "your": 21, "jupyt": 21, "book": 21, "A": 21, "b": 21, "base": 21, "c": 21, "markdown": [22, 23, 24], "file": 22, "what": 22, "myst": [22, 23, 24], "sampl": 22, "role": 22, "direct": 22, "citat": 22, "more": 22, "notebook": [23, 24], "an": 23, "exampl": 23, "cell": 23, "creat": 23, "quickli": 23, "add": 23, "yaml": 23, "metadata": 23, "content": 24, "code": 24, "block": 24, "output": 24, "adoubl": [], "identifi": 20, "dqn": 20}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinxcontrib.bibtex": 9, "sphinx": 60}, "alltitles": {"Markov decision process": [[0, "markov-decision-process"]], "Policy iteration": [[1, "policy-iteration"], [1, "id1"]], "Policy evaluation (E)": [[1, "policy-evaluation-e"]], " (Policy-evaluation)": [[1, "simple-algorithm"]], "Policy improvement (I)": [[1, "policy-improvement-i"]], " (Policy-improvement-lemma)": [[1, "my-theorem"]], " (Policy-iteration)": [[1, "simple-algorithm"]], "Value iteration": [[2, "value-iteration"]], " (Value iteration)": [[2, "simple-algorithm"]], "GridWorld environment": [[2, "gridworld-environment"]], "Value iteration implementation": [[2, "value-iteration-implementation"]], "Learning the optimal policy": [[2, "learning-the-optimal-policy"]], "Theoretic analysis": [[3, "theoretic-analysis"]], "": [[3, "my-theorem"], [3, "my-theorem"]], "Monte Carlo control": [[4, "monte-carlo-control"]], "Monte Carlo with exploring starts": [[4, "monte-carlo-with-exploring-starts"]], "Monte Carlo with stochastic policies": [[4, "monte-carlo-with-stochastic-policies"]], "Value estimation": [[5, "value-estimation"]], "First-visit v.s. every visit Monte Carlo": [[5, "first-visit-v-s-every-visit-monte-carlo"]], "BlackJack": [[5, "blackjack"]], "Estimating of value functions": [[5, "estimating-of-value-functions"]], "Estimating of q-value function": [[5, "estimating-of-q-value-function"]], "Actor critic methods": [[6, "actor-critic-methods"]], "Monte Carlo approximation": [[6, "monte-carlo-approximation"]], "Temporal difference approximation": [[6, "temporal-difference-approximation"]], "Actor critic algorithm": [[6, "actor-critic-algorithm"]], " (Actor critic algorithm)": [[6, "my-algorithm"]], "Actor critic approximation": [[6, "actor-critic-approximation"]], "Policy and value function implementation": [[6, "policy-and-value-function-implementation"]], "Actor critic with Monte Carlo implementation": [[6, "actor-critic-with-monte-carlo-implementation"]], "Actor critic with TD implementation": [[6, "actor-critic-with-td-implementation"]], "Deterministic policy gradient method": [[7, "deterministic-policy-gradient-method"]], "Deep deterministic policy gradient": [[7, "deep-deterministic-policy-gradient"]], " (Deep deterministic policy gradient)": [[7, "my-algorithm"]], "Action value function update": [[7, "action-value-function-update"]], "Exploration": [[7, "exploration"]], "DDPG Implementation": [[7, "ddpg-implementation"]], "Generalized advantage estimation": [[8, "generalized-advantage-estimation"]], "Generalized advantage estimator implementation": [[8, "generalized-advantage-estimator-implementation"]], "Policy gradient method": [[9, "policy-gradient-method"]], "Policy gradient theorem (Informal)": [[9, "policy-gradient-theorem-informal"]], " (REINFORCE)": [[9, "my-algorithm"]], "REINFORCE Implementation": [[9, "reinforce-implementation"]], "Policy gradient theorem": [[10, "policy-gradient-theorem"]], "Proximal policy gradient": [[11, "proximal-policy-gradient"]], "PPO Implementation": [[11, "ppo-implementation"]], "Trust region policy optimization": [[13, "trust-region-policy-optimization"]], " (Improvement bound)": [[13, "my-theorem"]], " (Policy iteration algorithm)": [[13, "my-theorem"]], "Natural policy gradient": [[13, "natural-policy-gradient"]], "1. Average KL divergence": [[13, "average-kl-divergence"]], "2. Linearize objective and constraint": [[13, "linearize-objective-and-constraint"]], " (Natural policy gradient)": [[13, "natural-policy-gradient"]], "Truncated natural policy gradient": [[13, "truncated-natural-policy-gradient"]], " (Truncated natural policy gradient)": [[13, "truncated-natural-policy-gradient"]], "Trust region policy optimization (TRPO)": [[13, "trust-region-policy-optimization-trpo"]], " (Backtracking line search)": [[13, "line-search"]], " (Trust region policy optimization)": [[13, "trpo"]], "TRPO Implementation": [[13, "trpo-implementation"]], "Variance reduction": [[14, "variance-reduction"]], "Reward to go": [[14, "reward-to-go"]], "Baseline subtraction": [[14, "baseline-subtraction"]], "Temporal difference": [[15, "temporal-difference"]], " (Temporal difference policy evaluation)": [[15, "my-algorithm"]], "Cliff walking": [[15, "cliff-walking"]], "Estimating value function": [[15, "estimating-value-function"]], "Temporal difference control": [[16, "temporal-difference-control"]], "SARSA": [[16, "sarsa"]], " (SARSA)": [[16, "my-algorithm"]], "Q-Learning": [[16, "q-learning"]], " (Q-learning)": [[16, "my-algorithm"]], "Deep Q Learning": [[18, "deep-q-learning"]], "Vanilla deep Q Learning": [[18, "vanilla-deep-q-learning"]], "Deep Q Learning with experience replay": [[18, "deep-q-learning-with-experience-replay"]], "Double Q Learning": [[19, "double-q-learning"]], " (Double Q Learning)": [[19, "my-algorithm"]], " (Deep double Q Learning)": [[19, "my-algorithm"]], "Double Q Learning Implementation": [[19, "double-q-learning-implementation"]], "Policy": [[19, "policy"]], "Lunar Lander": [[19, "lunar-lander"]], "Dueling Q Learning": [[20, "dueling-q-learning"]], "Identifiability": [[20, "identifiability"]], "Duel DQN implementation": [[20, "duel-dqn-implementation"]], "Welcome to your Jupyter Book": [[21, "welcome-to-your-jupyter-book"]], "A. Markov decision process": [[21, null]], "B. Value based methods": [[21, null]], "C. Policy based methods": [[21, null]], "Markdown Files": [[22, "markdown-files"]], "What is MyST?": [[22, "what-is-myst"]], "Sample Roles and Directives": [[22, "sample-roles-and-directives"]], "Citations": [[22, "citations"]], "Learn more": [[22, "learn-more"]], "Notebooks with MyST Markdown": [[23, "notebooks-with-myst-markdown"]], "An example cell": [[23, "an-example-cell"]], "Create a notebook with MyST Markdown": [[23, "create-a-notebook-with-myst-markdown"]], "Quickly add YAML metadata for MyST Notebooks": [[23, "quickly-add-yaml-metadata-for-myst-notebooks"]], "Content with notebooks": [[24, "content-with-notebooks"]], "Markdown + notebooks": [[24, "markdown-notebooks"]], "MyST markdown": [[24, "myst-markdown"]], "Code blocks and outputs": [[24, "code-blocks-and-outputs"]], "Soft actor critic": [[12, "soft-actor-critic"]], "Maximum entropy objective": [[12, "maximum-entropy-objective"]], " (Soft policy iteration)": [[12, "my-algorithm"]], "Implementation": [[12, "implementation"]]}, "indexentries": {}})