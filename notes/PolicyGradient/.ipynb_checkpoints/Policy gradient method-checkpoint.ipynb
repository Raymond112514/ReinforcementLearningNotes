{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8166629f",
   "metadata": {},
   "source": [
    "# Policy gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1d66f",
   "metadata": {},
   "source": [
    "In policy gradient method, we optimize\n",
    "\n",
    "$$\\theta^* = \\underset{\\theta}{\\text{argmax}} \\; J(\\theta)  =  \\underset{\\theta}{\\text{argmax}} \\; \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\bigg[\\underbrace{\\sum_{t\\geq 0}r(\\mathbf{s}_t, \\mathbf{a}_t)}_{r(\\tau)}\\bigg]$$\n",
    "\n",
    "To solve the optimization problem, we apply gradient acsend directly on the objective. Evaluating the gradient gives us\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\theta} J(\\theta) &= \\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[r(\\tau)]\\\\\n",
    "    &= \\nabla_{\\theta} \\int r(\\tau)p_{\\theta}(\\tau)d\\tau\\\\\n",
    "    &= \\int r(\\tau) \\nabla_{\\theta}p_{\\theta}(\\tau)d\\tau\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Next, we use the trick that\n",
    "\n",
    "$$\\nabla_{\\theta} \\log p_{\\theta}(\\tau) = \\frac{\\nabla_{\\theta} p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)}$$\n",
    "\n",
    "Substituting gives us\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{\\theta} J(\\theta) &= \\int r(\\tau) \\nabla_{\\theta}p_{\\theta}(\\tau)d\\tau\\\\\n",
    "    &= \\int r(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) p_{\\theta}(\\tau)d\\tau\\\\\n",
    "    &= \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\bigg[r(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau)\\bigg]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can further simplify the gradient by noting that\n",
    "\n",
    "$$p_{\\theta}(\\tau) = p(\\mathbf{s}_0)\\prod_{t\\geq 0} \\pi_{\\theta}(\\mathbf{a}_t|\\mathbf{s}_t) p(\\mathbf{s}_{t+1}|\\mathbf{s}_t, \\mathbf{a}_t) \\implies \\log p_{\\theta}(\\tau) = \\sum_{t\\geq 0} \\nabla_{\\theta} \\pi_{\\theta}(\\mathbf{a}_t|\\mathbf{s}_t)$$\n",
    "\n",
    "This gives us\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) =  \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\bigg[\\bigg(\\sum_{t\\geq 0}r(\\mathbf{s}_t, \\mathbf{a}_t)\\bigg)\\bigg(\\sum_{t\\geq 0}\\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_t|\\mathbf{s}_t)\\bigg)\\bigg] \\approx \\frac{1}{N}\\sum_{i=1}^N  \\bigg(\\sum_{t\\geq 0}r(\\mathbf{s}_{i,t}, \\mathbf{a}_{i,t})\\bigg)\\bigg(\\sum_{t\\geq 0}\\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t})\\bigg)$$\n",
    "\n",
    "Which now can be approximated using Monte Carlo methods through sampling over the trajectory! This motivates the REINFORCE algorithm, presented below\n",
    "\n",
    "```{prf:algorithm} REINFORCE\n",
    ":label: my-algorithm\n",
    "\n",
    "**Inputs** Learning rate $\\alpha, \\epsilon \\in (0, 1)$\n",
    "\n",
    "**Output** Estimated policy $\\pi(\\mathbf{a}|\\mathbf{s})$\n",
    "\n",
    "1. Initialize $Q(s, a)$ arbitrary for all $s\\in \\mathcal{S}, a\\in \\mathcal{A}$ expect for terminal states\n",
    "2. For each episode\n",
    "\t1. Initialize state $\\mathbf{s}_0$\n",
    "\t2. While not terminate\n",
    "\t\t1. Choose $\\mathbf{a}_t$ from current $Q$ function estimate with $\\epsilon$-greedy strategy.\n",
    "\t\t2. Take action $\\mathbf{a}_t$, observe $r_t, \\mathbf{s}_{t+1}$\n",
    "        3. Choose $\\mathbf{a}_{t+1}$ from current $Q$ function estimate with $\\epsilon$-greedy strategy.\n",
    "        3. Update $Q(\\mathbf{s}_t, \\mathbf{a}_t) \\leftarrow Q(\\mathbf{s}_t, \\mathbf{a}_t) + \\alpha \\cdot [r_t + \\gamma Q(\\mathbf{s}_{t+1}, \\mathbf{a}_{t+1}) - Q(\\mathbf{s}_{t}, \\mathbf{a}_{t})]$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0398c2c3",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d5407",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "!pip -q install pybullet\n",
    "!pip -q install stable-baselines3[extra]\n",
    "!pip -q install pyvirtualdisplay\n",
    "!apt-get install -y xvfb\n",
    "\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvirtualdisplay\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.net(obs)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state).float().to(DEVICE)\n",
    "        action_dist = self.net(state)\n",
    "        action_dist = torch.distributions.Categorical(probs=action_dist)\n",
    "        action = action_dist.sample()\n",
    "        action_log_prob = action_dist.log_prob(action)\n",
    "        return action, action_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, env, policy, lr=1e-3, device=\"cpu\"):\n",
    "        self.env = env\n",
    "        self.policy = policy.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "    def learn_episode(self, batch_size, max_steps=10000, gamma=1):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "        episode_reward = 0.0\n",
    "        for i in range(batch_size):\n",
    "            total_log_prob = 0\n",
    "            total_reward = 0\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "            while not done and steps < max_steps:\n",
    "                action, action_log_prob = self.policy.select_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(int(action.item()))\n",
    "                total_log_prob += action_log_prob\n",
    "                total_reward += reward * gamma ** steps\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "            loss += -total_log_prob * total_reward / batch_size\n",
    "            episode_reward += total_reward / batch_size\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item(), episode_reward"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
