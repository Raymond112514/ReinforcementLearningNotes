
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5.3. Temporal difference control &#8212; Notes on Reinforcement learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=4787184b" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'TD/Temporal-difference-control';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6. Deep Q Learning" href="../ValueBasedMethods/Deep-Q-Learning.html" />
    <link rel="prev" title="5. Temporal difference" href="Temporal-difference.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Notes on Reinforcement learning</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">A. Markov decision process</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../MDP/Markov-decision-process.html">1. Markov decision process</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MDP/Value-iteration.html">2. Value iteration</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../MDP/Value-iteration-convergence.html">2.4. Theoretic analysis</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../MDP/Policy-iteration.html">3. Policy iteration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">B. Value based methods</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../MonteCarlo/Monte-Carlo-value-estimation.html">4. Value estimation</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../MonteCarlo/Monte-Carlo-Control.html">4.5. Monte Carlo control</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="Temporal-difference.html">5. Temporal difference</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">5.3. Temporal difference control</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ValueBasedMethods/Deep-Q-Learning.html">6. Deep Q Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ValueBasedMethods/Double-Q-Learning.html">7. Double Q Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ValueBasedMethods/Dueling-Q-Learning.html">8. Dueling Q Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">C. Policy based methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../PolicyGradient/Policy-gradient-method.html">9. Policy gradient method</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PolicyGradient/Actor-critic-methods.html">10. Actor critic methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PolicyGradient/Generalized-advantage-estimation.html">11. Generalized advantage estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PolicyGradient/TRPO.html">12. Trust region policy optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PolicyGradient/Proximal-policy-gradient.html">13. Proximal policy gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PolicyGradient/Deterministic-policy-gradient-method.html">14. Deterministic policy gradient method</a></li>
<li class="toctree-l1"><a class="reference internal" href="../PolicyGradient/Soft-actor-critic.html">15. Soft actor critic</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FTD/Temporal-difference-control.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/TD/Temporal-difference-control.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Temporal difference control</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa">5.3.1. SARSA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning">5.3.2. Q-Learning</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="temporal-difference-control">
<h1><span class="section-number">5.3. </span>Temporal difference control<a class="headerlink" href="#temporal-difference-control" title="Link to this heading">#</a></h1>
<p>In the previous section, we discussed the temporal difference method way of approximating the value function of a policy. In this section, we cover how temporal difference can help us determine the optimal policy. The idea is similar to that of Monte Carlo control. To determine the optimal policy, we need to estimate the state action value function. In a temporal difference setup, the update rule is given by</p>
<div class="math notranslate nohighlight">
\[Q(\mathbf{s}_t, \mathbf{a}_t) \leftarrow Q(\mathbf{s}_t, \mathbf{a}_t) + \alpha \cdot [r_t + \gamma Q(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}) - Q(\mathbf{s}_{t}, \mathbf{a}_{t})]\]</div>
<section id="sarsa">
<h2><span class="section-number">5.3.1. </span>SARSA<a class="headerlink" href="#sarsa" title="Link to this heading">#</a></h2>
<p>We adopt a strategy as in Monte Carlo method and update our policy while estimating the state action value function. This gives the following algorithm known as SARSA.</p>
<div class="proof algorithm admonition" id="my-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 5.3 </span> (SARSA)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Learning rate <span class="math notranslate nohighlight">\(\alpha, \epsilon \in (0, 1)\)</span></p>
<p><strong>Output</strong> Estimated policy <span class="math notranslate nohighlight">\(\pi(\mathbf{a}|\mathbf{s})\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(Q(s, a)\)</span> arbitrary for all <span class="math notranslate nohighlight">\(s\in \mathcal{S}, a\in \mathcal{A}\)</span> expect for terminal states</p></li>
<li><p>For each episode</p>
<ol class="arabic simple">
<li><p>Initialize state <span class="math notranslate nohighlight">\(\mathbf{s}_0\)</span></p></li>
<li><p>While not terminate</p>
<ol class="arabic simple">
<li><p>Choose <span class="math notranslate nohighlight">\(\mathbf{a}_t\)</span> from current <span class="math notranslate nohighlight">\(Q\)</span> function estimate with <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy strategy.</p></li>
<li><p>Take action <span class="math notranslate nohighlight">\(\mathbf{a}_t\)</span>, observe <span class="math notranslate nohighlight">\(r_t, \mathbf{s}_{t+1}\)</span></p></li>
<li><p>Choose <span class="math notranslate nohighlight">\(\mathbf{a}_{t+1}\)</span> from current <span class="math notranslate nohighlight">\(Q\)</span> function estimate with <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy strategy.</p></li>
<li><p>Update <span class="math notranslate nohighlight">\(Q(\mathbf{s}_t, \mathbf{a}_t) \leftarrow Q(\mathbf{s}_t, \mathbf{a}_t) + \alpha \cdot [r_t + \gamma Q(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}) - Q(\mathbf{s}_{t}, \mathbf{a}_{t})]\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</section>
</div><p>The algorithm above is called SARSA since each update requires using a <span class="math notranslate nohighlight">\((\mathbf{s}_t, \mathbf{a}_t, r_t, \mathbf{s}_{t+1}, \mathbf{a}_{t+1})\)</span> sample. Below, we implement SARSA to determine the policy for the cliff-walking environment.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Polygon</span>
<span class="kn">from</span> <span class="nn">td</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">SARSA</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">n_episodes</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">states</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">actions</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="p">{</span><span class="n">state</span><span class="p">:</span> <span class="p">{</span><span class="n">action</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">}</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">states</span><span class="p">}</span>
    
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">q_values</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">cum_reward</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span> <span class="ow">and</span> <span class="n">step_count</span> <span class="o">&lt;</span> <span class="n">max_steps</span><span class="p">:</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)[:</span><span class="mi">4</span><span class="p">]</span>
            <span class="n">new_action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">new_state</span><span class="p">,</span> <span class="n">q_values</span><span class="p">)</span>
            <span class="n">q_values</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">q_values</span><span class="p">[</span><span class="n">new_state</span><span class="p">][</span><span class="n">new_action</span><span class="p">]</span> <span class="o">-</span> <span class="n">q_values</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">new_action</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">==</span> <span class="o">-</span><span class="mi">100</span>
            <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="mi">47</span><span class="p">:</span>
                <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">True</span>
            <span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">cum_reward</span> <span class="o">+=</span> <span class="n">reward</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">**</span> <span class="n">step_count</span>
    <span class="k">return</span> <span class="n">q_values</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LearnedPolicy</span><span class="p">(</span><span class="n">Policy</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">q_values</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="n">action_values</span> <span class="o">=</span> <span class="n">q_values</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> 
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actions</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">action_values</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CliffWalking-v0&#39;</span><span class="p">)</span>
<span class="n">states</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
<span class="n">actions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">LearnedPolicy</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
<span class="n">q_values</span> <span class="o">=</span> <span class="n">SARSA</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="mi">3000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We ran SARSA for around <span class="math notranslate nohighlight">\(3000\)</span> epochs with learning rate <span class="math notranslate nohighlight">\(\alpha=0.1\)</span> and <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span>. The discount factor is set to be <span class="math notranslate nohighlight">\(1\)</span>. The estimated policy is shown in the figure below. As we can see, the estimated policy move away from states closing to the cliff and prefers the path that is longer but safer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">q_values</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ba9417fb75b60a77351634c9e035492845514a543636f34b0e42f2d918f35bc3.png" src="../_images/ba9417fb75b60a77351634c9e035492845514a543636f34b0e42f2d918f35bc3.png" />
</div>
</div>
</section>
<section id="q-learning">
<h2><span class="section-number">5.3.2. </span>Q-Learning<a class="headerlink" href="#q-learning" title="Link to this heading">#</a></h2>
<p>SARSA is an on policy temporal difference method, since we are modifying the policy in place while training. An offline version of temporal difference control is proposed in 1989 by Watkins. The algorithm is known as Q-learning. Instead of using the bootstrapped estimate of future value, Q learning updates the state action function as follows</p>
<div class="math notranslate nohighlight">
\[Q(\mathbf{s}_t, \mathbf{a}_t) \leftarrow Q(\mathbf{s}_t, \mathbf{a}_t) + \alpha \cdot [r_t + \gamma \max_{\mathbf{a}\in \mathcal{A}} Q(\mathbf{s}_{t+1}, \mathbf{a}) - Q(\mathbf{s}_{t}, \mathbf{a}_{t})]\]</div>
<p>Note that this is an offline method because we generate the samples using an <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy method but updates our estimate using the greedy policy</p>
<div class="math notranslate nohighlight">
\[\hat{\pi}(\mathbf{a}|\mathbf{s}) = \underset{\mathbf{a}\in \mathcal{A}}{\text{argmax}}\; Q(\mathbf{s}_{t+1}, \mathbf{a})\]</div>
<p>The psuedocode for Q learning is provided below</p>
<div class="proof algorithm admonition" id="my-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 5.3 </span> (Q-learning)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Learning rate <span class="math notranslate nohighlight">\(\alpha, \epsilon \in (0, 1)\)</span></p>
<p><strong>Output</strong> Estimated policy <span class="math notranslate nohighlight">\(\pi(\mathbf{a}|\mathbf{s})\)</span></p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(Q(s, a)\)</span> arbitrary for all <span class="math notranslate nohighlight">\(s\in \mathcal{S}, a\in \mathcal{A}\)</span> expect for terminal states</p></li>
<li><p>For each episode</p>
<ol class="arabic simple">
<li><p>Initialize state <span class="math notranslate nohighlight">\(\mathbf{s}_0\)</span></p></li>
<li><p>While not terminate</p>
<ol class="arabic simple">
<li><p>Choose <span class="math notranslate nohighlight">\(\mathbf{a}_t\)</span> from current <span class="math notranslate nohighlight">\(Q\)</span> function estimate with <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy strategy.</p></li>
<li><p>Take action <span class="math notranslate nohighlight">\(\mathbf{a}_t\)</span>, observe <span class="math notranslate nohighlight">\(r_t, \mathbf{s}_{t+1}\)</span></p></li>
<li><p>Choose <span class="math notranslate nohighlight">\(\mathbf{a}_{t+1}\)</span> from current <span class="math notranslate nohighlight">\(Q\)</span> function estimate with <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy strategy.</p></li>
<li><p>Update <span class="math notranslate nohighlight">\(Q(\mathbf{s}_t, \mathbf{a}_t) \leftarrow Q(\mathbf{s}_t, \mathbf{a}_t) + \alpha \cdot [r_t + \gamma \max_{\mathbf{a}\in \mathcal{A}} Q(\mathbf{s}_{t+1}, \mathbf{a}) - Q(\mathbf{s}_{t}, \mathbf{a}_{t})]\)</span></p></li>
</ol>
</li>
</ol>
</li>
</ol>
</section>
</div><p>Below we implement and tested Q learning on the cliff walking enviroment. We ran it for around <span class="math notranslate nohighlight">\(3000\)</span> episodes following the same setting as SARSA.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Q_Learning</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">n_episodes</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">states</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">actions</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="p">{</span><span class="n">state</span><span class="p">:</span> <span class="p">{</span><span class="n">action</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">}</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">states</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">q_values</span><span class="p">)</span>
        <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span> <span class="ow">and</span> <span class="n">step_count</span> <span class="o">&lt;</span> <span class="n">max_steps</span><span class="p">:</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)[:</span><span class="mi">4</span><span class="p">]</span>
            <span class="n">new_action</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">new_state</span><span class="p">,</span> <span class="n">q_values</span><span class="p">)</span>
            <span class="n">q_values</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">q_values</span><span class="p">[</span><span class="n">new_state</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">-</span> <span class="n">q_values</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">])</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">new_action</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">==</span> <span class="o">-</span><span class="mi">100</span>
            <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="mi">47</span><span class="p">:</span>
                <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="kc">True</span>
            <span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">q_values</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">states</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
<span class="n">actions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">LearnedPolicy</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

<span class="n">q_values</span> <span class="o">=</span> <span class="n">Q_Learning</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>Note the difference between the estimated policy between SARSA and Q-learning. SARSA learns a conservative solution—it chooses a solution that is safer (i.e. far away from cliff), but takes more timestep to complete. Q-learning on the other hand, learns a more risky solution. It uses less timestep but risk falling into the cliff. One explanation is that Q-learning learns thorough taking greedy actions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">q_values</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/62ecede4e2f6ca5c9717cc6053b16be5e4ee5c4ab94943141bd4b6fbfc882c2b.png" src="../_images/62ecede4e2f6ca5c9717cc6053b16be5e4ee5c4ab94943141bd4b6fbfc882c2b.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./TD"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Temporal-difference.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Temporal difference</p>
      </div>
    </a>
    <a class="right-next"
       href="../ValueBasedMethods/Deep-Q-Learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Deep Q Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa">5.3.1. SARSA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning">5.3.2. Q-Learning</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Raymond Tsao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>