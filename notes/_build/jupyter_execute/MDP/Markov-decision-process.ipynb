{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf95167",
   "metadata": {},
   "source": [
    "# Markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d0f9e",
   "metadata": {},
   "source": [
    "A Markov decision process (MDP) model contains four elements\n",
    "\n",
    "1. State space $\\mathcal{S}$\n",
    "2. Action space $\\mathcal{A}$\n",
    "3. Transition dynamics $p(\\mathbf{s}', r|\\mathbf{s}, \\mathbf{a})$\n",
    "4. Reward dynamics $r(\\mathbf{s}, \\mathbf{a})$\n",
    "\n",
    "An agent, starting at state $\\mathbf{s}\\in \\mathcal{S}$, takes an action $\\mathbf{a}\\in \\mathcal{A}$, recieves reward $r(\\mathbf{s}, \\mathbf{a})$ and arrive at a new state $\\mathbf{s}'$ according to the transition dynamics $p(\\mathbf{s}', r|\\mathbf{s}, \\mathbf{a})$. If we continue this process, we get a trajectory $\\tau=(\\mathbf{s}_1, \\mathbf{a}_1, r_1, \\mathbf{s}_2, \\mathbf{a}_2, r_2,...)$, which could potentially goes on forever. Given any trajectory $\\tau$, we define the reward associated as\n",
    "\n",
    "$$r(\\tau) = \\sum_{t\\geq 0} \\gamma^t r_t$$\n",
    "\n",
    "Where $\\gamma\\in (0,1)$ denotes the discount factor that prevents the reward $r(\\tau)$ to diverge. Naturally, our goal is to find a policy that maximizes the reward. A policy $\\pi(\\mathbf{a}|\\mathbf{s})$ maps a state to a distribution over actions. If we follow along $\\pi$ in the MDP, then it induces a distribution over the trajectory, which, by the Markov property, can be expressed as\n",
    "\n",
    "$$p_{\\pi}(\\tau) = p(\\mathbf{s}_0)\\prod_{t\\geq 0}\\pi(\\mathbf{a}_t|\\mathbf{s}_t) p(\\mathbf{s}_{t+1}|\\mathbf{s}_t, \\mathbf{a}_t)$$\n",
    "\n",
    "We can also define the expected reward associated with a policy,\n",
    "\n",
    "$$\\eta(\\pi) = \\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)}[r(\\tau)]$$\n",
    "\n",
    "The goal is MDP is to find a policy that maximizes the expected reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51997a",
   "metadata": {},
   "source": [
    "## Value function and action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33b1de",
   "metadata": {},
   "source": [
    "Two quantities of particular interest in a Markov Decision Process (MDP) are the value function $V(\\mathbf{s})$ and the state action value function $Q(\\mathbf{s}, \\mathbf{a})$. The value function is defined as the expected reward starting from $\\mathbf{s}$\n",
    "\n",
    "$$V_{\\pi}(\\mathbf{s}) = \\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)|\\mathbf{s}_0=\\mathbf{s}}[r(\\tau)|\\mathbf{s}_0=\\mathbf{s}]$$\n",
    "\n",
    "By applying the law of total expectation, we see that it is related the the expected reward\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\eta(\\pi) &= \\mathbb{E}_{\\mathbf{s}_0\\sim p(\\mathbf{s}_0)}[\\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)|\\mathbf{s}_0=\\mathbf{s}}[r(\\tau)|\\mathbf{s}_0=\\mathbf{s}]]\\\\\n",
    "&= \\mathbb{E}_{\\mathbf{s}_0\\sim p(\\mathbf{s}_0)}[V_{\\pi}(\\mathbf{s}_0)]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since we can't optimize the prior distribution $p(\\mathbf{s}_0)$, maximizing the expected reward is equivalent as maximizing the value function for all states $\\mathbf{s}\\in \\mathcal{S}$. The state action value function, on the other hand, is defined as the expected starting starting at state $\\mathbf{s}$ and taking action $\\mathbf{a}$.\n",
    "\n",
    "$$Q_{\\pi}(\\mathbf{s}, \\mathbf{a}) = \\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)|\\mathbf{s}_0=\\mathbf{s}, \\mathbf{a}_0=\\mathbf{a}}[r(\\tau)|\\mathbf{s}_0=\\mathbf{s}, \\mathbf{a}_0=\\mathbf{a}]$$\n",
    "\n",
    "By applying the law of total probabilities again, we see that the state action value function is related to the value function\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V_{\\pi}(\\mathbf{s}) &= \\mathbb{E}_{\\mathbf{a}_0\\sim \\pi(\\cdot|\\mathbf{s}_0)}[ \\mathbb{E}_{\\tau\\sim p_{\\pi}(\\tau)|\\mathbf{s}_0=\\mathbf{s}, \\mathbf{a}_0=\\mathbf{a}}[r(\\tau)|\\mathbf{s}_0=\\mathbf{s}, \\mathbf{a}_0=\\mathbf{a}]]\\\\\n",
    "&= \\mathbb{E}_{\\mathbf{a}_0\\sim \\pi(\\cdot|\\mathbf{s}_0)}[Q_{\\pi}(\\mathbf{s}, \\mathbf{a})]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In the next section, we introduce a simple algorithm of estimating the optimal policy using these two functions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}